{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "from ivyspt.input_processing import split_surfaces, IVSurfaceDataset\n",
    "from ivyspt.trainer import Trainer\n",
    "from ivyspt.ivyspt import IvySPT\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_data = pd.read_csv('data/pre_train_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "fine_tune_data = pd.read_csv('data/fine_tune_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "pre_train_surfaces_train, pre_train_surfaces_validation, pre_train_surfaces_test = split_surfaces(\n",
    "    pre_train_data,\n",
    "    toy_sample=True,\n",
    "    max_points=600,\n",
    "    max_surfaces=60,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "fine_tune_surfaces_train, fine_tune_surfaces_validation, fine_tune_surfaces_test = split_surfaces(\n",
    "    fine_tune_data,\n",
    "    toy_sample=True,\n",
    "    max_points=600,\n",
    "    max_surfaces=60,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Number of Query Points' : 1,\n",
    "        'Batch Size' : 50\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 32,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 128,\n",
    "        'Attention Dropout' : 0.,\n",
    "        'Gate Dropout' : 0.,\n",
    "        'FFN Dropout' : 0.,\n",
    "        'Number of Blocks' : 4,\n",
    "        'External Feature Dimension' : 5,\n",
    "        'Weight Initializer Std.' : 0.02,\n",
    "        'Linear Bias Initializer' : 0.0,\n",
    "        'Gate Bias Inititalizer' : 10.0\n",
    "    },\n",
    "    'Adaptive Loss Weights' : {\n",
    "        'Asymmetry' : 1.5,\n",
    "    },\n",
    "    'Trainer' : {\n",
    "        'Pre-Train' : {\n",
    "            'Number of Epochs' : 20,\n",
    "            'Warmup Ratio' : 0.15,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-5,\n",
    "            'Gradient Clipping' : 0,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : None,\n",
    "        },\n",
    "        'Fine-Tune' : {\n",
    "            'Number of Epochs' : 10,\n",
    "            'Warmup Ratio' : 0.1,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-6,\n",
    "            'Gradient Clipping' : 0,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : 0.9,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_dataset_train = IVSurfaceDataset(\n",
    "    pre_train_surfaces_train, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_train = DataLoader(\n",
    "    pre_train_dataset_train, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "pre_train_dataset_validation = IVSurfaceDataset(\n",
    "    pre_train_surfaces_validation, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_validation = DataLoader(\n",
    "    pre_train_dataset_validation, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "pre_train_dataset_test = IVSurfaceDataset(\n",
    "    pre_train_surfaces_test, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_test = DataLoader(\n",
    "    pre_train_dataset_test, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "embedding_dims = [4, 8, 16, 32]\n",
    "num_blocks = [1, 2, 4, 8]\n",
    "peak_learning_rates = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# Generate all combinations of embedding dimensions, number of blocks, and peak learning rates\n",
    "combinations = list(product(embedding_dims, num_blocks, peak_learning_rates))\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for embedding_dim, blocks, peak_lr in tqdm(combinations, total=len(combinations)):\n",
    "    # Deep copy the default hyperparameters to test specific combinations\n",
    "    test_hyperparameters = copy.deepcopy(hyperparameters)\n",
    "    \n",
    "    # Set the specific hyperparameters\n",
    "    test_hyperparameters['Surface Embedding']['Embedding Dimension'] = embedding_dim\n",
    "    test_hyperparameters['Surface Encoding']['Number of Blocks'] = blocks\n",
    "    test_hyperparameters['Surface Encoding']['FFN Hidden Dimension'] = 4 * embedding_dim\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'] = peak_lr\n",
    "    \n",
    "    # Initialize the model with the test hyperparameters\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    model_pre_train = IvySPT(\n",
    "        test_hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "        test_hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "        test_hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "        test_hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "        test_hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "        test_hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "        test_hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "        test_hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "        test_hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "        test_hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "        test_hyperparameters['Surface Encoding']['Gate Bias Inititalizer']\n",
    "    )\n",
    "    \n",
    "    # Initialize the trainer with the test hyperparameters\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)        \n",
    "    pre_trainer = Trainer(\n",
    "        model_pre_train,\n",
    "        pre_train_data_loader_train,\n",
    "        pre_train_data_loader_validation,\n",
    "        pre_train_data_loader_test,\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "        test_hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Train the model and get the validation loss components\n",
    "    _, _, validate_loss_components_history = pre_trainer.train()\n",
    "    \n",
    "    # Get the final validation losses for each component\n",
    "    final_validation_losses = validate_loss_components_history[-1]\n",
    "    \n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        'Embedding Dimension': embedding_dim,\n",
    "        'Number of Blocks': blocks,\n",
    "        'Peak Learning Rate': peak_lr,\n",
    "        'MSE Loss': final_validation_losses[0],\n",
    "        'Calendar Arbitrage Loss': final_validation_losses[1],\n",
    "        'Butterfly Arbitrage Loss': final_validation_losses[2]\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create a multi-index DataFrame with embedding dimension, number of blocks, and peak learning rate as the index\n",
    "results_df.set_index(['Embedding Dimension', 'Number of Blocks', 'Peak Learning Rate'], inplace=True)\n",
    "\n",
    "# Rank the columns, where the lowest loss is rank 1\n",
    "ranked_df = results_df.rank(axis=0, method='min', ascending=True)\n",
    "\n",
    "# Create an average rank column and sort by it\n",
    "ranked_df['Average Rank'] = ranked_df.mean(axis=1)\n",
    "ranked_df.sort_values(by='Average Rank', ascending=True, inplace=True)\n",
    "\n",
    "ranked_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
