{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "from ivyspt.input_processing import split_surfaces, IVSurfaceDataset\n",
    "from ivyspt.trainer import Trainer\n",
    "from ivyspt.ivyspt import IvySPT\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_data = pd.read_csv('data/pre_train_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "fine_tune_data = pd.read_csv('data/fine_tune_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "pre_train_surfaces_train, pre_train_surfaces_validation, pre_train_surfaces_test = split_surfaces(\n",
    "    pre_train_data,\n",
    "    toy_sample=True,\n",
    "    max_points=20,\n",
    "    max_surfaces=20,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "fine_tune_surfaces_train, fine_tune_surfaces_validation, fine_tune_surfaces_test = split_surfaces(\n",
    "    fine_tune_data,\n",
    "    toy_sample=True,\n",
    "    max_points=20,\n",
    "    max_surfaces=20,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Number of Query Points' : None,\n",
    "        'Batch Size' : 10\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 32,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 128,\n",
    "        'Attention Dropout' : 0.,\n",
    "        'Gate Dropout' : 0.,\n",
    "        'FFN Dropout' : 0.,\n",
    "        'Number of Blocks' : 4,\n",
    "        'External Feature Dimension' : 5,\n",
    "        'Weight Initializer Std.' : 0.02,\n",
    "        'Linear Bias Initializer' : 0.0,\n",
    "        'Gate Bias Inititalizer' : 10.0\n",
    "    },\n",
    "    'Adaptive Loss Weights' : {\n",
    "        'Asymmetry' : 1.5,\n",
    "    },\n",
    "    'Trainer' : {\n",
    "        'Pre-Train' : {\n",
    "            'Number of Epochs' : 10,\n",
    "            'Warmup Ratio' : 0.15,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-6,\n",
    "            'Gradient Clipping' : None,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : None,\n",
    "        },\n",
    "        'Fine-Tune' : {\n",
    "            'Number of Epochs' : 10,\n",
    "            'Warmup Ratio' : 0.1,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-6,\n",
    "            'Gradient Clipping' : None,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : 0.9,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_dataset_train = IVSurfaceDataset(\n",
    "    pre_train_surfaces_train, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_train = DataLoader(\n",
    "    pre_train_dataset_train, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "pre_train_dataset_validation = IVSurfaceDataset(\n",
    "    pre_train_surfaces_validation, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_validation = DataLoader(\n",
    "    pre_train_dataset_validation, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "pre_train_dataset_test = IVSurfaceDataset(\n",
    "    pre_train_surfaces_test, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_test = DataLoader(\n",
    "    pre_train_dataset_test, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [03:27<1:05:48, 207.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 0.1, 'Peak Learning Rate': 1e-05, 'MSE Loss': 0.007978086, 'Calendar Arbitrage Loss': 0.0020727417, 'Butterfly Arbitrage Loss': 0.8096614}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [06:51<1:01:39, 205.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 0.1, 'Peak Learning Rate': 0.0001, 'MSE Loss': 0.0074118595, 'Calendar Arbitrage Loss': 0.0064550326, 'Butterfly Arbitrage Loss': 4.9240584}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [10:18<58:26, 206.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 0.1, 'Peak Learning Rate': 0.001, 'MSE Loss': 0.01865702, 'Calendar Arbitrage Loss': 0.0002020784, 'Butterfly Arbitrage Loss': 0.017323326}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [13:44<54:58, 206.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 0.1, 'Peak Learning Rate': 0.01, 'MSE Loss': 0.0042393375, 'Calendar Arbitrage Loss': 2.2148943e-06, 'Butterfly Arbitrage Loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [17:08<51:18, 205.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 1, 'Peak Learning Rate': 1e-05, 'MSE Loss': 0.0075784232, 'Calendar Arbitrage Loss': 0.0011701168, 'Butterfly Arbitrage Loss': 1.2276402}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [20:32<47:48, 204.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 1, 'Peak Learning Rate': 0.0001, 'MSE Loss': 0.010340053, 'Calendar Arbitrage Loss': 0.0021425385, 'Butterfly Arbitrage Loss': 0.6851499}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [24:02<44:46, 206.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 1, 'Peak Learning Rate': 0.001, 'MSE Loss': 0.004120839, 'Calendar Arbitrage Loss': 0.0002127424, 'Butterfly Arbitrage Loss': 0.06854783}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [27:28<41:14, 206.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 1, 'Peak Learning Rate': 0.01, 'MSE Loss': 0.0042548305, 'Calendar Arbitrage Loss': 1.9511893e-07, 'Butterfly Arbitrage Loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [30:52<37:40, 205.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 10, 'Peak Learning Rate': 1e-05, 'MSE Loss': 0.008406518, 'Calendar Arbitrage Loss': 0.0050522583, 'Butterfly Arbitrage Loss': 0.4986096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [34:07<33:42, 202.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 10, 'Peak Learning Rate': 0.0001, 'MSE Loss': 0.012325285, 'Calendar Arbitrage Loss': 0.007740541, 'Butterfly Arbitrage Loss': 0.29566038}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [37:34<30:35, 203.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 10, 'Peak Learning Rate': 0.001, 'MSE Loss': 0.007920148, 'Calendar Arbitrage Loss': 0.0062787095, 'Butterfly Arbitrage Loss': 0.15583532}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [41:04<27:26, 205.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 10, 'Peak Learning Rate': 0.01, 'MSE Loss': 0.0044159107, 'Calendar Arbitrage Loss': 3.4739884e-07, 'Butterfly Arbitrage Loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [44:30<24:00, 205.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 100, 'Peak Learning Rate': 1e-05, 'MSE Loss': 0.0069962144, 'Calendar Arbitrage Loss': 0.0023423047, 'Butterfly Arbitrage Loss': 0.50326407}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [47:56<20:34, 205.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 100, 'Peak Learning Rate': 0.0001, 'MSE Loss': 0.00970708, 'Calendar Arbitrage Loss': 0.0030775196, 'Butterfly Arbitrage Loss': 0.5944328}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [51:22<17:09, 205.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 100, 'Peak Learning Rate': 0.001, 'MSE Loss': 0.015611563, 'Calendar Arbitrage Loss': 0.0023918725, 'Butterfly Arbitrage Loss': 0.14590287}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [54:50<13:45, 206.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 100, 'Peak Learning Rate': 0.01, 'MSE Loss': 0.006873709, 'Calendar Arbitrage Loss': 7.790407e-07, 'Butterfly Arbitrage Loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [58:12<10:15, 205.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 1000, 'Peak Learning Rate': 1e-05, 'MSE Loss': 0.008306649, 'Calendar Arbitrage Loss': 0.012935673, 'Butterfly Arbitrage Loss': 2.2684705}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [1:01:38<06:50, 205.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 1000, 'Peak Learning Rate': 0.0001, 'MSE Loss': 0.009268843, 'Calendar Arbitrage Loss': 0.003214235, 'Butterfly Arbitrage Loss': 0.37632728}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [1:04:55<03:22, 202.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 1000, 'Peak Learning Rate': 0.001, 'MSE Loss': 0.022455947, 'Calendar Arbitrage Loss': 0.0062397327, 'Butterfly Arbitrage Loss': 1.03869415e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:08:14<00:00, 204.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gradient Clipping': 1000, 'Peak Learning Rate': 0.01, 'MSE Loss': 0.009691524, 'Calendar Arbitrage Loss': 0.00049700326, 'Butterfly Arbitrage Loss': 0.00062475837}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE Loss</th>\n",
       "      <th>Calendar Arbitrage Loss</th>\n",
       "      <th>Butterfly Arbitrage Loss</th>\n",
       "      <th>Average Rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Clipping</th>\n",
       "      <th>Peak Learning Rate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0.01000</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <th>0.01000</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>0.01000</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <th>0.01000</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0.00100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0</th>\n",
       "      <th>0.01000</th>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <th>0.00100</th>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>0.00100</th>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <th>0.00100</th>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0</th>\n",
       "      <th>0.00010</th>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0</th>\n",
       "      <th>0.00100</th>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0.00010</th>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <th>0.00010</th>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <th>0.00010</th>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>0.00010</th>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      MSE Loss  Calendar Arbitrage Loss  \\\n",
       "Gradient Clipping Peak Learning Rate                                      \n",
       "1.0               0.01000                  3.0                      1.0   \n",
       "0.1               0.01000                  2.0                      4.0   \n",
       "10.0              0.01000                  4.0                      2.0   \n",
       "100.0             0.01000                  5.0                      3.0   \n",
       "1.0               0.00100                  1.0                      6.0   \n",
       "1000.0            0.01000                 14.0                      7.0   \n",
       "0.1               0.00100                 19.0                      5.0   \n",
       "100.0             0.00001                  6.0                     11.0   \n",
       "1.0               0.00001                  8.0                      8.0   \n",
       "10.0              0.00100                  9.0                     17.0   \n",
       "0.1               0.00001                 10.0                      9.0   \n",
       "100.0             0.00100                 18.0                     12.0   \n",
       "1000.0            0.00010                 13.0                     14.0   \n",
       "10.0              0.00001                 12.0                     15.0   \n",
       "1000.0            0.00100                 20.0                     16.0   \n",
       "1.0               0.00010                 16.0                     10.0   \n",
       "100.0             0.00010                 15.0                     13.0   \n",
       "0.1               0.00010                  7.0                     18.0   \n",
       "10.0              0.00010                 17.0                     19.0   \n",
       "1000.0            0.00001                 11.0                     20.0   \n",
       "\n",
       "                                      Butterfly Arbitrage Loss  Average Rank  \n",
       "Gradient Clipping Peak Learning Rate                                          \n",
       "1.0               0.01000                                  1.0      1.666667  \n",
       "0.1               0.01000                                  1.0      2.333333  \n",
       "10.0              0.01000                                  1.0      2.333333  \n",
       "100.0             0.01000                                  1.0      3.000000  \n",
       "1.0               0.00100                                  8.0      5.000000  \n",
       "1000.0            0.01000                                  6.0      9.000000  \n",
       "0.1               0.00100                                  7.0     10.333333  \n",
       "100.0             0.00001                                 14.0     10.333333  \n",
       "1.0               0.00001                                 18.0     11.333333  \n",
       "10.0              0.00100                                 10.0     12.000000  \n",
       "0.1               0.00001                                 17.0     12.000000  \n",
       "100.0             0.00100                                  9.0     13.000000  \n",
       "1000.0            0.00010                                 12.0     13.000000  \n",
       "10.0              0.00001                                 13.0     13.333333  \n",
       "1000.0            0.00100                                  5.0     13.666667  \n",
       "1.0               0.00010                                 16.0     14.000000  \n",
       "100.0             0.00010                                 15.0     14.333333  \n",
       "0.1               0.00010                                 20.0     15.000000  \n",
       "10.0              0.00010                                 11.0     15.666667  \n",
       "1000.0            0.00001                                 19.0     16.666667  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "gradient_clipping_values = [1e-1, 1, 10, 100, 1000]\n",
    "peak_learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# Generate all combinations of gradient clipping values and peak learning rates\n",
    "combinations = list(product(gradient_clipping_values, peak_learning_rates))\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for gradient_clip, peak_lr in tqdm(combinations, total=len(combinations)):\n",
    "    # Deep copy the default hyperparameters to test specific combinations\n",
    "    test_hyperparameters = copy.deepcopy(hyperparameters)\n",
    "    \n",
    "    # Set the specific hyperparameters\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'] = peak_lr\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'] = gradient_clip\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'] = max(1e-6, peak_lr * 0.01)\n",
    "    \n",
    "    # Initialize the model with the test hyperparameters\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    model_pre_train = IvySPT(\n",
    "        test_hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "        test_hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "        test_hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "        test_hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "        test_hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "        test_hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "        test_hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "        test_hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "        test_hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "        test_hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "        test_hyperparameters['Surface Encoding']['Gate Bias Inititalizer']\n",
    "    )\n",
    "    \n",
    "    # Initialize the trainer with the test hyperparameters\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)        \n",
    "    pre_trainer = Trainer(\n",
    "        model_pre_train,\n",
    "        pre_train_data_loader_train,\n",
    "        pre_train_data_loader_validation,\n",
    "        pre_train_data_loader_test,\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "        test_hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Train the model and get the validation loss components\n",
    "    _, _, validate_loss_components_history = pre_trainer.train()\n",
    "    \n",
    "    # Get the final validation losses for each component\n",
    "    final_validation_losses = validate_loss_components_history[-1]\n",
    "\n",
    "    print({\n",
    "        'Gradient Clipping': gradient_clip,\n",
    "        'Peak Learning Rate': peak_lr,\n",
    "        'MSE Loss': final_validation_losses[0],\n",
    "        'Calendar Arbitrage Loss': final_validation_losses[1],\n",
    "        'Butterfly Arbitrage Loss': final_validation_losses[2]\n",
    "    })\n",
    "    \n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        'Gradient Clipping': gradient_clip,\n",
    "        'Peak Learning Rate': peak_lr,\n",
    "        'MSE Loss': final_validation_losses[0],\n",
    "        'Calendar Arbitrage Loss': final_validation_losses[1],\n",
    "        'Butterfly Arbitrage Loss': final_validation_losses[2]\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create a multi-index DataFrame with gradient clipping and peak learning rate as the index\n",
    "results_df.set_index(['Gradient Clipping', 'Peak Learning Rate'], inplace=True)\n",
    "\n",
    "# Rank the columns, where the lowest loss is rank 1\n",
    "ranked_df = results_df.rank(axis=0, method='min', ascending=True)\n",
    "\n",
    "# Create an average rank column and sort by it\n",
    "ranked_df['Average Rank'] = ranked_df.mean(axis=1)\n",
    "ranked_df.sort_values(by='Average Rank', ascending=True, inplace=True)\n",
    "\n",
    "ranked_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE Loss</th>\n",
       "      <th>Calendar Arbitrage Loss</th>\n",
       "      <th>Butterfly Arbitrage Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Clipping</th>\n",
       "      <th>Peak Learning Rate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">0.1</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>0.007978</td>\n",
       "      <td>2.072742e-03</td>\n",
       "      <td>0.809661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00010</th>\n",
       "      <td>0.007412</td>\n",
       "      <td>6.455033e-03</td>\n",
       "      <td>4.924058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00100</th>\n",
       "      <td>0.018657</td>\n",
       "      <td>2.020784e-04</td>\n",
       "      <td>0.017323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01000</th>\n",
       "      <td>0.004239</td>\n",
       "      <td>2.214894e-06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>0.007578</td>\n",
       "      <td>1.170117e-03</td>\n",
       "      <td>1.227640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00010</th>\n",
       "      <td>0.010340</td>\n",
       "      <td>2.142539e-03</td>\n",
       "      <td>0.685150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00100</th>\n",
       "      <td>0.004121</td>\n",
       "      <td>2.127424e-04</td>\n",
       "      <td>0.068548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01000</th>\n",
       "      <td>0.004255</td>\n",
       "      <td>1.951189e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">10.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>0.008407</td>\n",
       "      <td>5.052258e-03</td>\n",
       "      <td>0.498610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00010</th>\n",
       "      <td>0.012325</td>\n",
       "      <td>7.740541e-03</td>\n",
       "      <td>0.295660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00100</th>\n",
       "      <td>0.007920</td>\n",
       "      <td>6.278710e-03</td>\n",
       "      <td>0.155835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01000</th>\n",
       "      <td>0.004416</td>\n",
       "      <td>3.473988e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">100.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>0.006996</td>\n",
       "      <td>2.342305e-03</td>\n",
       "      <td>0.503264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00010</th>\n",
       "      <td>0.009707</td>\n",
       "      <td>3.077520e-03</td>\n",
       "      <td>0.594433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00100</th>\n",
       "      <td>0.015612</td>\n",
       "      <td>2.391872e-03</td>\n",
       "      <td>0.145903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01000</th>\n",
       "      <td>0.006874</td>\n",
       "      <td>7.790407e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1000.0</th>\n",
       "      <th>0.00001</th>\n",
       "      <td>0.008307</td>\n",
       "      <td>1.293567e-02</td>\n",
       "      <td>2.268471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00010</th>\n",
       "      <td>0.009269</td>\n",
       "      <td>3.214235e-03</td>\n",
       "      <td>0.376327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00100</th>\n",
       "      <td>0.022456</td>\n",
       "      <td>6.239733e-03</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01000</th>\n",
       "      <td>0.009692</td>\n",
       "      <td>4.970033e-04</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      MSE Loss  Calendar Arbitrage Loss  \\\n",
       "Gradient Clipping Peak Learning Rate                                      \n",
       "0.1               0.00001             0.007978             2.072742e-03   \n",
       "                  0.00010             0.007412             6.455033e-03   \n",
       "                  0.00100             0.018657             2.020784e-04   \n",
       "                  0.01000             0.004239             2.214894e-06   \n",
       "1.0               0.00001             0.007578             1.170117e-03   \n",
       "                  0.00010             0.010340             2.142539e-03   \n",
       "                  0.00100             0.004121             2.127424e-04   \n",
       "                  0.01000             0.004255             1.951189e-07   \n",
       "10.0              0.00001             0.008407             5.052258e-03   \n",
       "                  0.00010             0.012325             7.740541e-03   \n",
       "                  0.00100             0.007920             6.278710e-03   \n",
       "                  0.01000             0.004416             3.473988e-07   \n",
       "100.0             0.00001             0.006996             2.342305e-03   \n",
       "                  0.00010             0.009707             3.077520e-03   \n",
       "                  0.00100             0.015612             2.391872e-03   \n",
       "                  0.01000             0.006874             7.790407e-07   \n",
       "1000.0            0.00001             0.008307             1.293567e-02   \n",
       "                  0.00010             0.009269             3.214235e-03   \n",
       "                  0.00100             0.022456             6.239733e-03   \n",
       "                  0.01000             0.009692             4.970033e-04   \n",
       "\n",
       "                                      Butterfly Arbitrage Loss  \n",
       "Gradient Clipping Peak Learning Rate                            \n",
       "0.1               0.00001                             0.809661  \n",
       "                  0.00010                             4.924058  \n",
       "                  0.00100                             0.017323  \n",
       "                  0.01000                             0.000000  \n",
       "1.0               0.00001                             1.227640  \n",
       "                  0.00010                             0.685150  \n",
       "                  0.00100                             0.068548  \n",
       "                  0.01000                             0.000000  \n",
       "10.0              0.00001                             0.498610  \n",
       "                  0.00010                             0.295660  \n",
       "                  0.00100                             0.155835  \n",
       "                  0.01000                             0.000000  \n",
       "100.0             0.00001                             0.503264  \n",
       "                  0.00010                             0.594433  \n",
       "                  0.00100                             0.145903  \n",
       "                  0.01000                             0.000000  \n",
       "1000.0            0.00001                             2.268471  \n",
       "                  0.00010                             0.376327  \n",
       "                  0.00100                             0.000010  \n",
       "                  0.01000                             0.000625  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "embedding_dims = [4, 8, 16, 32]\n",
    "num_blocks = [1, 2, 4, 8]\n",
    "peak_learning_rates = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# Generate all combinations of embedding dimensions, number of blocks, and peak learning rates\n",
    "combinations = list(product(embedding_dims, num_blocks, peak_learning_rates))\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for embedding_dim, blocks, peak_lr in tqdm(combinations, total=len(combinations)):\n",
    "    # Deep copy the default hyperparameters to test specific combinations\n",
    "    test_hyperparameters = copy.deepcopy(hyperparameters)\n",
    "    \n",
    "    # Set the specific hyperparameters\n",
    "    test_hyperparameters['Surface Embedding']['Embedding Dimension'] = embedding_dim\n",
    "    test_hyperparameters['Surface Encoding']['Number of Blocks'] = blocks\n",
    "    test_hyperparameters['Surface Encoding']['FFN Hidden Dimension'] = 4 * embedding_dim\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'] = peak_lr\n",
    "    \n",
    "    # Initialize the model with the test hyperparameters\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    model_pre_train = IvySPT(\n",
    "        test_hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "        test_hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "        test_hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "        test_hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "        test_hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "        test_hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "        test_hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "        test_hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "        test_hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "        test_hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "        test_hyperparameters['Surface Encoding']['Gate Bias Inititalizer']\n",
    "    )\n",
    "    \n",
    "    # Initialize the trainer with the test hyperparameters\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)        \n",
    "    pre_trainer = Trainer(\n",
    "        model_pre_train,\n",
    "        pre_train_data_loader_train,\n",
    "        pre_train_data_loader_validation,\n",
    "        pre_train_data_loader_test,\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "        test_hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "        test_hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Train the model and get the validation loss components\n",
    "    _, _, validate_loss_components_history = pre_trainer.train()\n",
    "    \n",
    "    # Get the final validation losses for each component\n",
    "    final_validation_losses = validate_loss_components_history[-1]\n",
    "    \n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        'Embedding Dimension': embedding_dim,\n",
    "        'Number of Blocks': blocks,\n",
    "        'Peak Learning Rate': peak_lr,\n",
    "        'MSE Loss': final_validation_losses[0],\n",
    "        'Calendar Arbitrage Loss': final_validation_losses[1],\n",
    "        'Butterfly Arbitrage Loss': final_validation_losses[2]\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create a multi-index DataFrame with embedding dimension, number of blocks, and peak learning rate as the index\n",
    "results_df.set_index(['Embedding Dimension', 'Number of Blocks', 'Peak Learning Rate'], inplace=True)\n",
    "\n",
    "# Rank the columns, where the lowest loss is rank 1\n",
    "ranked_df = results_df.rank(axis=0, method='min', ascending=True)\n",
    "\n",
    "# Create an average rank column and sort by it\n",
    "ranked_df['Average Rank'] = ranked_df.mean(axis=1)\n",
    "ranked_df.sort_values(by='Average Rank', ascending=True, inplace=True)\n",
    "\n",
    "ranked_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
