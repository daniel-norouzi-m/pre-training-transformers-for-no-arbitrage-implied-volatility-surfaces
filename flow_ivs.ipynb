{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference with Planar Flows for Implied Volatility Surfaces\n",
    "\n",
    "In this notebook, we will implement a variational autoencoder (VAE) with planar flows to model and fill implied volatility (IV) surfaces. We will use PyTorch to build and train the model. The key components include:\n",
    "1. Generating synthetic sample data for IV surfaces and features.\n",
    "2. Defining the Planar Flow layer to ensure invertibility.\n",
    "3. Creating the Encoder and Decoder networks.\n",
    "4. Implementing the VAE with planar flows.\n",
    "5. Training the model and evaluating its performance.\n",
    "\n",
    "## Imports\n",
    "\n",
    "First, we import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Generation\n",
    "\n",
    "We generate synthetic sample data for implied volatility (IV) surfaces and additional features. The IV surfaces are represented as grids with some missing values to simulate real-world scenarios.\n",
    "\n",
    "- `iv_grids`: A 10x10 grid of IV values.\n",
    "- `features`: Additional asset and market features.\n",
    "- `strike_times`: Strike prices and times to maturity for the options.\n",
    "\n",
    "We flatten the IV grids and concatenate them with the features and strike_times for the encoder input. The target IV points are the center points of the grids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sample data\n",
    "num_samples = 1000\n",
    "iv_grid_dim = (10, 10)  # 10x10 IV grid\n",
    "feature_dim = 5  # Number of asset and market features\n",
    "\n",
    "# Create random IV grids with some NaNs to simulate missing data\n",
    "iv_grids = np.random.rand(num_samples, *iv_grid_dim)\n",
    "iv_grids[np.random.rand(*iv_grids.shape) < 0.1] = np.nan  # Introduce some NaNs\n",
    "\n",
    "# Create random asset and market features\n",
    "features = np.random.rand(num_samples, feature_dim)\n",
    "\n",
    "# Create random spot prices, strike prices, and times to maturity in days\n",
    "spot_prices = np.random.rand(num_samples) * 100  # Example spot prices\n",
    "strike_prices = np.random.rand(num_samples) * 100  # Example strike prices\n",
    "time_to_maturity_days = np.random.randint(1, 365, num_samples)\n",
    "\n",
    "# Compute log moneyness and time to maturity in years\n",
    "log_moneyness = np.log(spot_prices / strike_prices)\n",
    "time_to_maturity_years = time_to_maturity_days / 365\n",
    "\n",
    "# Flatten the IV grids and concatenate with features for the encoder input\n",
    "iv_grids_flat = iv_grids.reshape(num_samples, -1)\n",
    "encoder_inputs = np.hstack((iv_grids_flat, features))\n",
    "\n",
    "# Prepare the target data and conditional features for the decoder\n",
    "targets = []\n",
    "cond_features = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for row in range(iv_grid_dim[0]):\n",
    "        for col in range(iv_grid_dim[1]):\n",
    "            if not np.isnan(iv_grids[i, row, col]):\n",
    "                targets.append(iv_grids[i, row, col])\n",
    "                cond_features.append(np.hstack((features[i], log_moneyness[i], time_to_maturity_years[i])))\n",
    "\n",
    "targets = np.array(targets)\n",
    "cond_features = np.array(cond_features)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "encoder_inputs = torch.tensor(encoder_inputs, dtype=torch.float32)\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.float32).unsqueeze(1)  # Make targets (num_targets, 1)\n",
    "cond_features_tensor = torch.tensor(cond_features, dtype=torch.float32)\n",
    "\n",
    "# Ensure encoder inputs are repeated for each target\n",
    "encoder_inputs_repeated = encoder_inputs.repeat_interleave(len(targets_tensor), dim=0)\n",
    "\n",
    "# Create data loader\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(encoder_inputs_repeated, targets_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create loader for conditioning features (features, log moneyness, and time to maturity)\n",
    "cond_features_loader = DataLoader(cond_features_tensor, batch_size=batch-size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planar Flow Layer\n",
    "\n",
    "We define the Planar Flow layer which modifies its parameters to ensure invertibility. This involves using the condition \\( w^T u \\geq -1 \\).\n",
    "\n",
    "- `PlanarFlow`: A class defining the planar flow transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.u = nn.Parameter(torch.randn(z_dim))\n",
    "        self.w = nn.Parameter(torch.randn(z_dim))\n",
    "        self.b = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, z):\n",
    "        linear = torch.matmul(z, self.w) + self.b\n",
    "        u_hat = self.u + (torch.log(1 + torch.exp(torch.matmul(self.w, self.u))) - 1 - torch.matmul(self.w, self.u)) * self.w / torch.norm(self.w, p=2)\n",
    "        activation = torch.tanh(linear)\n",
    "        z_next = z + u_hat * activation\n",
    "        psi = (1 - activation ** 2) * self.w\n",
    "        log_det_jacobian = torch.log(torch.abs(1 + torch.matmul(psi, u_hat.T)))\n",
    "        return z_next, log_det_jacobian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Network\n",
    "\n",
    "The encoder network takes the IV grid and additional features as input and outputs the parameters of the initial approximate posterior distribution.\n",
    "\n",
    "- `Encoder`: A class defining the encoder neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mu = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc2_mu(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "        return mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Network\n",
    "\n",
    "The decoder network takes the latent variable \\( z \\) and conditioning features (including strike price and time to maturity) to generate a single implied volatility point.\n",
    "\n",
    "- `Decoder`: A class defining the decoder neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, cond_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim + cond_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, cond_features):\n",
    "        x = torch.cat([z, cond_features], dim=-1)\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        iv = self.fc2(h)\n",
    "        return iv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE) with Planar Flows\n",
    "\n",
    "We integrate the encoder, multiple planar flows, and decoder into a VAE. The model uses the encoded input and planar flows to learn a flexible posterior distribution, and the decoder generates the IV points.\n",
    "\n",
    "- `VAE`: A class defining the variational autoencoder with planar flows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim, cond_dim, flow_length):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, z_dim)\n",
    "        self.flows = nn.ModuleList([PlanarFlow(z_dim) for _ in range(flow_length)])\n",
    "        self.decoder = Decoder(z_dim, cond_dim, hidden_dim, 1)  # output_dim=1 for single IV point\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, cond_features):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z_0 = self.reparameterize(mu, logvar)\n",
    "        z_k = z_0\n",
    "        log_det_jacobian_sum = 0\n",
    "        for flow in self.flows:\n",
    "            z_k, log_det_jacobian = flow(z_k)\n",
    "            log_det_jacobian_sum += log_det_jacobian\n",
    "        iv = self.decoder(z_k, cond_features)\n",
    "        return iv, mu, logvar, log_det_jacobian_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO Loss Function\n",
    "\n",
    "We define the Evidence Lower Bound (ELBO) loss function. The ELBO combines the reconstruction term (mean squared error between the predicted and actual IV points) and the KL divergence term (measuring the divergence between the approximate posterior and the prior).\n",
    "\n",
    "- `elbo_loss`: A function calculating the ELBO loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo_loss(reconstructed_iv, actual_iv, mu, logvar, log_det_jacobian_sum):\n",
    "    # Reconstruction loss (mean squared error)\n",
    "    recon_loss = nn.MSELoss()(reconstructed_iv, actual_iv)\n",
    "    \n",
    "    # KL Divergence\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # ELBO\n",
    "    elbo = recon_loss + kl_div - log_det_jacobian_sum.sum()\n",
    "    return elbo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "We implement the training loop to optimize the model parameters. The loop iterates through the data, performs forward and backward passes, and updates the model parameters using the Adam optimizer.\n",
    "\n",
    "- `train`: A function that trains the VAE model using the provided data loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, data_loader, cond_features_loader, epochs, learning_rate):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (x, iv) in enumerate(data_loader):\n",
    "            cond_features = next(iter(cond_features_loader))\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_iv, mu, logvar, log_det_jacobian_sum = model(x, cond_features)\n",
    "            loss = elbo_loss(reconstructed_iv, iv, mu, logvar, log_det_jacobian_sum)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Implementation Example\n",
    "\n",
    "Below is the full implementation example, combining all the components discussed above. This includes generating synthetic data, defining the model, and training it using the defined functions.\n",
    "\n",
    "- **Step 1:** Generate synthetic data for IV surfaces and features.\n",
    "- **Step 2:** Define the Planar Flow layer to ensure invertibility.\n",
    "- **Step 3:** Create the Encoder and Decoder networks.\n",
    "- **Step 4:** Implement the VAE with planar flows.\n",
    "- **Step 5:** Define the ELBO loss function.\n",
    "- **Step 6:** Train the model using the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_dim = encoder_inputs.shape[1]\n",
    "hidden_dim = 128\n",
    "z_dim = 32\n",
    "cond_dim = features_tensor.shape[1] + strike_times_tensor.shape[1]\n",
    "flow_length = 2\n",
    "\n",
    "# Instantiate the VAE model\n",
    "model = VAE(input_dim, hidden_dim, z_dim, cond_dim, flow_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Train the model using the defined train function\n",
    "train(model, data_loader, cond_features_loader, epochs, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cobra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
