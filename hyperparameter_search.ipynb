{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ivyspt.input_processing import split_surfaces, IVSurfaceDataset\n",
    "from ivyspt.trainer import Trainer\n",
    "from ivyspt.ivyspt import IvySPT\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_data = pd.read_csv('data/pre_train_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "fine_tune_data = pd.read_csv('data/fine_tune_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_train_surfaces_train, pre_train_surfaces_validation, pre_train_surfaces_test = split_surfaces(\n",
    "#     pre_train_data,\n",
    "#     toy_sample=True,\n",
    "#     max_points=400,\n",
    "#     max_surfaces=400,\n",
    "#     random_state=RANDOM_STATE\n",
    "# )\n",
    "# fine_tune_surfaces_train, fine_tune_surfaces_validation, fine_tune_surfaces_test = split_surfaces(\n",
    "#     fine_tune_data,\n",
    "#     toy_sample=True,\n",
    "#     max_points=400,\n",
    "#     max_surfaces=400,\n",
    "#     random_state=RANDOM_STATE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_train_surfaces_train, pre_train_surfaces_validation, pre_train_surfaces_test = split_surfaces(\n",
    "#     pre_train_data,\n",
    "#     toy_sample=True,\n",
    "#     max_points=200,\n",
    "#     max_surfaces=200,\n",
    "#     random_state=RANDOM_STATE\n",
    "# )\n",
    "# fine_tune_surfaces_train, fine_tune_surfaces_validation, fine_tune_surfaces_test = split_surfaces(\n",
    "#     fine_tune_data,\n",
    "#     toy_sample=True,\n",
    "#     max_points=200,\n",
    "#     max_surfaces=200,\n",
    "#     random_state=RANDOM_STATE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_train_surfaces_train, pre_train_surfaces_validation, pre_train_surfaces_test = split_surfaces(\n",
    "#     pre_train_data,\n",
    "#     toy_sample=True,\n",
    "#     max_points=100,\n",
    "#     max_surfaces=100,\n",
    "#     random_state=RANDOM_STATE\n",
    "# )\n",
    "# fine_tune_surfaces_train, fine_tune_surfaces_validation, fine_tune_surfaces_test = split_surfaces(\n",
    "#     fine_tune_data,\n",
    "#     toy_sample=True,\n",
    "#     max_points=100,\n",
    "#     max_surfaces=100,\n",
    "#     random_state=RANDOM_STATE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_surfaces_train, pre_train_surfaces_validation, pre_train_surfaces_test = split_surfaces(\n",
    "    pre_train_data,\n",
    "    toy_sample=True,\n",
    "    max_points=50,\n",
    "    max_surfaces=50,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "fine_tune_surfaces_train, fine_tune_surfaces_validation, fine_tune_surfaces_test = split_surfaces(\n",
    "    fine_tune_data,\n",
    "    toy_sample=True,\n",
    "    max_points=50,\n",
    "    max_surfaces=50,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Category    | Symbol | Count  | Sector/Type   |\n",
    "|-------------|--------|--------|---------------|\n",
    "| Pre-Train   | SPX    | 373,295| Index         |\n",
    "| Pre-Train   | GLD    | 346,588| Commodity     |\n",
    "| Pre-Train   | AAPL   | 306,950| Technology    |\n",
    "| Pre-Train   | TLT    | 161,764| Bond          |\n",
    "| Pre-Train   | XLE    | 138,507| Energy        |\n",
    "| Fine-Tune   | SCOR   | 5,300  | Technology    |\n",
    "| Fine-Tune   | AIN    | 5,288  | Industrial    |\n",
    "| Fine-Tune   | AZPN   | 5,284  | Software      |\n",
    "| Fine-Tune   | RWO    | 5,258  | Real Estate   |\n",
    "| Fine-Tune   | PNNT   | 5,232  | Financials    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Number of Query Points' : None,\n",
    "        'Batch Size' : 10\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_dataset_train = IVSurfaceDataset(\n",
    "    pre_train_surfaces_train, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_train = DataLoader(\n",
    "    pre_train_dataset_train, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "pre_train_dataset_validation = IVSurfaceDataset(\n",
    "    pre_train_surfaces_validation, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_validation = DataLoader(\n",
    "    pre_train_dataset_validation, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "pre_train_dataset_test = IVSurfaceDataset(\n",
    "    pre_train_surfaces_test, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_test = DataLoader(\n",
    "    pre_train_dataset_test, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_dataset_train = IVSurfaceDataset(\n",
    "    fine_tune_surfaces_train, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "fine_tune_data_loader_train = DataLoader(\n",
    "    fine_tune_dataset_train, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "fine_tune_dataset_validation = IVSurfaceDataset(\n",
    "    fine_tune_surfaces_validation, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "fine_tune_data_loader_validation = DataLoader(\n",
    "    fine_tune_dataset_validation, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "fine_tune_dataset_test = IVSurfaceDataset(\n",
    "    fine_tune_surfaces_test, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "fine_tune_data_loader_test = DataLoader(\n",
    "    fine_tune_dataset_test, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_hyperparameters = copy.deepcopy(hyperparameters)\n",
    "test_hyperparameters = {\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 32,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 128,\n",
    "        'Attention Dropout' : 0.,\n",
    "        'Gate Dropout' : 0.,\n",
    "        'FFN Dropout' : 0.,\n",
    "        'Number of Blocks' : 4,\n",
    "        'External Feature Dimension' : 5,\n",
    "        'Weight Initializer Std.' : 0.02,\n",
    "        'Linear Bias Initializer' : 0.0,\n",
    "        'Gate Bias Inititalizer' : 5.0\n",
    "    },\n",
    "    'Adaptive Loss Weights' : {\n",
    "        'Asymmetry' : 1.5,\n",
    "    },\n",
    "    'Trainer' : {\n",
    "        'Pre-Train' : {\n",
    "            'Number of Epochs' : 20,\n",
    "            'Warmup Ratio' : 0.15,\n",
    "            'Peak Learning Rate' : 1e-5,\n",
    "            'Minimal Learning Rate' : 1e-6,\n",
    "            'Gradient Clipping' : None,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : None,\n",
    "        },\n",
    "        'Fine-Tune' : {\n",
    "            'Number of Epochs' : 10,\n",
    "            'Warmup Ratio' : 0.1,\n",
    "            'Peak Learning Rate' : 1e-2,\n",
    "            'Minimal Learning Rate' : 1e-6,\n",
    "            'Gradient Clipping' : None,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : 0.9,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "model_pre_train = IvySPT(\n",
    "    test_hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "    test_hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "    test_hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "    test_hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "    test_hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "    test_hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "    test_hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "    test_hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "    test_hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "    test_hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "    test_hyperparameters['Surface Encoding']['Gate Bias Inititalizer'],\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pre_trainer = Trainer(\n",
    "    model_pre_train,\n",
    "    pre_train_data_loader_train,\n",
    "    pre_train_data_loader_validation,\n",
    "    pre_train_data_loader_test,\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "    test_hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "    test_hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "fine_tuner = Trainer(\n",
    "    model_pre_train,\n",
    "    fine_tune_data_loader_train,\n",
    "    fine_tune_data_loader_validation,\n",
    "    fine_tune_data_loader_test,\n",
    "    test_hyperparameters['Trainer']['Fine-Tune']['Number of Epochs'],\n",
    "    test_hyperparameters['Trainer']['Fine-Tune']['Warmup Ratio'],\n",
    "    test_hyperparameters['Trainer']['Fine-Tune']['Peak Learning Rate'],\n",
    "    test_hyperparameters['Trainer']['Fine-Tune']['Minimal Learning Rate'],\n",
    "    test_hyperparameters['Trainer']['Fine-Tune']['Gradient Clipping'],\n",
    "    test_hyperparameters['Trainer']['Fine-Tune']['Adam Betas'],\n",
    "    test_hyperparameters['Trainer']['Fine-Tune']['Adam Epsilon'],\n",
    "    test_hyperparameters['Trainer']['Fine-Tune']['Adam Weight Decay'],\n",
    "    test_hyperparameters['Trainer']['Fine-Tune']['Layer-Wise Decay'],\n",
    "    test_hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training Loss: [0.00370402 0.         0.        ], Validation Loss: [3.5611878e-03 3.4837864e-19 0.0000000e+00]\n",
      "Epoch 2/20 - Training Loss: [0.00388041 0.         0.        ], Validation Loss: [5.079069e-03 4.235155e-20 0.000000e+00]\n",
      "Epoch 3/20 - Training Loss: [0.00363348 0.         0.        ], Validation Loss: [4.9511744e-03 5.5982925e-20 0.0000000e+00]\n",
      "Epoch 4/20 - Training Loss: [0.00377841 0.         0.        ], Validation Loss: [4.6288371e-03 3.1831345e-20 0.0000000e+00]\n",
      "Epoch 5/20 - Training Loss: [0.00359821 0.         0.        ], Validation Loss: [5.2378997e-03 4.1957538e-20 0.0000000e+00]\n",
      "Epoch 6/20 - Training Loss: [0.00408735 0.         0.        ], Validation Loss: [5.2646231e-03 3.9977804e-20 0.0000000e+00]\n",
      "Epoch 7/20 - Training Loss: [0.00345339 0.         0.        ], Validation Loss: [3.1058704e-03 6.7368458e-20 0.0000000e+00]\n",
      "Epoch 8/20 - Training Loss: [0.00394519 0.         0.        ], Validation Loss: [3.1659254e-03 4.2108463e-20 0.0000000e+00]\n",
      "Epoch 9/20 - Training Loss: [0.00353453 0.         0.        ], Validation Loss: [2.9895650e-03 3.1509226e-20 0.0000000e+00]\n",
      "Epoch 10/20 - Training Loss: [0.00362624 0.         0.        ], Validation Loss: [5.1545328e-03 3.2940597e-20 0.0000000e+00]\n",
      "Epoch 11/20 - Training Loss: [0.0039504 0.        0.       ], Validation Loss: [4.9080276e-03 4.6542054e-20 0.0000000e+00]\n",
      "Epoch 12/20 - Training Loss: [0.00365207 0.         0.        ], Validation Loss: [4.8780954e-03 3.2093235e-20 0.0000000e+00]\n",
      "Epoch 13/20 - Training Loss: [0.00376895 0.         0.        ], Validation Loss: [5.3011458e-03 4.5464315e-20 0.0000000e+00]\n",
      "Epoch 14/20 - Training Loss: [0.0034578 0.        0.       ], Validation Loss: [2.7103769e-03 2.9274584e-20 0.0000000e+00]\n",
      "Epoch 15/20 - Training Loss: [0.00375824 0.         0.        ], Validation Loss: [5.4030665e-03 7.3573069e-20 0.0000000e+00]\n",
      "Epoch 16/20 - Training Loss: [0.00337495 0.         0.        ], Validation Loss: [4.6071848e-03 2.7674248e-20 0.0000000e+00]\n",
      "Epoch 17/20 - Training Loss: [0.00373353 0.         0.        ], Validation Loss: [4.7912607e-03 4.0229864e-20 0.0000000e+00]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pre_train_loss_coefficients_history, pre_train_train_loss_components_history, pre_train_validate_loss_components_history \u001b[38;5;241m=\u001b[39m \u001b[43mpre_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scikit_learn_data/implied-volatility-surface-with-transformers/ivyspt/trainer.py:168\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Reset gradients \u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 168\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m adaptive_loss_weights(train_loss_components, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfinal_layer)\n\u001b[1;32m    171\u001b[0m total_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pre_train_loss_coefficients_history, pre_train_train_loss_components_history, pre_train_validate_loss_components_history = pre_trainer.train(experiment_name='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
