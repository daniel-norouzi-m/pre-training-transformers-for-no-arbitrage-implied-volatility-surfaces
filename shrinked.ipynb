{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Number of Query Points' : 1,\n",
    "        'Batch Size' : 4\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 8,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 16,\n",
    "        'Attention Dropout' : 0.1,\n",
    "        'Gate Dropout' : 0.1,\n",
    "        'FFN Dropout' : 0.1,\n",
    "        'Number of Blocks' : 2,\n",
    "        'External Feature Dimension' : 3,\n",
    "    },\n",
    "    'No-Arbitrage' : {\n",
    "        'Butterfly' : 1,\n",
    "        'Calendar' : 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Log Moneyness</th>\n",
       "      <th>Time to Maturity</th>\n",
       "      <th>Implied Volatility</th>\n",
       "      <th>Market Return</th>\n",
       "      <th>Market Volatility</th>\n",
       "      <th>Treasury Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.316688</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.316688</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.304266</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.304266</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.291996</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-06-28</th>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.427518</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.434898</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.434898</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.442224</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.442224</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574326 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Log Moneyness  Time to Maturity  Implied Volatility  \\\n",
       "Datetime   Symbol                                                        \n",
       "2013-01-02 AAPL        -0.316688          0.007937              0.3726   \n",
       "           AAPL        -0.316688          0.007937              0.6095   \n",
       "           AAPL        -0.304266          0.007937              0.3726   \n",
       "           AAPL        -0.304266          0.007937              0.6095   \n",
       "           AAPL        -0.291996          0.007937              0.3726   \n",
       "...                          ...               ...                 ...   \n",
       "2013-06-28 GOOGL        0.427518          2.253968              0.2430   \n",
       "           GOOGL        0.434898          2.253968              0.2383   \n",
       "           GOOGL        0.434898          2.253968              0.2426   \n",
       "           GOOGL        0.442224          2.253968              0.2402   \n",
       "           GOOGL        0.442224          2.253968              0.2433   \n",
       "\n",
       "                   Market Return  Market Volatility  Treasury Rate  \n",
       "Datetime   Symbol                                                   \n",
       "2013-01-02 AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "...                          ...                ...            ...  \n",
       "2013-06-28 GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "\n",
       "[574326 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_googl_data = pd.read_csv('volatility_surface_AAPL_GOOGL_2013_01_2013_06.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "aapl_googl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': Timestamp('2013-01-02 00:00:00'),\n",
       " 'Symbol': 'AAPL',\n",
       " 'Market Features': {'Market Return': 0.0250861159586972,\n",
       "  'Market Volatility': 14.68000030517578,\n",
       "  'Treasury Rate': 0.0549999997019767},\n",
       " 'Surface': {'Log Moneyness': array([-0.31668849, -0.31668849, -0.30426597, ...,  0.63882295,\n",
       "          0.6483924 ,  0.6483924 ]),\n",
       "  'Time to Maturity': array([0.00793651, 0.00793651, 0.00793651, ..., 2.95634921, 2.95634921,\n",
       "         2.95634921]),\n",
       "  'Implied Volatility': array([0.3726, 0.6095, 0.3726, ..., 0.3387, 0.3342, 0.3389])}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def implied_volatility_surfaces(options_market_data):\n",
    "    # Group the data by Datetime and Symbol\n",
    "    grouped_data = options_market_data.groupby(level=['Datetime', 'Symbol'])\n",
    "\n",
    "    surfaces = []\n",
    "    for (date, symbol), surface in grouped_data:\n",
    "        surface_dict = {\n",
    "            'Datetime': date,\n",
    "            'Symbol': symbol,\n",
    "            'Market Features': {\n",
    "                'Market Return': surface['Market Return'].values[0],\n",
    "                'Market Volatility': surface['Market Volatility'].values[0],\n",
    "                'Treasury Rate': surface['Treasury Rate'].values[0],\n",
    "            },\n",
    "            'Surface': {\n",
    "                'Log Moneyness': surface['Log Moneyness'].values,\n",
    "                'Time to Maturity': surface['Time to Maturity'].values,\n",
    "                'Implied Volatility': surface['Implied Volatility'].values,\n",
    "            }\n",
    "        }\n",
    "        surfaces.append(surface_dict)\n",
    "\n",
    "    return surfaces\n",
    "\n",
    "surfaces = implied_volatility_surfaces(aapl_googl_data)\n",
    "surfaces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Padidar\\.conda\\envs\\Cobra\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Padidar\\.conda\\envs\\Cobra\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Padidar\\.conda\\envs\\Cobra\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Padidar\\.conda\\envs\\Cobra\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00'),\n",
       "  Timestamp('2013-03-07 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL', 'AAPL'],\n",
       " 'Mask Proportion': [0.7, 0.7, 0.1, 0.3],\n",
       " 'Market Features': {'Market Return': tensor([-0.0003, -0.0019, -0.0007,  0.0018]),\n",
       "  'Market Volatility': tensor([15.4400, 13.5700, 13.0200, 13.0600]),\n",
       "  'Treasury Rate': tensor([0.0400, 0.0600, 0.0350, 0.0900]),\n",
       "  'IV Mean': tensor([0.3143, 0.3340, 0.2723, 0.3624]),\n",
       "  'IV Std.': tensor([0.0559, 0.0639, 0.0500, 0.1235])},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-0.2852, -0.2553, -0.1573, -0.1310, -0.1181, -0.0928, -0.0804, -0.0804,\n",
       "           -0.0681, -0.0560, -0.0440, -0.0205,  0.0025,  0.0138,  0.0470,  0.0578,\n",
       "            0.0791,  0.0999,  0.1102,  0.1203,  0.1304,  0.1304,  0.1403,  0.1696,\n",
       "            0.1886,  0.1886,  0.1980,  0.2073, -0.7859, -0.7859, -0.7371, -0.6906,\n",
       "           -0.6906, -0.6462, -0.6247, -0.6036, -0.5430, -0.5236, -0.5045, -0.4858,\n",
       "           -0.4318, -0.4144, -0.3973, -0.3805, -0.3639, -0.3159, -0.3004, -0.3004,\n",
       "           -0.2553, -0.2263, -0.1981, -0.1707, -0.1707, -0.1573, -0.1181, -0.1054,\n",
       "           -0.0928, -0.0928, -0.0804, -0.0560, -0.0440, -0.0322, -0.0089,  0.0360,\n",
       "            0.0470,  0.0578,  0.0685,  0.0895,  0.1203,  0.1304,  0.1502,  0.1599,\n",
       "            0.1696,  0.1886,  0.1980,  0.2165,  0.2347,  0.2701,  0.2788,  0.2874,\n",
       "            0.3292,  0.3694,  0.3850,  0.3850,  0.3927,  0.3927,  0.4004,  0.4230,\n",
       "            0.4230,  0.4378,  0.4739,  0.4739,  0.5088,  0.5156,  0.5224,  0.5291,\n",
       "            0.5291,  0.5358,  0.5491,  0.5686,  0.5750,  0.5941,  0.6004,  0.6004,\n",
       "            0.6432,  0.6551,  0.6610,  0.6669,  0.6785,  0.6842,  0.6900,  0.6957,\n",
       "            0.6957,  0.7070,  0.7126,  0.7181,  0.7237,  0.7347,  0.7401,  0.7455,\n",
       "            0.7509,  0.7616,  0.7722,  0.7775,  0.7879, -0.3159, -0.2553, -0.2407,\n",
       "           -0.2263, -0.1707, -0.1441, -0.1310, -0.1181, -0.0928, -0.0804, -0.0681,\n",
       "           -0.0560, -0.0322, -0.0205, -0.0089,  0.0025,  0.0138,  0.0250,  0.0578,\n",
       "            0.0685,  0.1102,  0.1696,  0.1791,  0.1980,  0.2073,  0.2257,  0.2257,\n",
       "            0.2347,  0.2437,  0.2526,  0.2701, -0.1981, -0.1707, -0.1573, -0.1181,\n",
       "           -0.0928, -0.0928, -0.0804, -0.0089, -0.0089,  0.0025,  0.0138,  0.0250,\n",
       "            0.0791,  0.0895,  0.1102,  0.1203,  0.1304,  0.1403,  0.1403,  0.1502,\n",
       "            0.1886,  0.2073,  0.2165,  0.2257,  0.2257,  0.2347,  0.2526,  0.2614,\n",
       "           -0.6247, -0.6036, -0.5830, -0.5628, -0.5045, -0.4495, -0.4318, -0.4318,\n",
       "           -0.4144, -0.3477, -0.3477, -0.3159, -0.2852, -0.2701, -0.2701, -0.2553,\n",
       "           -0.2407, -0.2263, -0.1981, -0.1844, -0.1707, -0.1707, -0.1310, -0.1181,\n",
       "           -0.1181, -0.1054, -0.0928, -0.0560, -0.0560,  0.0025,  0.0138,  0.1304,\n",
       "            0.1403,  0.1980,  0.2347,  0.2701,  0.3043,  0.3127,  0.3455,  0.3694,\n",
       "            0.3850,  0.3927,  0.3927,  0.4155,  0.4305,  0.4524,  0.4524,  0.4810,\n",
       "            0.4880,  0.4950,  0.4950,  0.5019,  0.5088,  0.5156,  0.5224,  0.5358,\n",
       "            0.5425,  0.5491,  0.5491,  0.5750,  0.5814,  0.5878,  0.6066,  0.6066,\n",
       "            0.6128,  0.6189,  0.6251,  0.6372,  0.6372,  0.6492,  0.6551,  0.6669,\n",
       "            0.6727,  0.6785,  0.6785,  0.7126,  0.7237,  0.7292,  0.7401,  0.7455,\n",
       "            0.7509,  0.7563,  0.7616,  0.7616,  0.7669,  0.7722,  0.7775,  0.7930,\n",
       "            0.8084,  0.8135,  0.8185,  0.8335,  0.8433,  0.8433, -0.7859, -0.7371,\n",
       "           -0.7371, -0.7136, -0.6906, -0.6462, -0.6036, -0.5628, -0.5430, -0.4858,\n",
       "           -0.4675, -0.4495, -0.2407, -0.2263, -0.1981, -0.1844, -0.1310, -0.1181,\n",
       "           -0.1054, -0.0928, -0.0804, -0.0804, -0.0681, -0.0560, -0.0089, -0.0089,\n",
       "            0.0025,  0.0138,  0.0250,  0.0360,  0.0578,  0.0999,  0.1304,  0.1403,\n",
       "            0.1403,  0.1599,  0.1886,  0.1980,  0.2437,  0.2614,  0.2788,  0.3127,\n",
       "            0.3210,  0.3292,  0.3615,  0.3694,  0.3927, -0.6906, -0.6681, -0.6462,\n",
       "           -0.6036, -0.5830, -0.5628, -0.4495, -0.4318, -0.3805, -0.3639, -0.3317,\n",
       "           -0.3159, -0.3159, -0.2553, -0.2407, -0.2263, -0.1981, -0.1844, -0.1707,\n",
       "           -0.0804, -0.0681, -0.0322, -0.0205,  0.0025,  0.0025,  0.0138,  0.0360,\n",
       "            0.0470,  0.0578,  0.0685,  0.0791,  0.0895,  0.0895,  0.0999,  0.1102,\n",
       "            0.1502,  0.1886,  0.1980,  0.2073,  0.2165,  0.2347,  0.2437,  0.2959,\n",
       "            0.2959,  0.3043,  0.3127,  0.3210,  0.3694,  0.3772,  0.3927,  0.4155,\n",
       "            0.4155,  0.4230,  0.4230, -0.6036, -0.5236, -0.4675, -0.4495, -0.4495,\n",
       "           -0.3973, -0.3805, -0.3639, -0.3477, -0.3317, -0.3159, -0.3159, -0.3004,\n",
       "           -0.2701, -0.2553, -0.2263, -0.2263, -0.1981, -0.1844, -0.1844, -0.1707,\n",
       "           -0.1707, -0.1441, -0.1441, -0.1181, -0.0681, -0.0560, -0.0440, -0.0322,\n",
       "           -0.0205, -0.0205, -0.0089,  0.0025,  0.0578,  0.0895,  0.1102,  0.1304,\n",
       "            0.1403,  0.1502,  0.1599,  0.1696,  0.1791,  0.1980,  0.2257,  0.2347,\n",
       "            0.2614,  0.3043,  0.3127,  0.3292,  0.3374,  0.3374,  0.3455,  0.3535,\n",
       "            0.3615,  0.3772,  0.3850,  0.3927,  0.4080,  0.4080,  0.4155,  0.4305,\n",
       "            0.4378,  0.4452,  0.4524,  0.4524,  0.4668,  0.4739,  0.4810,  0.4880,\n",
       "            0.5019,  0.5088,  0.5088,  0.5224,  0.5491,  0.5556,  0.5621,  0.5621,\n",
       "            0.5750,  0.5750,  0.5814,  0.5878,  0.6066,  0.6128,  0.6189,  0.6311,\n",
       "            0.6372,  0.6372,  0.6492,  0.6842,  0.6842,  0.6900,  0.6957, -0.7612,\n",
       "           -0.7136, -0.6681, -0.6462, -0.6247, -0.6036, -0.5628, -0.5430, -0.5045,\n",
       "           -0.4858, -0.4675, -0.4675, -0.4318, -0.4144, -0.3805, -0.3805, -0.3317,\n",
       "           -0.3159, -0.3004, -0.2852, -0.2701, -0.2407, -0.2263, -0.2121, -0.1981,\n",
       "           -0.1844, -0.1844, -0.1573, -0.1310, -0.0804, -0.0440, -0.0205,  0.0025,\n",
       "            0.0250,  0.0999,  0.1203,  0.1304,  0.1696,  0.1886,  0.1980,  0.2257,\n",
       "            0.2347,  0.2614,  0.2701,  0.2788,  0.2874,  0.3374,  0.3455,  0.3694,\n",
       "            0.3772,  0.3927,  0.4004,  0.4230,  0.4305,  0.4378,  0.4452,  0.4452,\n",
       "            0.4524,  0.4597,  0.4597,  0.4880,  0.5019,  0.5088,  0.5224,  0.5358,\n",
       "            0.6128,  0.6189,  0.6251,  0.6311,  0.6372,  0.6432,  0.6432,  0.6727,\n",
       "            0.6785,  0.6842,  0.6957,  0.7237,  0.7347,  0.7401,  0.7401,  0.7509,\n",
       "            0.7509,  0.7563,  0.7616,  0.7669,  0.7879,  0.7982,  0.8033,  0.8033,\n",
       "            0.8335,  0.8433,  0.8531,  0.8627, -0.7612, -0.6247, -0.5236, -0.5045,\n",
       "           -0.5045, -0.4858, -0.4495, -0.4495, -0.3805, -0.3639, -0.3639, -0.3317,\n",
       "           -0.3159, -0.3159, -0.3004, -0.2701, -0.2701, -0.2553, -0.2263, -0.1981,\n",
       "           -0.1981, -0.1573, -0.1310, -0.1181, -0.0681, -0.0560, -0.0440, -0.0205,\n",
       "           -0.0089,  0.0025,  0.0138,  0.0250,  0.0250,  0.0360,  0.0578,  0.0685,\n",
       "            0.0791,  0.0999,  0.1403,  0.1599,  0.1791,  0.1980,  0.2073,  0.2165,\n",
       "            0.2257,  0.2347,  0.2437,  0.2526,  0.2614,  0.2788,  0.2788,  0.2959,\n",
       "            0.2959,  0.3043,  0.3210,  0.3292,  0.3615,  0.3694,  0.3850,  0.3927,\n",
       "            0.4004,  0.4004,  0.4080,  0.4230, -0.8112, -0.8112, -0.7371, -0.7136,\n",
       "           -0.6036, -0.5628, -0.5628, -0.4495, -0.3477, -0.2263, -0.1981, -0.1844,\n",
       "           -0.1573, -0.1573, -0.1441, -0.1054, -0.0804, -0.0440, -0.0205,  0.0250,\n",
       "            0.0360,  0.0685,  0.0791,  0.0999,  0.1102,  0.1403,  0.1502,  0.1599,\n",
       "            0.1599,  0.1886,  0.1980,  0.2257,  0.2614,  0.3772,  0.3927,  0.4080,\n",
       "            0.4230,  0.4378,  0.4524,  0.4668,  0.5224,  0.5358,  0.5358,  0.6372,\n",
       "            0.6727,  0.6957,  0.7070,  0.7181,  0.7616,  0.7722,  0.7827,  0.7827,\n",
       "            0.7930,  0.8033,  0.8235,  0.8335,  0.8433,  0.8723,  0.8723]),\n",
       "   tensor([-2.0897e-01, -1.5563e-01, -1.4272e-01, -6.8615e-02, -2.2095e-02,\n",
       "           -1.0795e-02,  3.7784e-04,  1.1428e-02,  1.1428e-02,  2.2357e-02,\n",
       "            7.5279e-02,  9.5688e-02,  1.0574e-01,  1.5453e-01,  1.6401e-01,\n",
       "            1.9192e-01,  1.9192e-01,  2.0105e-01,  2.1907e-01,  2.2796e-01,\n",
       "            2.2796e-01,  2.4550e-01,  2.7125e-01,  2.7969e-01,  2.8806e-01,\n",
       "            3.2085e-01,  3.5260e-01,  3.6810e-01, -1.4272e-01, -1.4272e-01,\n",
       "           -1.2998e-01, -1.1741e-01, -1.0498e-01, -6.8615e-02, -6.8615e-02,\n",
       "           -5.6781e-02,  2.2357e-02,  3.3168e-02,  3.3168e-02,  5.4445e-02,\n",
       "            6.4916e-02,  8.5536e-02,  9.5688e-02,  1.2554e-01,  1.3530e-01,\n",
       "            1.3530e-01,  1.5453e-01,  1.6401e-01,  2.0105e-01,  2.1907e-01,\n",
       "            2.2796e-01,  2.7125e-01,  2.8806e-01, -5.2914e-01, -4.7408e-01,\n",
       "           -4.5638e-01, -4.0509e-01, -3.7230e-01, -2.9474e-01, -2.6533e-01,\n",
       "           -2.2277e-01, -2.0897e-01, -1.8194e-01, -1.6870e-01, -1.5563e-01,\n",
       "           -1.0498e-01, -9.2713e-02, -8.0591e-02, -5.6781e-02,  2.2357e-02,\n",
       "            3.3168e-02,  5.4445e-02,  5.4445e-02,  8.5536e-02,  9.5688e-02,\n",
       "            1.2554e-01,  1.2554e-01,  1.3530e-01,  1.3530e-01,  1.4496e-01,\n",
       "            1.8270e-01,  2.1010e-01,  2.2796e-01,  2.2796e-01,  2.3677e-01,\n",
       "            2.4550e-01,  2.5416e-01,  2.8806e-01,  3.0459e-01,  3.1275e-01,\n",
       "            3.4476e-01,  3.6810e-01,  3.7577e-01,  3.8337e-01,  3.9092e-01,\n",
       "            3.9841e-01,  3.9841e-01,  4.0584e-01,  4.2055e-01,  4.2782e-01,\n",
       "            4.4933e-01,  4.7730e-01,  4.9100e-01,  4.9778e-01,  5.0451e-01,\n",
       "            5.1120e-01,  5.1785e-01,  5.1785e-01,  5.2445e-01,  5.6316e-01,\n",
       "            5.8197e-01,  5.8816e-01,  5.9432e-01,  6.0651e-01,  6.1256e-01,\n",
       "            6.1256e-01,  6.1856e-01,  6.3047e-01,  6.4806e-01,  6.5386e-01,\n",
       "            6.5962e-01,  6.6535e-01,  6.6535e-01,  6.7105e-01,  6.7672e-01,\n",
       "            6.7672e-01,  6.9353e-01,  7.0457e-01,  7.1550e-01,  7.1550e-01,\n",
       "            7.2092e-01,  7.3168e-01,  7.3168e-01,  7.4232e-01,  7.4759e-01,\n",
       "            7.5284e-01,  7.5806e-01,  7.7357e-01,  7.7868e-01,  7.8377e-01,\n",
       "            7.8884e-01,  7.9387e-01,  8.2844e-01, -1.4272e-01, -6.8615e-02,\n",
       "           -6.8615e-02, -4.5085e-02, -2.2095e-02,  5.4445e-02,  5.4445e-02,\n",
       "            6.4916e-02,  7.5279e-02,  9.5688e-02,  1.1569e-01,  1.1569e-01,\n",
       "            1.2554e-01,  1.5453e-01,  1.6401e-01,  1.9192e-01,  1.9192e-01,\n",
       "            2.0105e-01,  2.1010e-01,  2.3677e-01, -2.2095e-02,  3.7784e-04,\n",
       "            3.7784e-04,  2.2357e-02,  3.3168e-02,  9.5688e-02,  1.0574e-01,\n",
       "            1.1569e-01,  1.2554e-01,  1.3530e-01,  1.5453e-01,  1.7340e-01,\n",
       "            1.8270e-01,  2.0105e-01,  2.1010e-01,  2.3677e-01,  2.4550e-01,\n",
       "            2.7125e-01,  2.7125e-01, -5.4819e-01, -5.1045e-01, -4.9210e-01,\n",
       "           -4.9210e-01, -4.7408e-01, -4.5638e-01, -4.5638e-01, -4.3899e-01,\n",
       "           -2.9474e-01, -2.7992e-01, -2.7992e-01, -2.5094e-01, -2.5094e-01,\n",
       "           -2.3675e-01, -2.3675e-01, -2.2277e-01, -2.2277e-01, -1.9537e-01,\n",
       "           -1.5563e-01, -1.4272e-01, -1.2998e-01, -1.0498e-01, -5.6781e-02,\n",
       "           -2.2095e-02, -2.2095e-02, -1.0795e-02,  3.3168e-02,  4.3863e-02,\n",
       "            4.3863e-02,  6.4916e-02,  6.4916e-02,  8.5536e-02,  9.5688e-02,\n",
       "            1.0574e-01,  1.2554e-01,  1.4496e-01,  1.6401e-01,  1.8270e-01,\n",
       "            2.1010e-01,  2.6274e-01,  2.6274e-01,  2.7125e-01,  2.7125e-01,\n",
       "            2.8806e-01,  2.8806e-01,  3.0459e-01,  3.3685e-01,  3.4476e-01,\n",
       "            3.4476e-01,  3.7577e-01,  3.8337e-01,  3.9092e-01,  4.1322e-01,\n",
       "            4.3504e-01,  4.4221e-01,  4.4221e-01,  4.5640e-01,  4.6341e-01,\n",
       "            4.7038e-01,  4.7730e-01,  4.7730e-01,  4.8417e-01,  4.8417e-01,\n",
       "            4.9100e-01,  5.2445e-01,  5.3101e-01,  5.3752e-01,  5.4399e-01,\n",
       "            5.5042e-01,  5.7574e-01,  5.9432e-01,  6.0651e-01,  6.1256e-01,\n",
       "            6.3637e-01,  6.5386e-01,  6.8235e-01,  6.8235e-01,  7.1005e-01,\n",
       "           -5.4819e-01, -4.7408e-01, -4.5638e-01, -4.3899e-01, -4.2189e-01,\n",
       "           -3.5630e-01, -3.4055e-01, -2.7992e-01, -2.6533e-01, -2.3675e-01,\n",
       "           -2.3675e-01, -2.2277e-01, -2.2277e-01, -2.0897e-01, -1.9537e-01,\n",
       "           -1.6870e-01, -1.2998e-01, -1.1741e-01, -9.2713e-02, -8.0591e-02,\n",
       "           -3.3524e-02,  3.7784e-04,  2.2357e-02,  2.2357e-02,  3.3168e-02,\n",
       "            4.3863e-02,  5.4445e-02,  7.5279e-02,  8.5536e-02,  8.5536e-02,\n",
       "            9.5688e-02,  1.6401e-01,  1.7340e-01,  1.7340e-01,  1.8270e-01,\n",
       "            1.9192e-01,  2.0105e-01,  2.4550e-01,  2.6274e-01,  2.7125e-01,\n",
       "            2.7969e-01,  2.7969e-01,  2.8806e-01,  2.9636e-01,  3.4476e-01,\n",
       "            3.4476e-01,  3.5260e-01,  3.6810e-01,  3.7577e-01,  3.8337e-01,\n",
       "            4.0584e-01,  4.2055e-01,  4.3504e-01,  4.4221e-01,  4.4933e-01,\n",
       "            4.5640e-01,  4.7730e-01,  4.9778e-01,  5.0451e-01,  5.1785e-01,\n",
       "            5.2445e-01,  5.2445e-01,  5.3101e-01,  5.3101e-01,  5.4399e-01,\n",
       "            5.7574e-01,  5.8197e-01,  6.0043e-01,  6.0651e-01,  6.1256e-01,\n",
       "            6.1856e-01,  6.3047e-01,  6.4806e-01,  6.5962e-01,  6.7672e-01,\n",
       "            6.7672e-01,  6.8795e-01,  6.8795e-01,  7.1550e-01,  7.1550e-01,\n",
       "            7.2092e-01,  7.2631e-01,  7.3168e-01,  7.4759e-01,  7.6326e-01,\n",
       "            7.6843e-01,  7.9889e-01,  8.0884e-01,  8.0884e-01,  8.1869e-01,\n",
       "            8.2844e-01,  8.4768e-01, -5.6761e-01, -5.1045e-01, -5.1045e-01,\n",
       "           -4.7408e-01, -4.5638e-01, -4.3899e-01, -4.2189e-01, -4.0509e-01,\n",
       "           -3.5630e-01, -3.5630e-01, -3.4055e-01, -2.9474e-01, -2.2277e-01,\n",
       "           -1.9537e-01, -1.8194e-01, -1.8194e-01, -1.6870e-01, -1.4272e-01,\n",
       "           -1.2998e-01, -1.1741e-01, -1.0498e-01, -8.0591e-02, -1.0795e-02,\n",
       "            3.3168e-02,  3.3168e-02,  6.4916e-02,  7.5279e-02,  9.5688e-02,\n",
       "            1.0574e-01,  1.2554e-01,  1.3530e-01,  1.4496e-01,  1.6401e-01,\n",
       "            2.0105e-01,  2.1010e-01,  2.1010e-01,  2.1907e-01,  2.2796e-01,\n",
       "            2.4550e-01,  2.5416e-01,  2.6274e-01,  2.7969e-01,  3.2888e-01,\n",
       "            3.2888e-01,  3.3685e-01,  3.4476e-01,  3.6038e-01,  3.6810e-01,\n",
       "            3.7577e-01,  3.8337e-01,  3.9092e-01,  4.0584e-01,  4.0584e-01,\n",
       "            4.1322e-01,  4.2055e-01,  4.5640e-01,  4.6341e-01,  4.7038e-01,\n",
       "            4.7730e-01,  4.8417e-01,  4.9778e-01,  4.9778e-01,  5.0451e-01,\n",
       "            5.0451e-01,  5.1120e-01, -5.4819e-01, -5.2914e-01, -5.1045e-01,\n",
       "           -4.9210e-01, -4.7408e-01, -4.5638e-01, -4.3899e-01, -3.7230e-01,\n",
       "           -3.4055e-01, -3.4055e-01, -3.0978e-01, -2.9474e-01, -2.5094e-01,\n",
       "           -2.3675e-01, -2.2277e-01, -2.2277e-01, -1.8194e-01, -1.6870e-01,\n",
       "           -1.5563e-01, -1.4272e-01, -1.2998e-01, -1.1741e-01, -1.1741e-01,\n",
       "           -9.2713e-02, -6.8615e-02, -5.6781e-02, -4.5085e-02, -2.2095e-02,\n",
       "           -2.2095e-02,  2.2357e-02,  4.3863e-02,  8.5536e-02,  9.5688e-02,\n",
       "            9.5688e-02,  1.0574e-01,  1.0574e-01,  1.1569e-01,  1.2554e-01,\n",
       "            1.3530e-01,  1.6401e-01,  1.8270e-01,  2.3677e-01,  2.3677e-01,\n",
       "            2.4550e-01,  2.7969e-01,  2.8806e-01,  2.8806e-01,  2.9636e-01,\n",
       "            3.0459e-01,  3.1275e-01,  3.2888e-01,  3.3685e-01,  3.9092e-01,\n",
       "            4.0584e-01,  4.1322e-01,  4.2782e-01,  4.4221e-01,  4.5640e-01,\n",
       "            4.7038e-01,  4.7730e-01,  4.8417e-01,  4.9100e-01,  4.9100e-01,\n",
       "            5.0451e-01,  5.2445e-01,  5.3101e-01,  5.3752e-01,  5.6316e-01,\n",
       "            5.6947e-01,  5.8197e-01,  5.8816e-01,  5.9432e-01,  6.0043e-01,\n",
       "            6.1256e-01,  6.2453e-01,  6.2453e-01,  6.3637e-01,  6.4223e-01,\n",
       "            6.4223e-01,  6.4806e-01,  6.7672e-01,  6.7672e-01,  6.8235e-01,\n",
       "            6.8235e-01,  6.9353e-01,  6.9353e-01,  6.9907e-01,  6.9907e-01,\n",
       "            7.0457e-01,  7.1005e-01,  7.2092e-01,  7.2631e-01,  7.4232e-01,\n",
       "            7.4759e-01,  7.4759e-01,  7.5284e-01,  7.5806e-01,  7.6326e-01,\n",
       "           -5.4819e-01, -5.2914e-01, -4.7408e-01, -4.5638e-01, -4.3899e-01,\n",
       "           -4.2189e-01, -4.0509e-01, -4.0509e-01, -3.8856e-01, -3.8856e-01,\n",
       "           -3.5630e-01, -3.2504e-01, -3.0978e-01, -2.6533e-01, -2.0897e-01,\n",
       "           -1.5563e-01, -1.5563e-01, -1.2998e-01, -1.1741e-01, -1.0498e-01,\n",
       "           -1.0498e-01, -9.2713e-02, -6.8615e-02, -5.6781e-02, -4.5085e-02,\n",
       "           -1.0795e-02,  1.1428e-02,  2.2357e-02,  4.3863e-02,  6.4916e-02,\n",
       "            7.5279e-02,  8.5536e-02,  1.2554e-01,  1.6401e-01,  1.7340e-01,\n",
       "            1.8270e-01,  1.9192e-01,  2.0105e-01,  2.1010e-01,  2.1010e-01,\n",
       "            2.3677e-01,  2.5416e-01,  3.0459e-01,  3.4476e-01,  3.6810e-01,\n",
       "            3.7577e-01,  3.8337e-01,  3.9092e-01,  3.9092e-01,  3.9841e-01,\n",
       "            3.9841e-01,  4.0584e-01,  4.2055e-01,  4.2782e-01,  4.3504e-01,\n",
       "            4.4221e-01,  4.4933e-01,  4.4933e-01,  4.7038e-01,  4.7730e-01,\n",
       "            4.9778e-01,  4.9778e-01,  5.2445e-01,  5.3752e-01,  5.5042e-01,\n",
       "            5.5681e-01,  5.7574e-01,  5.7574e-01,  5.9432e-01,  6.1256e-01,\n",
       "            6.4806e-01,  6.5386e-01,  6.8235e-01,  6.8795e-01,  6.9353e-01,\n",
       "            6.9353e-01,  7.0457e-01,  7.1005e-01,  7.1005e-01,  7.2092e-01,\n",
       "            7.4759e-01,  7.5806e-01,  7.5806e-01,  7.6326e-01,  7.6843e-01,\n",
       "            7.6843e-01,  7.7357e-01,  7.8884e-01,  7.8884e-01,  7.9889e-01,\n",
       "            8.1869e-01,  8.1869e-01,  8.2844e-01, -5.4819e-01, -5.2914e-01,\n",
       "           -4.5638e-01, -3.8856e-01, -3.4055e-01, -3.4055e-01, -3.2504e-01,\n",
       "           -3.0978e-01, -2.9474e-01, -2.9474e-01, -2.7992e-01, -2.6533e-01,\n",
       "           -2.3675e-01, -1.9537e-01, -1.6870e-01, -1.5563e-01, -1.4272e-01,\n",
       "           -1.4272e-01, -8.0591e-02, -6.8615e-02,  1.1428e-02,  3.3168e-02,\n",
       "            3.3168e-02,  6.4916e-02,  7.5279e-02,  8.5536e-02,  9.5688e-02,\n",
       "            1.1569e-01,  1.2554e-01,  1.3530e-01,  1.4496e-01,  1.7340e-01,\n",
       "            1.8270e-01,  1.9192e-01,  2.2796e-01,  2.5416e-01,  2.6274e-01,\n",
       "            2.8806e-01,  2.9636e-01,  3.0459e-01,  3.0459e-01,  3.1275e-01,\n",
       "            3.2085e-01,  3.2888e-01,  3.3685e-01,  3.6038e-01,  3.6810e-01,\n",
       "            3.6810e-01,  3.7577e-01,  3.7577e-01,  3.9841e-01,  4.2055e-01,\n",
       "            4.5640e-01,  4.6341e-01,  4.7038e-01,  4.7730e-01,  4.8417e-01,\n",
       "            4.9778e-01,  5.0451e-01,  5.1120e-01,  5.1785e-01,  5.3752e-01,\n",
       "            5.6316e-01,  6.0043e-01,  6.1256e-01,  6.1856e-01,  6.4806e-01,\n",
       "            6.4806e-01,  6.5962e-01,  6.7105e-01, -6.2823e-01, -6.0761e-01,\n",
       "           -6.0761e-01, -5.8741e-01, -5.4819e-01, -5.4819e-01, -5.2914e-01,\n",
       "           -4.9210e-01, -4.7408e-01, -4.7408e-01, -4.5638e-01, -4.3899e-01,\n",
       "           -4.3899e-01, -4.2189e-01, -4.2189e-01, -4.0509e-01, -4.0509e-01,\n",
       "           -3.8856e-01, -3.7230e-01, -3.0978e-01, -2.6533e-01, -2.5094e-01,\n",
       "           -2.2277e-01, -1.6870e-01, -1.4272e-01, -1.4272e-01, -1.2998e-01,\n",
       "           -1.1741e-01, -1.0498e-01, -9.2713e-02, -8.0591e-02, -5.6781e-02,\n",
       "           -5.6781e-02, -1.0795e-02,  5.4445e-02,  6.4916e-02,  7.5279e-02,\n",
       "            7.5279e-02,  1.0574e-01,  1.1569e-01,  1.2554e-01,  1.7340e-01,\n",
       "            1.9192e-01,  2.0105e-01,  2.2796e-01,  2.2796e-01,  2.3677e-01,\n",
       "            2.4550e-01,  2.9636e-01,  3.1275e-01,  3.2085e-01,  3.2888e-01,\n",
       "            3.4476e-01,  3.4476e-01,  3.6038e-01,  3.6038e-01,  3.6810e-01,\n",
       "            3.9841e-01,  3.9841e-01,  4.2782e-01,  4.3504e-01,  4.5640e-01,\n",
       "            4.6341e-01,  4.7730e-01,  4.8417e-01,  4.9778e-01,  5.0451e-01,\n",
       "            5.4399e-01,  5.5681e-01,  5.5681e-01,  5.6316e-01,  5.7574e-01,\n",
       "            5.8197e-01,  5.9432e-01,  6.0043e-01,  6.0651e-01,  6.1856e-01,\n",
       "            6.3637e-01,  6.3637e-01,  6.4806e-01,  6.5962e-01,  6.6535e-01,\n",
       "            6.7105e-01,  6.7105e-01,  6.8235e-01,  7.1005e-01,  7.2092e-01,\n",
       "            7.2631e-01,  7.3168e-01,  7.3168e-01,  7.3701e-01,  7.5284e-01,\n",
       "            7.6843e-01,  7.8884e-01,  7.9387e-01,  8.1869e-01,  8.2844e-01,\n",
       "            8.2844e-01,  8.3811e-01, -6.7079e-01, -6.2823e-01, -5.8741e-01,\n",
       "           -5.1045e-01, -4.3899e-01, -4.0509e-01, -2.5094e-01, -2.2277e-01,\n",
       "           -1.9537e-01, -1.4272e-01, -9.2713e-02, -9.2713e-02, -6.8615e-02,\n",
       "           -2.2095e-02,  3.7784e-04,  6.4916e-02,  8.5536e-02,  1.0574e-01,\n",
       "            1.2554e-01,  1.4496e-01,  1.6401e-01,  2.3677e-01,  2.5416e-01,\n",
       "            2.7125e-01,  3.0459e-01,  3.3685e-01,  3.5260e-01,  3.6810e-01,\n",
       "            3.8337e-01,  3.9841e-01,  4.1322e-01,  4.2782e-01,  4.7038e-01,\n",
       "            4.8417e-01,  6.0043e-01,  6.1256e-01,  6.4806e-01,  6.8235e-01,\n",
       "            6.8235e-01,  6.9353e-01,  6.9353e-01,  7.0457e-01,  7.1550e-01,\n",
       "            7.3701e-01,  7.3701e-01,  7.7868e-01,  7.8884e-01]),\n",
       "   tensor([-0.3741, -0.3741, -0.3661,  ...,  0.4034,  0.4107,  0.4107]),\n",
       "   tensor([-0.1790, -0.1790, -0.1652,  ...,  0.8818,  0.8914,  0.8914])],\n",
       "  'Time to Maturity': [tensor([0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254]),\n",
       "   tensor([0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437,\n",
       "           0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437,\n",
       "           0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270,\n",
       "           0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270,\n",
       "           0.1270, 0.1270, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532]),\n",
       "   tensor([0.0159, 0.0159, 0.0159,  ..., 2.4087, 2.4087, 2.4087]),\n",
       "   tensor([0.0040, 0.0040, 0.0040,  ..., 2.7024, 2.7024, 2.7024])],\n",
       "  'Implied Volatility': [tensor([0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.4187, 0.3716,\n",
       "           0.3716, 0.3617, 0.3363, 0.3212, 0.3147, 0.3133, 0.3133, 0.3716, 0.3133,\n",
       "           0.3716, 0.3133, 0.3716, 0.3133, 0.3133, 0.3133, 0.3716, 0.3133, 0.3716,\n",
       "           0.3716, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473,\n",
       "           0.3473, 0.3258, 0.3258, 0.3258, 0.3258, 0.3473, 0.3473, 0.3258, 0.3258,\n",
       "           0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3473, 0.3473,\n",
       "           0.3258, 0.3258, 0.3180, 0.3207, 0.3086, 0.2900, 0.2851, 0.2781, 0.2698,\n",
       "           0.2607, 0.2637, 0.2668, 0.2688, 0.2859, 0.3138, 0.2561, 0.2561, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295,\n",
       "           0.3295, 0.3295, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295,\n",
       "           0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.3295, 0.2561, 0.3295, 0.2561,\n",
       "           0.2561, 0.3295, 0.2561, 0.3295, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.3295, 0.3381,\n",
       "           0.3381, 0.3381, 0.3381, 0.3265, 0.3632, 0.3381, 0.3216, 0.3165, 0.3037,\n",
       "           0.3004, 0.2912, 0.2812, 0.2803, 0.2746, 0.2723, 0.2720, 0.2671, 0.2695,\n",
       "           0.2617, 0.2847, 0.2702, 0.2702, 0.3356, 0.2702, 0.3356, 0.2702, 0.3356,\n",
       "           0.2702, 0.2702, 0.3356, 0.3564, 0.3187, 0.3187, 0.3002, 0.2968, 0.2920,\n",
       "           0.2829, 0.2660, 0.2642, 0.2607, 0.2606, 0.2599, 0.2623, 0.2655, 0.2648,\n",
       "           0.2754, 0.2797, 0.2871, 0.2661, 0.2920, 0.2661, 0.3215, 0.2661, 0.3215,\n",
       "           0.2661, 0.3215, 0.3215, 0.2661, 0.3369, 0.3510, 0.3510, 0.3369, 0.3369,\n",
       "           0.3369, 0.3510, 0.3369, 0.3369, 0.3510, 0.3369, 0.3510, 0.3510, 0.3510,\n",
       "           0.3369, 0.3369, 0.3369, 0.3369, 0.3295, 0.3397, 0.3226, 0.3172, 0.3031,\n",
       "           0.2975, 0.2933, 0.2938, 0.2866, 0.2749, 0.2728, 0.2622, 0.2596, 0.2658,\n",
       "           0.2692, 0.2912, 0.3146, 0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.3302, 0.3302, 0.3302,\n",
       "           0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302,\n",
       "           0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.2695, 0.3302, 0.3302, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.3302, 0.3302, 0.2695, 0.3695,\n",
       "           0.5483, 0.3695, 0.3695, 0.5483, 0.5483, 0.5483, 0.5483, 0.5483, 0.3695,\n",
       "           0.5483, 0.3695, 0.3469, 0.4183, 0.3310, 0.3267, 0.3105, 0.3345, 0.3049,\n",
       "           0.3031, 0.3168, 0.3001, 0.2975, 0.3078, 0.2949, 0.2905, 0.2926, 0.2908,\n",
       "           0.2887, 0.2866, 0.2852, 0.2911, 0.2942, 0.2847, 0.2983, 0.2876, 0.2911,\n",
       "           0.3135, 0.3053, 0.3091, 0.3461, 0.3318, 0.3461, 0.3386, 0.3461, 0.3415,\n",
       "           0.3461, 0.3459, 0.6848, 0.3459, 0.6848, 0.6642, 0.3459, 0.5333, 0.5164,\n",
       "           0.4740, 0.4519, 0.4324, 0.4189, 0.3381, 0.3202, 0.3164, 0.3511, 0.3059,\n",
       "           0.3018, 0.2989, 0.2883, 0.2808, 0.2774, 0.2753, 0.2727, 0.2728, 0.2728,\n",
       "           0.2691, 0.2710, 0.2698, 0.2669, 0.2671, 0.2664, 0.2710, 0.2712, 0.2666,\n",
       "           0.2672, 0.2711, 0.2715, 0.2728, 0.2745, 0.2770, 0.2782, 0.2864, 0.3104,\n",
       "           0.3104, 0.2921, 0.3104, 0.3076, 0.3133, 0.3104, 0.3256, 0.3104, 0.3273,\n",
       "           0.3104, 0.3472, 0.3472, 0.3472, 0.3822, 0.3472, 0.3472, 0.3822, 0.3422,\n",
       "           0.3822, 0.3320, 0.3775, 0.3280, 0.3672, 0.3508, 0.3103, 0.3265, 0.3036,\n",
       "           0.2970, 0.3053, 0.2938, 0.3040, 0.2912, 0.2958, 0.2864, 0.2824, 0.2764,\n",
       "           0.2747, 0.2725, 0.2711, 0.2698, 0.2707, 0.2691, 0.2686, 0.2639, 0.2656,\n",
       "           0.2663, 0.2630, 0.2670, 0.2628, 0.2658, 0.2692, 0.2642, 0.2661, 0.2684,\n",
       "           0.2775, 0.2869, 0.2849, 0.2849, 0.2853, 0.2869, 0.2849, 0.2849, 0.2919,\n",
       "           0.2849, 0.3020, 0.3043, 0.2849, 0.3108, 0.2849, 0.3125, 0.2849, 0.3207,\n",
       "           0.2849, 0.3229, 0.2849, 0.3229, 0.3229, 0.3124, 0.2849, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.6576, 0.6189, 0.5936, 0.3469, 0.3469, 0.5412,\n",
       "           0.3469, 0.3469, 0.3469, 0.4520, 0.4388, 0.3434, 0.4219, 0.3313, 0.3866,\n",
       "           0.3244, 0.3141, 0.3109, 0.3080, 0.3051, 0.3263, 0.3153, 0.2955, 0.2940,\n",
       "           0.3014, 0.2983, 0.2901, 0.2870, 0.2879, 0.2810, 0.2771, 0.2740, 0.2726,\n",
       "           0.2743, 0.2690, 0.2725, 0.2678, 0.2671, 0.2746, 0.2683, 0.2779, 0.2691,\n",
       "           0.2702, 0.2804, 0.2854, 0.2824, 0.2965, 0.2942, 0.2804, 0.3037, 0.2857,\n",
       "           0.3085, 0.3089, 0.2902, 0.2918, 0.2932, 0.3089, 0.3089, 0.2964, 0.3089,\n",
       "           0.3042, 0.3057, 0.3089, 0.3089, 0.3116, 0.3116, 0.3089, 0.3116, 0.3089,\n",
       "           0.3116, 0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3116, 0.3116,\n",
       "           0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116,\n",
       "           0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3439, 0.3439, 0.4487,\n",
       "           0.4355, 0.3331, 0.4234, 0.4020, 0.3242, 0.3154, 0.3551, 0.3130, 0.3100,\n",
       "           0.3391, 0.3078, 0.3058, 0.3209, 0.3031, 0.3157, 0.2985, 0.3037, 0.2965,\n",
       "           0.2930, 0.2948, 0.2915, 0.2874, 0.2867, 0.2854, 0.2849, 0.2848, 0.2818,\n",
       "           0.2839, 0.2803, 0.2834, 0.2811, 0.2824, 0.2822, 0.2819, 0.2779, 0.2814,\n",
       "           0.2811, 0.2763, 0.2819, 0.2824, 0.2767, 0.2829, 0.2817, 0.2824, 0.2765,\n",
       "           0.2772, 0.2783, 0.2858, 0.2779, 0.2890, 0.2871, 0.2882, 0.2902, 0.2938,\n",
       "           0.2975, 0.3004, 0.2849, 0.2853, 0.2980, 0.3009, 0.2999, 0.5636, 0.3514,\n",
       "           0.3463, 0.5072, 0.3388, 0.4173, 0.3351, 0.3262, 0.3204, 0.3153, 0.3135,\n",
       "           0.3134, 0.3101, 0.3126, 0.3115, 0.3105, 0.3050, 0.3026, 0.3019, 0.3061,\n",
       "           0.3052, 0.2983, 0.3050, 0.2977, 0.3053, 0.3043, 0.2967, 0.2968, 0.3043,\n",
       "           0.3043, 0.3040, 0.2955, 0.2960, 0.3071, 0.3091, 0.3101, 0.2967, 0.3110,\n",
       "           0.2989, 0.3118, 0.3167, 0.3020, 0.3136, 0.3194, 0.3105, 0.3119, 0.3129,\n",
       "           0.3137, 0.3182, 0.3192, 0.3202, 0.3194, 0.3194, 0.3232, 0.3194, 0.3194,\n",
       "           0.3284, 0.3320, 0.3194]),\n",
       "   tensor([0.4484, 0.6109, 0.5840, 0.4310, 0.3847, 0.3745, 0.3700, 0.3681, 0.3691,\n",
       "           0.3676, 0.4731, 0.4101, 0.4101, 0.4731, 0.4731, 0.4101, 0.4731, 0.4101,\n",
       "           0.4101, 0.4101, 0.4731, 0.4731, 0.4101, 0.4101, 0.4731, 0.4731, 0.4101,\n",
       "           0.4731, 0.6166, 0.3907, 0.5844, 0.5625, 0.5080, 0.4146, 0.3476, 0.3420,\n",
       "           0.3454, 0.3265, 0.3439, 0.3683, 0.3367, 0.3893, 0.4189, 0.3943, 0.4067,\n",
       "           0.3968, 0.4312, 0.3968, 0.4799, 0.3968, 0.3968, 0.3968, 0.4799, 0.3801,\n",
       "           0.7247, 0.3801, 0.3801, 0.3801, 0.3801, 0.7247, 0.3801, 0.3801, 0.5664,\n",
       "           0.5411, 0.5014, 0.3375, 0.3299, 0.3253, 0.3174, 0.2933, 0.2945, 0.2957,\n",
       "           0.3209, 0.3369, 0.3101, 0.3313, 0.3544, 0.3365, 0.3711, 0.3510, 0.4041,\n",
       "           0.4277, 0.4456, 0.4041, 0.4583, 0.4041, 0.4751, 0.4041, 0.4041, 0.5174,\n",
       "           0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174,\n",
       "           0.5174, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.5174, 0.4041,\n",
       "           0.5174, 0.4041, 0.5174, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041,\n",
       "           0.5174, 0.4041, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041,\n",
       "           0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174,\n",
       "           0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4398,\n",
       "           0.3205, 0.3120, 0.3063, 0.3011, 0.2909, 0.3002, 0.2918, 0.2923, 0.2980,\n",
       "           0.3056, 0.3182, 0.3390, 0.3309, 0.3597, 0.3585, 0.3597, 0.3597, 0.3597,\n",
       "           0.4029, 0.3010, 0.2962, 0.2972, 0.2978, 0.2974, 0.2949, 0.2967, 0.2858,\n",
       "           0.3025, 0.2893, 0.2893, 0.3239, 0.3283, 0.3410, 0.2893, 0.3656, 0.2893,\n",
       "           0.3919, 0.2893, 0.3481, 0.5191, 0.3962, 0.3481, 0.3962, 0.3962, 0.3481,\n",
       "           0.3962, 0.3962, 0.3962, 0.3055, 0.3962, 0.3347, 0.3962, 0.3481, 0.3962,\n",
       "           0.3493, 0.3711, 0.3167, 0.3389, 0.3262, 0.3045, 0.2948, 0.2862, 0.2920,\n",
       "           0.2843, 0.2819, 0.2831, 0.2904, 0.2848, 0.2935, 0.2975, 0.2964, 0.3026,\n",
       "           0.2938, 0.2957, 0.3013, 0.3090, 0.3151, 0.3437, 0.3151, 0.3525, 0.3151,\n",
       "           0.3578, 0.3151, 0.3578, 0.3578, 0.3578, 0.3151, 0.3578, 0.3578, 0.3578,\n",
       "           0.3151, 0.3151, 0.3578, 0.3151, 0.3578, 0.3151, 0.3151, 0.3578, 0.3151,\n",
       "           0.3578, 0.3151, 0.3578, 0.3151, 0.3151, 0.3151, 0.3578, 0.3578, 0.3151,\n",
       "           0.3578, 0.3151, 0.3578, 0.3151, 0.3151, 0.3578, 0.3151, 0.3578, 0.3632,\n",
       "           0.3632, 0.3355, 0.3632, 0.3632, 0.3632, 0.3632, 0.3570, 0.3396, 0.3379,\n",
       "           0.3194, 0.3372, 0.3153, 0.3263, 0.3226, 0.3101, 0.2963, 0.2949, 0.2899,\n",
       "           0.2829, 0.2792, 0.2771, 0.2748, 0.2811, 0.2805, 0.2752, 0.2750, 0.2756,\n",
       "           0.2759, 0.2822, 0.2824, 0.2831, 0.2851, 0.2893, 0.2856, 0.2894, 0.2945,\n",
       "           0.3025, 0.3017, 0.3123, 0.3165, 0.3017, 0.3017, 0.3229, 0.3434, 0.3017,\n",
       "           0.3494, 0.3017, 0.3017, 0.3017, 0.3017, 0.3017, 0.3017, 0.3017, 0.3784,\n",
       "           0.3784, 0.3784, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017,\n",
       "           0.3017, 0.3017, 0.3784, 0.3017, 0.3017, 0.3784, 0.3017, 0.3017, 0.3017,\n",
       "           0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3017, 0.3784,\n",
       "           0.3784, 0.3784, 0.3784, 0.3784, 0.3784, 0.3784, 0.3017, 0.3784, 0.3017,\n",
       "           0.3017, 0.4610, 0.4610, 0.3484, 0.4610, 0.3484, 0.4610, 0.4610, 0.4610,\n",
       "           0.4352, 0.3484, 0.4214, 0.3835, 0.3245, 0.3356, 0.3331, 0.3173, 0.3161,\n",
       "           0.3128, 0.3110, 0.3098, 0.3111, 0.3037, 0.2954, 0.2920, 0.3008, 0.3012,\n",
       "           0.2899, 0.2902, 0.2903, 0.2909, 0.3035, 0.3054, 0.3075, 0.3150, 0.2965,\n",
       "           0.3164, 0.3156, 0.2999, 0.3271, 0.3032, 0.3041, 0.3284, 0.3222, 0.3284,\n",
       "           0.3284, 0.3284, 0.3323, 0.3335, 0.3284, 0.3284, 0.3443, 0.3511, 0.3284,\n",
       "           0.3284, 0.3563, 0.3689, 0.3284, 0.3284, 0.3678, 0.3678, 0.3678, 0.3284,\n",
       "           0.3678, 0.3284, 0.3678, 0.3532, 0.3532, 0.3306, 0.3532, 0.3439, 0.3532,\n",
       "           0.3496, 0.3976, 0.3716, 0.3346, 0.3605, 0.3439, 0.3260, 0.3109, 0.3226,\n",
       "           0.3096, 0.3047, 0.3021, 0.3030, 0.2977, 0.2967, 0.2965, 0.2939, 0.2891,\n",
       "           0.2838, 0.2848, 0.2870, 0.2803, 0.2878, 0.2769, 0.2834, 0.2861, 0.2746,\n",
       "           0.2861, 0.2755, 0.2883, 0.2761, 0.2859, 0.2773, 0.2866, 0.2802, 0.2879,\n",
       "           0.2990, 0.2888, 0.3016, 0.2943, 0.3016, 0.2969, 0.3016, 0.3020, 0.3016,\n",
       "           0.3016, 0.3205, 0.3016, 0.3016, 0.3016, 0.3378, 0.3016, 0.3468, 0.3016,\n",
       "           0.3514, 0.3543, 0.3016, 0.3016, 0.3634, 0.3016, 0.3701, 0.3016, 0.3016,\n",
       "           0.3016, 0.3701, 0.3701, 0.3701, 0.3016, 0.3701, 0.3016, 0.3701, 0.3701,\n",
       "           0.3016, 0.3016, 0.3701, 0.3016, 0.3701, 0.3016, 0.3701, 0.3016, 0.3701,\n",
       "           0.3016, 0.3701, 0.3701, 0.3701, 0.3016, 0.3016, 0.3701, 0.3016, 0.3016,\n",
       "           0.3701, 0.3701, 0.3792, 0.3792, 0.3370, 0.3370, 0.3792, 0.3792, 0.3792,\n",
       "           0.3396, 0.3792, 0.3317, 0.3646, 0.3232, 0.3353, 0.3108, 0.3024, 0.2949,\n",
       "           0.2960, 0.2912, 0.2892, 0.2854, 0.2883, 0.2882, 0.2810, 0.2792, 0.2779,\n",
       "           0.2822, 0.2813, 0.2804, 0.2774, 0.2708, 0.2776, 0.2806, 0.2712, 0.2810,\n",
       "           0.2725, 0.2717, 0.2726, 0.2839, 0.2728, 0.2844, 0.2910, 0.2922, 0.3057,\n",
       "           0.2926, 0.2963, 0.2988, 0.3015, 0.3033, 0.3048, 0.3048, 0.3048, 0.3054,\n",
       "           0.3048, 0.3048, 0.3151, 0.3163, 0.3184, 0.3048, 0.3237, 0.3048, 0.3270,\n",
       "           0.3048, 0.3048, 0.3270, 0.3270, 0.3270, 0.3270, 0.3048, 0.3048, 0.3048,\n",
       "           0.3048, 0.3270, 0.3048, 0.3048, 0.3270, 0.3048, 0.3270, 0.3270, 0.3048,\n",
       "           0.3048, 0.3270, 0.3270, 0.3048, 0.3048, 0.3270, 0.3048, 0.3048, 0.3270,\n",
       "           0.3048, 0.3270, 0.3270, 0.3048, 0.3270, 0.3353, 0.4503, 0.3976, 0.3554,\n",
       "           0.3313, 0.3114, 0.3227, 0.3004, 0.3160, 0.3030, 0.3108, 0.2994, 0.3003,\n",
       "           0.2927, 0.2922, 0.2859, 0.2833, 0.2873, 0.2784, 0.2795, 0.2796, 0.2671,\n",
       "           0.2767, 0.2678, 0.2733, 0.2733, 0.2736, 0.2738, 0.2758, 0.2655, 0.2632,\n",
       "           0.2650, 0.2648, 0.2740, 0.2646, 0.2733, 0.2788, 0.2654, 0.2667, 0.2674,\n",
       "           0.2800, 0.2654, 0.2663, 0.2717, 0.2693, 0.2933, 0.2736, 0.2938, 0.2710,\n",
       "           0.2938, 0.2938, 0.2938, 0.2768, 0.2938, 0.2938, 0.2768, 0.2768, 0.2768,\n",
       "           0.2938, 0.2768, 0.2938, 0.2938, 0.2768, 0.2938, 0.2768, 0.2938, 0.2768,\n",
       "           0.2938, 0.2938, 0.2768, 0.3342, 0.3372, 0.3342, 0.3372, 0.3372, 0.3358,\n",
       "           0.3372, 0.3372, 0.3372, 0.3249, 0.3230, 0.3372, 0.3163, 0.3372, 0.3168,\n",
       "           0.3372, 0.3153, 0.3152, 0.3371, 0.3165, 0.2974, 0.2988, 0.2975, 0.2848,\n",
       "           0.2836, 0.2914, 0.2810, 0.2888, 0.2811, 0.2795, 0.2785, 0.2798, 0.2858,\n",
       "           0.2748, 0.2819, 0.2708, 0.2707, 0.2804, 0.2700, 0.2693, 0.2781, 0.2813,\n",
       "           0.2773, 0.2774, 0.2671, 0.2769, 0.2775, 0.2758, 0.2832, 0.2672, 0.2676,\n",
       "           0.2839, 0.2695, 0.2829, 0.2695, 0.2864, 0.2839, 0.2675, 0.2876, 0.2929,\n",
       "           0.2945, 0.2675, 0.2675, 0.2675, 0.2675, 0.2675, 0.2950, 0.2950, 0.2675,\n",
       "           0.2950, 0.2675, 0.2950, 0.2675, 0.2950, 0.2675, 0.2950, 0.2675, 0.2675,\n",
       "           0.2950, 0.2675, 0.2675, 0.2675, 0.2675, 0.2950, 0.2675, 0.2950, 0.2950,\n",
       "           0.2950, 0.2675, 0.2950, 0.2950, 0.2675, 0.2675, 0.2950, 0.2950, 0.2675,\n",
       "           0.2675, 0.2950, 0.2675, 0.3212, 0.3202, 0.3227, 0.3557, 0.3240, 0.3173,\n",
       "           0.3066, 0.3065, 0.3026, 0.2992, 0.2980, 0.3065, 0.3047, 0.3008, 0.3022,\n",
       "           0.2924, 0.2913, 0.2924, 0.2901, 0.2987, 0.2901, 0.2985, 0.3005, 0.2996,\n",
       "           0.2899, 0.2895, 0.3009, 0.2889, 0.3008, 0.2866, 0.3024, 0.2901, 0.3017,\n",
       "           0.3054, 0.2897, 0.2923, 0.3134, 0.2913, 0.3084, 0.2950, 0.3084, 0.2916,\n",
       "           0.2916, 0.2916, 0.3084, 0.2916, 0.2916]),\n",
       "   tensor([0.3347, 0.3682, 0.3347,  ..., 0.2350, 0.2302, 0.2355]),\n",
       "   tensor([0.5199, 0.7914, 0.5199,  ..., 0.3147, 0.2958, 0.3147])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([0.3210], requires_grad=True),\n",
       "   tensor([-0.2090], requires_grad=True),\n",
       "   tensor([-0.6174], requires_grad=True),\n",
       "   tensor([0.0661], requires_grad=True)],\n",
       "  'Time to Maturity': [tensor([0.5198], requires_grad=True),\n",
       "   tensor([0.0754], requires_grad=True),\n",
       "   tensor([0.8532], requires_grad=True),\n",
       "   tensor([0.1746], requires_grad=True)],\n",
       "  'Implied Volatility': [tensor([0.2849]),\n",
       "   tensor([0.6361]),\n",
       "   tensor([0.2944]),\n",
       "   tensor([0.2965])]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "class IVSurfaceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data, \n",
    "        mask_proportions, \n",
    "        random_state=0,\n",
    "        n_query_points=None\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.mask_proportions = mask_proportions\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        self.n_query_points = n_query_points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        surface_data = self.data[idx]\n",
    "        \n",
    "        # Extract the surface coordinates and volatilities\n",
    "        points_coordinates = np.stack([\n",
    "            surface_data['Surface']['Log Moneyness'], \n",
    "            surface_data['Surface']['Time to Maturity']\n",
    "        ], axis=1)\n",
    "        points_volatilities = surface_data['Surface']['Implied Volatility']\n",
    "\n",
    "        # Select a random mask proportion\n",
    "        proportion = self.rng.choice(self.mask_proportions)\n",
    "\n",
    "        # Perform clustering\n",
    "        n_clusters = int(np.ceil(1 / proportion))\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('kmeans', KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init='auto'))\n",
    "        ])\n",
    "        labels = pipeline.fit_predict(points_coordinates)\n",
    "        masked_indices = np.array([], dtype=int)\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_indices = np.where(labels == cluster)[0]\n",
    "            num_to_mask = int(np.ceil(len(cluster_indices) * proportion))\n",
    "            masked_indices = np.append(masked_indices, [self.rng.choice(cluster_indices, size=num_to_mask, replace=False)])\n",
    "        \n",
    "        unmasked_indices = np.setdiff1d(range(len(labels)), masked_indices)\n",
    "\n",
    "        # Calculate IV mean and std for unmasked points\n",
    "        iv_mean = np.mean(points_volatilities[unmasked_indices])\n",
    "        iv_std = np.std(points_volatilities[unmasked_indices])\n",
    "\n",
    "        # Define query indices based on n_query_points\n",
    "        if self.n_query_points is None:\n",
    "            query_indices = masked_indices\n",
    "        else:\n",
    "            query_indices = self.rng.choice(masked_indices, size=self.n_query_points, replace=False)\n",
    "            \n",
    "        data_item = {\n",
    "            'Datetime': surface_data['Datetime'],\n",
    "            'Symbol': surface_data['Symbol'],\n",
    "            'Mask Proportion': proportion,\n",
    "            'Market Features': {\n",
    "                'Market Return': torch.tensor(surface_data['Market Features']['Market Return'], dtype=torch.float32),\n",
    "                'Market Volatility': torch.tensor(surface_data['Market Features']['Market Volatility'], dtype=torch.float32),\n",
    "                'Treasury Rate': torch.tensor(surface_data['Market Features']['Treasury Rate'], dtype=torch.float32),\n",
    "                'IV Mean': torch.tensor(iv_mean, dtype=torch.float32),\n",
    "                'IV Std.': torch.tensor(iv_std, dtype=torch.float32),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[unmasked_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[unmasked_indices, 1], dtype=torch.float32),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[unmasked_indices], dtype=torch.float32)\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[query_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[query_indices, 1], dtype=torch.float32),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[query_indices], dtype=torch.float32)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return data_item\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batched_data = {\n",
    "            'Datetime': [item['Datetime'] for item in batch],\n",
    "            'Symbol': [item['Symbol'] for item in batch],\n",
    "            'Mask Proportion': [item['Mask Proportion'] for item in batch],\n",
    "            'Market Features': {\n",
    "                'Market Return': default_collate([item['Market Features']['Market Return'] for item in batch]),\n",
    "                'Market Volatility': default_collate([item['Market Features']['Market Volatility'] for item in batch]),\n",
    "                'Treasury Rate': default_collate([item['Market Features']['Treasury Rate'] for item in batch]),\n",
    "                'IV Mean': default_collate([item['Market Features']['IV Mean'] for item in batch]),\n",
    "                'IV Std.': default_collate([item['Market Features']['IV Std.'] for item in batch]),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': [item['Input Surface']['Log Moneyness'].clone().detach() for item in batch],\n",
    "                'Time to Maturity': [item['Input Surface']['Time to Maturity'].clone().detach() for item in batch],\n",
    "                'Implied Volatility': [item['Input Surface']['Implied Volatility'].clone().detach() for item in batch],\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': [item['Query Points']['Log Moneyness'].clone().detach().requires_grad_(True) for item in batch],\n",
    "                'Time to Maturity': [item['Query Points']['Time to Maturity'].clone().detach().requires_grad_(True) for item in batch],\n",
    "                'Implied Volatility': [item['Query Points']['Implied Volatility'].clone().detach() for item in batch],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "# Assuming surfaces is the output from the implied_volatility_surfaces function\n",
    "mask_proportions = HYPERPARAMETERS['Input Preprocessing']['Mask Proportions']  \n",
    "n_query_points = HYPERPARAMETERS['Input Preprocessing']['Number of Query Points']  \n",
    "dataset = IVSurfaceDataset(surfaces, mask_proportions, RANDOM_STATE, n_query_points)\n",
    "data_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=HYPERPARAMETERS['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Fetch one batch from the DataLoader\n",
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00'),\n",
       "  Timestamp('2013-03-07 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL', 'AAPL'],\n",
       " 'Mask Proportion': [0.7, 0.7, 0.1, 0.3],\n",
       " 'Market Features': {'Market Return': tensor([-0.0216, -0.4603, -0.1269,  0.6087], grad_fn=<SqueezeBackward1>),\n",
       "  'Market Volatility': tensor([ 1.6897, -0.2052, -0.7625, -0.7220], grad_fn=<SqueezeBackward1>),\n",
       "  'Treasury Rate': tensor([-0.7439,  0.1717, -0.9728,  1.5450], grad_fn=<SqueezeBackward1>),\n",
       "  'IV Mean': tensor([-0.1952,  0.4019, -1.4712,  1.2645], grad_fn=<SqueezeBackward1>),\n",
       "  'IV Std.': tensor([-0.5892, -0.3177, -0.7905,  1.6974], grad_fn=<SqueezeBackward1>)},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-0.8178, -0.7456, -0.5087, -0.4450, -0.4138, -0.3526, -0.3225, -0.3225,\n",
       "           -0.2929, -0.2636, -0.2346, -0.1777, -0.1221, -0.0948, -0.0146,  0.0115,\n",
       "            0.0630,  0.1134,  0.1382,  0.1627,  0.1870,  0.1870,  0.2111,  0.2819,\n",
       "            0.3280,  0.3280,  0.3507,  0.3732, -2.0288, -2.0288, -1.9108, -1.7983,\n",
       "           -1.7983, -1.6908, -1.6388, -1.5879, -1.4413, -1.3943, -1.3483, -1.3031,\n",
       "           -1.1723, -1.1303, -1.0889, -1.0483, -1.0083, -0.8922, -0.8547, -0.8547,\n",
       "           -0.7456, -0.6755, -0.6074, -0.5411, -0.5411, -0.5087, -0.4138, -0.3830,\n",
       "           -0.3526, -0.3526, -0.3225, -0.2636, -0.2346, -0.2060, -0.1497, -0.0410,\n",
       "           -0.0146,  0.0115,  0.0374,  0.0883,  0.1627,  0.1870,  0.2349,  0.2585,\n",
       "            0.2819,  0.3280,  0.3507,  0.3954,  0.4394,  0.5250,  0.5460,  0.5667,\n",
       "            0.6679,  0.7651,  0.8028,  0.8028,  0.8215,  0.8215,  0.8401,  0.8948,\n",
       "            0.8948,  0.9306,  1.0179,  1.0179,  1.1022,  1.1187,  1.1351,  1.1514,\n",
       "            1.1514,  1.1676,  1.1996,  1.2469,  1.2624,  1.3085,  1.3237,  1.3237,\n",
       "            1.4272,  1.4560,  1.4703,  1.4844,  1.5126,  1.5265,  1.5404,  1.5541,\n",
       "            1.5541,  1.5815,  1.5950,  1.6085,  1.6219,  1.6485,  1.6616,  1.6747,\n",
       "            1.6878,  1.7136,  1.7392,  1.7519,  1.7771, -0.8922, -0.7456, -0.7103,\n",
       "           -0.6755, -0.5411, -0.4766, -0.4450, -0.4138, -0.3526, -0.3225, -0.2929,\n",
       "           -0.2636, -0.2060, -0.1777, -0.1497, -0.1221, -0.0948, -0.0677,  0.0115,\n",
       "            0.0374,  0.1382,  0.2819,  0.3050,  0.3507,  0.3732,  0.4175,  0.4175,\n",
       "            0.4394,  0.4611,  0.4826,  0.5250, -0.6074, -0.5411, -0.5087, -0.4138,\n",
       "           -0.3526, -0.3526, -0.3225, -0.1497, -0.1497, -0.1221, -0.0948, -0.0677,\n",
       "            0.0630,  0.0883,  0.1382,  0.1627,  0.1870,  0.2111,  0.2111,  0.2349,\n",
       "            0.3280,  0.3732,  0.3954,  0.4175,  0.4175,  0.4394,  0.4826,  0.5039,\n",
       "           -1.6388, -1.5879, -1.5380, -1.4892, -1.3483, -1.2151, -1.1723, -1.1723,\n",
       "           -1.1303, -0.9690, -0.9690, -0.8922, -0.8178, -0.7814, -0.7814, -0.7456,\n",
       "           -0.7103, -0.6755, -0.6074, -0.5740, -0.5411, -0.5411, -0.4450, -0.4138,\n",
       "           -0.4138, -0.3830, -0.3526, -0.2636, -0.2636, -0.1221, -0.0948,  0.1870,\n",
       "            0.2111,  0.3507,  0.4394,  0.5250,  0.6077,  0.6280,  0.7072,  0.7651,\n",
       "            0.8028,  0.8215,  0.8215,  0.8767,  0.9128,  0.9659,  0.9659,  1.0350,\n",
       "            1.0520,  1.0689,  1.0689,  1.0856,  1.1022,  1.1187,  1.1351,  1.1676,\n",
       "            1.1836,  1.1996,  1.1996,  1.2624,  1.2779,  1.2932,  1.3387,  1.3387,\n",
       "            1.3537,  1.3686,  1.3834,  1.4127,  1.4127,  1.4416,  1.4560,  1.4844,\n",
       "            1.4985,  1.5126,  1.5126,  1.5950,  1.6219,  1.6352,  1.6616,  1.6747,\n",
       "            1.6878,  1.7007,  1.7136,  1.7136,  1.7265,  1.7392,  1.7519,  1.7896,\n",
       "            1.8267,  1.8390,  1.8512,  1.8873,  1.9112,  1.9112, -2.0288, -1.9108,\n",
       "           -1.9108, -1.8539, -1.7983, -1.6908, -1.5879, -1.4892, -1.4413, -1.3031,\n",
       "           -1.2587, -1.2151, -0.7103, -0.6755, -0.6074, -0.5740, -0.4450, -0.4138,\n",
       "           -0.3830, -0.3526, -0.3225, -0.3225, -0.2929, -0.2636, -0.1497, -0.1497,\n",
       "           -0.1221, -0.0948, -0.0677, -0.0410,  0.0115,  0.1134,  0.1870,  0.2111,\n",
       "            0.2111,  0.2585,  0.3280,  0.3507,  0.4611,  0.5039,  0.5460,  0.6280,\n",
       "            0.6480,  0.6679,  0.7459,  0.7651,  0.8215, -1.7983, -1.7440, -1.6908,\n",
       "           -1.5879, -1.5380, -1.4892, -1.2151, -1.1723, -1.0483, -1.0083, -0.9303,\n",
       "           -0.8922, -0.8922, -0.7456, -0.7103, -0.6755, -0.6074, -0.5740, -0.5411,\n",
       "           -0.3225, -0.2929, -0.2060, -0.1777, -0.1221, -0.1221, -0.0948, -0.0410,\n",
       "           -0.0146,  0.0115,  0.0374,  0.0630,  0.0883,  0.0883,  0.1134,  0.1382,\n",
       "            0.2349,  0.3280,  0.3507,  0.3732,  0.3954,  0.4394,  0.4611,  0.5873,\n",
       "            0.5873,  0.6077,  0.6280,  0.6480,  0.7651,  0.7840,  0.8215,  0.8767,\n",
       "            0.8767,  0.8948,  0.8948, -1.5879, -1.3943, -1.2587, -1.2151, -1.2151,\n",
       "           -1.0889, -1.0483, -1.0083, -0.9690, -0.9303, -0.8922, -0.8922, -0.8547,\n",
       "           -0.7814, -0.7456, -0.6755, -0.6755, -0.6074, -0.5740, -0.5740, -0.5411,\n",
       "           -0.5411, -0.4766, -0.4766, -0.4138, -0.2929, -0.2636, -0.2346, -0.2060,\n",
       "           -0.1777, -0.1777, -0.1497, -0.1221,  0.0115,  0.0883,  0.1382,  0.1870,\n",
       "            0.2111,  0.2349,  0.2585,  0.2819,  0.3050,  0.3507,  0.4175,  0.4394,\n",
       "            0.5039,  0.6077,  0.6280,  0.6679,  0.6877,  0.6877,  0.7072,  0.7267,\n",
       "            0.7459,  0.7840,  0.8028,  0.8215,  0.8584,  0.8584,  0.8767,  0.9128,\n",
       "            0.9306,  0.9484,  0.9659,  0.9659,  1.0007,  1.0179,  1.0350,  1.0520,\n",
       "            1.0856,  1.1022,  1.1022,  1.1351,  1.1996,  1.2155,  1.2312,  1.2312,\n",
       "            1.2624,  1.2624,  1.2779,  1.2932,  1.3387,  1.3537,  1.3686,  1.3981,\n",
       "            1.4127,  1.4127,  1.4416,  1.5265,  1.5265,  1.5404,  1.5541, -1.9691,\n",
       "           -1.8539, -1.7440, -1.6908, -1.6388, -1.5879, -1.4892, -1.4413, -1.3483,\n",
       "           -1.3031, -1.2587, -1.2587, -1.1723, -1.1303, -1.0483, -1.0483, -0.9303,\n",
       "           -0.8922, -0.8547, -0.8178, -0.7814, -0.7103, -0.6755, -0.6412, -0.6074,\n",
       "           -0.5740, -0.5740, -0.5087, -0.4450, -0.3225, -0.2346, -0.1777, -0.1221,\n",
       "           -0.0677,  0.1134,  0.1627,  0.1870,  0.2819,  0.3280,  0.3507,  0.4175,\n",
       "            0.4394,  0.5039,  0.5250,  0.5460,  0.5667,  0.6877,  0.7072,  0.7651,\n",
       "            0.7840,  0.8215,  0.8401,  0.8948,  0.9128,  0.9306,  0.9484,  0.9484,\n",
       "            0.9659,  0.9834,  0.9834,  1.0520,  1.0856,  1.1022,  1.1351,  1.1676,\n",
       "            1.3537,  1.3686,  1.3834,  1.3981,  1.4127,  1.4272,  1.4272,  1.4985,\n",
       "            1.5126,  1.5265,  1.5541,  1.6219,  1.6485,  1.6616,  1.6616,  1.6878,\n",
       "            1.6878,  1.7007,  1.7136,  1.7265,  1.7771,  1.8021,  1.8144,  1.8144,\n",
       "            1.8873,  1.9112,  1.9348,  1.9581, -1.9691, -1.6388, -1.3943, -1.3483,\n",
       "           -1.3483, -1.3031, -1.2151, -1.2151, -1.0483, -1.0083, -1.0083, -0.9303,\n",
       "           -0.8922, -0.8922, -0.8547, -0.7814, -0.7814, -0.7456, -0.6755, -0.6074,\n",
       "           -0.6074, -0.5087, -0.4450, -0.4138, -0.2929, -0.2636, -0.2346, -0.1777,\n",
       "           -0.1497, -0.1221, -0.0948, -0.0677, -0.0677, -0.0410,  0.0115,  0.0374,\n",
       "            0.0630,  0.1134,  0.2111,  0.2585,  0.3050,  0.3507,  0.3732,  0.3954,\n",
       "            0.4175,  0.4394,  0.4611,  0.4826,  0.5039,  0.5460,  0.5460,  0.5873,\n",
       "            0.5873,  0.6077,  0.6480,  0.6679,  0.7459,  0.7651,  0.8028,  0.8215,\n",
       "            0.8401,  0.8401,  0.8584,  0.8948, -2.0900, -2.0900, -1.9108, -1.8539,\n",
       "           -1.5879, -1.4892, -1.4892, -1.2151, -0.9690, -0.6755, -0.6074, -0.5740,\n",
       "           -0.5087, -0.5087, -0.4766, -0.3830, -0.3225, -0.2346, -0.1777, -0.0677,\n",
       "           -0.0410,  0.0374,  0.0630,  0.1134,  0.1382,  0.2111,  0.2349,  0.2585,\n",
       "            0.2585,  0.3280,  0.3507,  0.4175,  0.5039,  0.7840,  0.8215,  0.8584,\n",
       "            0.8948,  0.9306,  0.9659,  1.0007,  1.1351,  1.1676,  1.1676,  1.4127,\n",
       "            1.4985,  1.5541,  1.5815,  1.6085,  1.7136,  1.7392,  1.7646,  1.7646,\n",
       "            1.7896,  1.8144,  1.8633,  1.8873,  1.9112,  1.9813,  1.9813],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.6336, -0.5046, -0.4733, -0.2941, -0.1816, -0.1543, -0.1273, -0.1006,\n",
       "           -0.1006, -0.0741,  0.0538,  0.1032,  0.1275,  0.2455,  0.2684,  0.3359,\n",
       "            0.3359,  0.3580,  0.4016,  0.4231,  0.4231,  0.4655,  0.5278,  0.5482,\n",
       "            0.5684,  0.6477,  0.7245,  0.7620, -0.4733, -0.4733, -0.4425, -0.4121,\n",
       "           -0.3821, -0.2941, -0.2941, -0.2655, -0.0741, -0.0480, -0.0480,  0.0035,\n",
       "            0.0288,  0.0786,  0.1032,  0.1754,  0.1990,  0.1990,  0.2455,  0.2684,\n",
       "            0.3580,  0.4016,  0.4231,  0.5278,  0.5684, -1.4078, -1.2747, -1.2319,\n",
       "           -1.1078, -1.0285, -0.8410, -0.7698, -0.6669, -0.6336, -0.5682, -0.5362,\n",
       "           -0.5046, -0.3821, -0.3524, -0.3231, -0.2655, -0.0741, -0.0480,  0.0035,\n",
       "            0.0035,  0.0786,  0.1032,  0.1754,  0.1754,  0.1990,  0.1990,  0.2224,\n",
       "            0.3136,  0.3799,  0.4231,  0.4231,  0.4444,  0.4655,  0.4864,  0.5684,\n",
       "            0.6084,  0.6281,  0.7055,  0.7620,  0.7805,  0.7989,  0.8172,  0.8353,\n",
       "            0.8353,  0.8532,  0.8888,  0.9064,  0.9584,  1.0261,  1.0592,  1.0756,\n",
       "            1.0919,  1.1080,  1.1241,  1.1241,  1.1401,  1.2337,  1.2792,  1.2942,\n",
       "            1.3090,  1.3385,  1.3531,  1.3531,  1.3677,  1.3965,  1.4390,  1.4530,\n",
       "            1.4670,  1.4808,  1.4808,  1.4946,  1.5083,  1.5083,  1.5489,  1.5757,\n",
       "            1.6021,  1.6021,  1.6152,  1.6412,  1.6412,  1.6669,  1.6797,  1.6924,\n",
       "            1.7050,  1.7425,  1.7549,  1.7672,  1.7794,  1.7916,  1.8752, -0.4733,\n",
       "           -0.2941, -0.2941, -0.2372, -0.1816,  0.0035,  0.0035,  0.0288,  0.0538,\n",
       "            0.1032,  0.1516,  0.1516,  0.1754,  0.2455,  0.2684,  0.3359,  0.3359,\n",
       "            0.3580,  0.3799,  0.4444, -0.1816, -0.1273, -0.1273, -0.0741, -0.0480,\n",
       "            0.1032,  0.1275,  0.1516,  0.1754,  0.1990,  0.2455,  0.2911,  0.3136,\n",
       "            0.3580,  0.3799,  0.4444,  0.4655,  0.5278,  0.5278, -1.4539, -1.3626,\n",
       "           -1.3182, -1.3182, -1.2747, -1.2319, -1.2319, -1.1898, -0.8410, -0.8051,\n",
       "           -0.8051, -0.7350, -0.7350, -0.7007, -0.7007, -0.6669, -0.6669, -0.6007,\n",
       "           -0.5046, -0.4733, -0.4425, -0.3821, -0.2655, -0.1816, -0.1816, -0.1543,\n",
       "           -0.0480, -0.0221, -0.0221,  0.0288,  0.0288,  0.0786,  0.1032,  0.1275,\n",
       "            0.1754,  0.2224,  0.2684,  0.3136,  0.3799,  0.5072,  0.5072,  0.5278,\n",
       "            0.5278,  0.5684,  0.5684,  0.6084,  0.6864,  0.7055,  0.7055,  0.7805,\n",
       "            0.7989,  0.8172,  0.8711,  0.9239,  0.9412,  0.9412,  0.9755,  0.9925,\n",
       "            1.0093,  1.0261,  1.0261,  1.0427,  1.0427,  1.0592,  1.1401,  1.1559,\n",
       "            1.1717,  1.1873,  1.2029,  1.2641,  1.3090,  1.3385,  1.3531,  1.4107,\n",
       "            1.4530,  1.5219,  1.5219,  1.5889, -1.4539, -1.2747, -1.2319, -1.1898,\n",
       "           -1.1485, -0.9898, -0.9517, -0.8051, -0.7698, -0.7007, -0.7007, -0.6669,\n",
       "           -0.6669, -0.6336, -0.6007, -0.5362, -0.4425, -0.4121, -0.3524, -0.3231,\n",
       "           -0.2093, -0.1273, -0.0741, -0.0741, -0.0480, -0.0221,  0.0035,  0.0538,\n",
       "            0.0786,  0.0786,  0.1032,  0.2684,  0.2911,  0.2911,  0.3136,  0.3359,\n",
       "            0.3580,  0.4655,  0.5072,  0.5278,  0.5482,  0.5482,  0.5684,  0.5885,\n",
       "            0.7055,  0.7055,  0.7245,  0.7620,  0.7805,  0.7989,  0.8532,  0.8888,\n",
       "            0.9239,  0.9412,  0.9584,  0.9755,  1.0261,  1.0756,  1.0919,  1.1241,\n",
       "            1.1401,  1.1401,  1.1559,  1.1559,  1.1873,  1.2641,  1.2792,  1.3238,\n",
       "            1.3385,  1.3531,  1.3677,  1.3965,  1.4390,  1.4670,  1.5083,  1.5083,\n",
       "            1.5355,  1.5355,  1.6021,  1.6021,  1.6152,  1.6282,  1.6412,  1.6797,\n",
       "            1.7176,  1.7301,  1.8037,  1.8278,  1.8278,  1.8516,  1.8752,  1.9217,\n",
       "           -1.5008, -1.3626, -1.3626, -1.2747, -1.2319, -1.1898, -1.1485, -1.1078,\n",
       "           -0.9898, -0.9898, -0.9517, -0.8410, -0.6669, -0.6007, -0.5682, -0.5682,\n",
       "           -0.5362, -0.4733, -0.4425, -0.4121, -0.3821, -0.3231, -0.1543, -0.0480,\n",
       "           -0.0480,  0.0288,  0.0538,  0.1032,  0.1275,  0.1754,  0.1990,  0.2224,\n",
       "            0.2684,  0.3580,  0.3799,  0.3799,  0.4016,  0.4231,  0.4655,  0.4864,\n",
       "            0.5072,  0.5482,  0.6671,  0.6671,  0.6864,  0.7055,  0.7433,  0.7620,\n",
       "            0.7805,  0.7989,  0.8172,  0.8532,  0.8532,  0.8711,  0.8888,  0.9755,\n",
       "            0.9925,  1.0093,  1.0261,  1.0427,  1.0756,  1.0756,  1.0919,  1.0919,\n",
       "            1.1080, -1.4539, -1.4078, -1.3626, -1.3182, -1.2747, -1.2319, -1.1898,\n",
       "           -1.0285, -0.9517, -0.9517, -0.8773, -0.8410, -0.7350, -0.7007, -0.6669,\n",
       "           -0.6669, -0.5682, -0.5362, -0.5046, -0.4733, -0.4425, -0.4121, -0.4121,\n",
       "           -0.3524, -0.2941, -0.2655, -0.2372, -0.1816, -0.1816, -0.0741, -0.0221,\n",
       "            0.0786,  0.1032,  0.1032,  0.1275,  0.1275,  0.1516,  0.1754,  0.1990,\n",
       "            0.2684,  0.3136,  0.4444,  0.4444,  0.4655,  0.5482,  0.5684,  0.5684,\n",
       "            0.5885,  0.6084,  0.6281,  0.6671,  0.6864,  0.8172,  0.8532,  0.8711,\n",
       "            0.9064,  0.9412,  0.9755,  1.0093,  1.0261,  1.0427,  1.0592,  1.0592,\n",
       "            1.0919,  1.1401,  1.1559,  1.1717,  1.2337,  1.2490,  1.2792,  1.2942,\n",
       "            1.3090,  1.3238,  1.3531,  1.3821,  1.3821,  1.4107,  1.4249,  1.4249,\n",
       "            1.4390,  1.5083,  1.5083,  1.5219,  1.5219,  1.5489,  1.5489,  1.5623,\n",
       "            1.5623,  1.5757,  1.5889,  1.6152,  1.6282,  1.6669,  1.6797,  1.6797,\n",
       "            1.6924,  1.7050,  1.7176, -1.4539, -1.4078, -1.2747, -1.2319, -1.1898,\n",
       "           -1.1485, -1.1078, -1.1078, -1.0678, -1.0678, -0.9898, -0.9143, -0.8773,\n",
       "           -0.7698, -0.6336, -0.5046, -0.5046, -0.4425, -0.4121, -0.3821, -0.3821,\n",
       "           -0.3524, -0.2941, -0.2655, -0.2372, -0.1543, -0.1006, -0.0741, -0.0221,\n",
       "            0.0288,  0.0538,  0.0786,  0.1754,  0.2684,  0.2911,  0.3136,  0.3359,\n",
       "            0.3580,  0.3799,  0.3799,  0.4444,  0.4864,  0.6084,  0.7055,  0.7620,\n",
       "            0.7805,  0.7989,  0.8172,  0.8172,  0.8353,  0.8353,  0.8532,  0.8888,\n",
       "            0.9064,  0.9239,  0.9412,  0.9584,  0.9584,  1.0093,  1.0261,  1.0756,\n",
       "            1.0756,  1.1401,  1.1717,  1.2029,  1.2183,  1.2641,  1.2641,  1.3090,\n",
       "            1.3531,  1.4390,  1.4530,  1.5219,  1.5355,  1.5489,  1.5489,  1.5757,\n",
       "            1.5889,  1.5889,  1.6152,  1.6797,  1.7050,  1.7050,  1.7176,  1.7301,\n",
       "            1.7301,  1.7425,  1.7794,  1.7794,  1.8037,  1.8516,  1.8516,  1.8752,\n",
       "           -1.4539, -1.4078, -1.2319, -1.0678, -0.9517, -0.9517, -0.9143, -0.8773,\n",
       "           -0.8410, -0.8410, -0.8051, -0.7698, -0.7007, -0.6007, -0.5362, -0.5046,\n",
       "           -0.4733, -0.4733, -0.3231, -0.2941, -0.1006, -0.0480, -0.0480,  0.0288,\n",
       "            0.0538,  0.0786,  0.1032,  0.1516,  0.1754,  0.1990,  0.2224,  0.2911,\n",
       "            0.3136,  0.3359,  0.4231,  0.4864,  0.5072,  0.5684,  0.5885,  0.6084,\n",
       "            0.6084,  0.6281,  0.6477,  0.6671,  0.6864,  0.7433,  0.7620,  0.7620,\n",
       "            0.7805,  0.7805,  0.8353,  0.8888,  0.9755,  0.9925,  1.0093,  1.0261,\n",
       "            1.0427,  1.0756,  1.0919,  1.1080,  1.1241,  1.1717,  1.2337,  1.3238,\n",
       "            1.3531,  1.3677,  1.4390,  1.4390,  1.4670,  1.4946, -1.6474, -1.5976,\n",
       "           -1.5976, -1.5487, -1.4539, -1.4539, -1.4078, -1.3182, -1.2747, -1.2747,\n",
       "           -1.2319, -1.1898, -1.1898, -1.1485, -1.1485, -1.1078, -1.1078, -1.0678,\n",
       "           -1.0285, -0.8773, -0.7698, -0.7350, -0.6669, -0.5362, -0.4733, -0.4733,\n",
       "           -0.4425, -0.4121, -0.3821, -0.3524, -0.3231, -0.2655, -0.2655, -0.1543,\n",
       "            0.0035,  0.0288,  0.0538,  0.0538,  0.1275,  0.1516,  0.1754,  0.2911,\n",
       "            0.3359,  0.3580,  0.4231,  0.4231,  0.4444,  0.4655,  0.5885,  0.6281,\n",
       "            0.6477,  0.6671,  0.7055,  0.7055,  0.7433,  0.7433,  0.7620,  0.8353,\n",
       "            0.8353,  0.9064,  0.9239,  0.9755,  0.9925,  1.0261,  1.0427,  1.0756,\n",
       "            1.0919,  1.1873,  1.2183,  1.2183,  1.2337,  1.2641,  1.2792,  1.3090,\n",
       "            1.3238,  1.3385,  1.3677,  1.4107,  1.4107,  1.4390,  1.4670,  1.4808,\n",
       "            1.4946,  1.4946,  1.5219,  1.5889,  1.6152,  1.6282,  1.6412,  1.6412,\n",
       "            1.6541,  1.6924,  1.7301,  1.7794,  1.7916,  1.8516,  1.8752,  1.8752,\n",
       "            1.8986, -1.7504, -1.6474, -1.5487, -1.3626, -1.1898, -1.1078, -0.7350,\n",
       "           -0.6669, -0.6007, -0.4733, -0.3524, -0.3524, -0.2941, -0.1816, -0.1273,\n",
       "            0.0288,  0.0786,  0.1275,  0.1754,  0.2224,  0.2684,  0.4444,  0.4864,\n",
       "            0.5278,  0.6084,  0.6864,  0.7245,  0.7620,  0.7989,  0.8353,  0.8711,\n",
       "            0.9064,  1.0093,  1.0427,  1.3238,  1.3531,  1.4390,  1.5219,  1.5219,\n",
       "            1.5489,  1.5489,  1.5757,  1.6021,  1.6541,  1.6541,  1.7549,  1.7794],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-1.0328, -1.0328, -1.0136,  ...,  0.8474,  0.8651,  0.8651],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.5611, -0.5611, -0.5278,  ...,  2.0044,  2.0275,  2.0275],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068,\n",
       "           -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068,\n",
       "           -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068,\n",
       "           -0.9068, -0.9068, -0.9068, -0.9068, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8604,\n",
       "           -0.8604, -0.8604, -0.8604, -0.8604, -0.8604, -0.8256, -0.8256, -0.8256,\n",
       "           -0.8256, -0.8256, -0.8256, -0.8256, -0.8256, -0.8256, -0.8256, -0.8256,\n",
       "           -0.8256, -0.8256, -0.8256, -0.8256, -0.8256, -0.8256, -0.8256, -0.8256,\n",
       "           -0.8256, -0.8256, -0.8256, -0.8256, -0.8256, -0.8256, -0.8256, -0.8256,\n",
       "           -0.8256, -0.8256, -0.8256, -0.8256, -0.7849, -0.7849, -0.7849, -0.7849,\n",
       "           -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849,\n",
       "           -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849,\n",
       "           -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979,\n",
       "           -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.6979, -0.5353, -0.5353,\n",
       "           -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353,\n",
       "           -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353,\n",
       "           -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353,\n",
       "           -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353,\n",
       "           -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.5353,\n",
       "           -0.5353, -0.5353, -0.5353, -0.5353, -0.5353, -0.3322, -0.3322, -0.3322,\n",
       "           -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322,\n",
       "           -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322,\n",
       "           -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322,\n",
       "           -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322,\n",
       "           -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322,\n",
       "           -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322, -0.3322,\n",
       "           -0.3322, -0.3322, -0.3322, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,\n",
       "           -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697, -0.1697,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,  0.3586,\n",
       "            0.3586,  0.3586,  0.3586,  0.3586,  0.8868,  0.8868,  0.8868,  0.8868,\n",
       "            0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,\n",
       "            0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,\n",
       "            0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,\n",
       "            0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,\n",
       "            0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,\n",
       "            0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,\n",
       "            0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,  0.8868,\n",
       "            0.8868,  0.8868,  0.8868,  0.8868,  2.4715,  2.4715,  2.4715,  2.4715,\n",
       "            2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,\n",
       "            2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,\n",
       "            2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,\n",
       "            2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,\n",
       "            2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,\n",
       "            2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,\n",
       "            2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715,  2.4715],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068,\n",
       "           -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068,\n",
       "           -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068, -0.9068,\n",
       "           -0.9068, -0.9068, -0.9068, -0.9068, -0.8662, -0.8662, -0.8662, -0.8662,\n",
       "           -0.8662, -0.8662, -0.8662, -0.8662, -0.8662, -0.8662, -0.8662, -0.8662,\n",
       "           -0.8662, -0.8662, -0.8662, -0.8662, -0.8662, -0.8662, -0.8662, -0.8662,\n",
       "           -0.8662, -0.8662, -0.8662, -0.8662, -0.8662, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198,\n",
       "           -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.8198, -0.7849,\n",
       "           -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849,\n",
       "           -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849, -0.7849,\n",
       "           -0.7849, -0.7849, -0.7849, -0.7443, -0.7443, -0.7443, -0.7443, -0.7443,\n",
       "           -0.7443, -0.7443, -0.7443, -0.7443, -0.7443, -0.7443, -0.7443, -0.7443,\n",
       "           -0.7443, -0.7443, -0.7443, -0.7443, -0.7443, -0.7443, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572, -0.6572,\n",
       "           -0.6572, -0.6572, -0.6572, -0.6572, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541, -0.4541,\n",
       "           -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916,\n",
       "           -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916,\n",
       "           -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916,\n",
       "           -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916,\n",
       "           -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916,\n",
       "           -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916,\n",
       "           -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916,\n",
       "           -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916, -0.2916,\n",
       "           -0.2916, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884, -0.0884,\n",
       "           -0.0884, -0.0884, -0.0884,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,  0.0741,\n",
       "            0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,\n",
       "            0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,\n",
       "            0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,\n",
       "            0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,\n",
       "            0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,\n",
       "            0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,\n",
       "            0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,\n",
       "            0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,\n",
       "            0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  0.6024,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,  1.1306,\n",
       "            1.1306,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,\n",
       "            3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,\n",
       "            3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,\n",
       "            3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,\n",
       "            3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,\n",
       "            3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435,  3.2435],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9068, -0.9068, -0.9068,  ...,  2.5934,  2.5934,  2.5934],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9243, -0.9243, -0.9243,  ...,  3.0229,  3.0229,  3.0229],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.4187, 0.3716,\n",
       "           0.3716, 0.3617, 0.3363, 0.3212, 0.3147, 0.3133, 0.3133, 0.3716, 0.3133,\n",
       "           0.3716, 0.3133, 0.3716, 0.3133, 0.3133, 0.3133, 0.3716, 0.3133, 0.3716,\n",
       "           0.3716, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473,\n",
       "           0.3473, 0.3258, 0.3258, 0.3258, 0.3258, 0.3473, 0.3473, 0.3258, 0.3258,\n",
       "           0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3473, 0.3473,\n",
       "           0.3258, 0.3258, 0.3180, 0.3207, 0.3086, 0.2900, 0.2851, 0.2781, 0.2698,\n",
       "           0.2607, 0.2637, 0.2668, 0.2688, 0.2859, 0.3138, 0.2561, 0.2561, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295,\n",
       "           0.3295, 0.3295, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295,\n",
       "           0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.3295, 0.2561, 0.3295, 0.2561,\n",
       "           0.2561, 0.3295, 0.2561, 0.3295, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.3295, 0.3381,\n",
       "           0.3381, 0.3381, 0.3381, 0.3265, 0.3632, 0.3381, 0.3216, 0.3165, 0.3037,\n",
       "           0.3004, 0.2912, 0.2812, 0.2803, 0.2746, 0.2723, 0.2720, 0.2671, 0.2695,\n",
       "           0.2617, 0.2847, 0.2702, 0.2702, 0.3356, 0.2702, 0.3356, 0.2702, 0.3356,\n",
       "           0.2702, 0.2702, 0.3356, 0.3564, 0.3187, 0.3187, 0.3002, 0.2968, 0.2920,\n",
       "           0.2829, 0.2660, 0.2642, 0.2607, 0.2606, 0.2599, 0.2623, 0.2655, 0.2648,\n",
       "           0.2754, 0.2797, 0.2871, 0.2661, 0.2920, 0.2661, 0.3215, 0.2661, 0.3215,\n",
       "           0.2661, 0.3215, 0.3215, 0.2661, 0.3369, 0.3510, 0.3510, 0.3369, 0.3369,\n",
       "           0.3369, 0.3510, 0.3369, 0.3369, 0.3510, 0.3369, 0.3510, 0.3510, 0.3510,\n",
       "           0.3369, 0.3369, 0.3369, 0.3369, 0.3295, 0.3397, 0.3226, 0.3172, 0.3031,\n",
       "           0.2975, 0.2933, 0.2938, 0.2866, 0.2749, 0.2728, 0.2622, 0.2596, 0.2658,\n",
       "           0.2692, 0.2912, 0.3146, 0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.3302, 0.3302, 0.3302,\n",
       "           0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302,\n",
       "           0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.2695, 0.3302, 0.3302, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.3302, 0.3302, 0.2695, 0.3695,\n",
       "           0.5483, 0.3695, 0.3695, 0.5483, 0.5483, 0.5483, 0.5483, 0.5483, 0.3695,\n",
       "           0.5483, 0.3695, 0.3469, 0.4183, 0.3310, 0.3267, 0.3105, 0.3345, 0.3049,\n",
       "           0.3031, 0.3168, 0.3001, 0.2975, 0.3078, 0.2949, 0.2905, 0.2926, 0.2908,\n",
       "           0.2887, 0.2866, 0.2852, 0.2911, 0.2942, 0.2847, 0.2983, 0.2876, 0.2911,\n",
       "           0.3135, 0.3053, 0.3091, 0.3461, 0.3318, 0.3461, 0.3386, 0.3461, 0.3415,\n",
       "           0.3461, 0.3459, 0.6848, 0.3459, 0.6848, 0.6642, 0.3459, 0.5333, 0.5164,\n",
       "           0.4740, 0.4519, 0.4324, 0.4189, 0.3381, 0.3202, 0.3164, 0.3511, 0.3059,\n",
       "           0.3018, 0.2989, 0.2883, 0.2808, 0.2774, 0.2753, 0.2727, 0.2728, 0.2728,\n",
       "           0.2691, 0.2710, 0.2698, 0.2669, 0.2671, 0.2664, 0.2710, 0.2712, 0.2666,\n",
       "           0.2672, 0.2711, 0.2715, 0.2728, 0.2745, 0.2770, 0.2782, 0.2864, 0.3104,\n",
       "           0.3104, 0.2921, 0.3104, 0.3076, 0.3133, 0.3104, 0.3256, 0.3104, 0.3273,\n",
       "           0.3104, 0.3472, 0.3472, 0.3472, 0.3822, 0.3472, 0.3472, 0.3822, 0.3422,\n",
       "           0.3822, 0.3320, 0.3775, 0.3280, 0.3672, 0.3508, 0.3103, 0.3265, 0.3036,\n",
       "           0.2970, 0.3053, 0.2938, 0.3040, 0.2912, 0.2958, 0.2864, 0.2824, 0.2764,\n",
       "           0.2747, 0.2725, 0.2711, 0.2698, 0.2707, 0.2691, 0.2686, 0.2639, 0.2656,\n",
       "           0.2663, 0.2630, 0.2670, 0.2628, 0.2658, 0.2692, 0.2642, 0.2661, 0.2684,\n",
       "           0.2775, 0.2869, 0.2849, 0.2849, 0.2853, 0.2869, 0.2849, 0.2849, 0.2919,\n",
       "           0.2849, 0.3020, 0.3043, 0.2849, 0.3108, 0.2849, 0.3125, 0.2849, 0.3207,\n",
       "           0.2849, 0.3229, 0.2849, 0.3229, 0.3229, 0.3124, 0.2849, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.6576, 0.6189, 0.5936, 0.3469, 0.3469, 0.5412,\n",
       "           0.3469, 0.3469, 0.3469, 0.4520, 0.4388, 0.3434, 0.4219, 0.3313, 0.3866,\n",
       "           0.3244, 0.3141, 0.3109, 0.3080, 0.3051, 0.3263, 0.3153, 0.2955, 0.2940,\n",
       "           0.3014, 0.2983, 0.2901, 0.2870, 0.2879, 0.2810, 0.2771, 0.2740, 0.2726,\n",
       "           0.2743, 0.2690, 0.2725, 0.2678, 0.2671, 0.2746, 0.2683, 0.2779, 0.2691,\n",
       "           0.2702, 0.2804, 0.2854, 0.2824, 0.2965, 0.2942, 0.2804, 0.3037, 0.2857,\n",
       "           0.3085, 0.3089, 0.2902, 0.2918, 0.2932, 0.3089, 0.3089, 0.2964, 0.3089,\n",
       "           0.3042, 0.3057, 0.3089, 0.3089, 0.3116, 0.3116, 0.3089, 0.3116, 0.3089,\n",
       "           0.3116, 0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3116, 0.3116,\n",
       "           0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116,\n",
       "           0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3439, 0.3439, 0.4487,\n",
       "           0.4355, 0.3331, 0.4234, 0.4020, 0.3242, 0.3154, 0.3551, 0.3130, 0.3100,\n",
       "           0.3391, 0.3078, 0.3058, 0.3209, 0.3031, 0.3157, 0.2985, 0.3037, 0.2965,\n",
       "           0.2930, 0.2948, 0.2915, 0.2874, 0.2867, 0.2854, 0.2849, 0.2848, 0.2818,\n",
       "           0.2839, 0.2803, 0.2834, 0.2811, 0.2824, 0.2822, 0.2819, 0.2779, 0.2814,\n",
       "           0.2811, 0.2763, 0.2819, 0.2824, 0.2767, 0.2829, 0.2817, 0.2824, 0.2765,\n",
       "           0.2772, 0.2783, 0.2858, 0.2779, 0.2890, 0.2871, 0.2882, 0.2902, 0.2938,\n",
       "           0.2975, 0.3004, 0.2849, 0.2853, 0.2980, 0.3009, 0.2999, 0.5636, 0.3514,\n",
       "           0.3463, 0.5072, 0.3388, 0.4173, 0.3351, 0.3262, 0.3204, 0.3153, 0.3135,\n",
       "           0.3134, 0.3101, 0.3126, 0.3115, 0.3105, 0.3050, 0.3026, 0.3019, 0.3061,\n",
       "           0.3052, 0.2983, 0.3050, 0.2977, 0.3053, 0.3043, 0.2967, 0.2968, 0.3043,\n",
       "           0.3043, 0.3040, 0.2955, 0.2960, 0.3071, 0.3091, 0.3101, 0.2967, 0.3110,\n",
       "           0.2989, 0.3118, 0.3167, 0.3020, 0.3136, 0.3194, 0.3105, 0.3119, 0.3129,\n",
       "           0.3137, 0.3182, 0.3192, 0.3202, 0.3194, 0.3194, 0.3232, 0.3194, 0.3194,\n",
       "           0.3284, 0.3320, 0.3194]),\n",
       "   tensor([0.4484, 0.6109, 0.5840, 0.4310, 0.3847, 0.3745, 0.3700, 0.3681, 0.3691,\n",
       "           0.3676, 0.4731, 0.4101, 0.4101, 0.4731, 0.4731, 0.4101, 0.4731, 0.4101,\n",
       "           0.4101, 0.4101, 0.4731, 0.4731, 0.4101, 0.4101, 0.4731, 0.4731, 0.4101,\n",
       "           0.4731, 0.6166, 0.3907, 0.5844, 0.5625, 0.5080, 0.4146, 0.3476, 0.3420,\n",
       "           0.3454, 0.3265, 0.3439, 0.3683, 0.3367, 0.3893, 0.4189, 0.3943, 0.4067,\n",
       "           0.3968, 0.4312, 0.3968, 0.4799, 0.3968, 0.3968, 0.3968, 0.4799, 0.3801,\n",
       "           0.7247, 0.3801, 0.3801, 0.3801, 0.3801, 0.7247, 0.3801, 0.3801, 0.5664,\n",
       "           0.5411, 0.5014, 0.3375, 0.3299, 0.3253, 0.3174, 0.2933, 0.2945, 0.2957,\n",
       "           0.3209, 0.3369, 0.3101, 0.3313, 0.3544, 0.3365, 0.3711, 0.3510, 0.4041,\n",
       "           0.4277, 0.4456, 0.4041, 0.4583, 0.4041, 0.4751, 0.4041, 0.4041, 0.5174,\n",
       "           0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174,\n",
       "           0.5174, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.5174, 0.4041,\n",
       "           0.5174, 0.4041, 0.5174, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041,\n",
       "           0.5174, 0.4041, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041,\n",
       "           0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174,\n",
       "           0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4398,\n",
       "           0.3205, 0.3120, 0.3063, 0.3011, 0.2909, 0.3002, 0.2918, 0.2923, 0.2980,\n",
       "           0.3056, 0.3182, 0.3390, 0.3309, 0.3597, 0.3585, 0.3597, 0.3597, 0.3597,\n",
       "           0.4029, 0.3010, 0.2962, 0.2972, 0.2978, 0.2974, 0.2949, 0.2967, 0.2858,\n",
       "           0.3025, 0.2893, 0.2893, 0.3239, 0.3283, 0.3410, 0.2893, 0.3656, 0.2893,\n",
       "           0.3919, 0.2893, 0.3481, 0.5191, 0.3962, 0.3481, 0.3962, 0.3962, 0.3481,\n",
       "           0.3962, 0.3962, 0.3962, 0.3055, 0.3962, 0.3347, 0.3962, 0.3481, 0.3962,\n",
       "           0.3493, 0.3711, 0.3167, 0.3389, 0.3262, 0.3045, 0.2948, 0.2862, 0.2920,\n",
       "           0.2843, 0.2819, 0.2831, 0.2904, 0.2848, 0.2935, 0.2975, 0.2964, 0.3026,\n",
       "           0.2938, 0.2957, 0.3013, 0.3090, 0.3151, 0.3437, 0.3151, 0.3525, 0.3151,\n",
       "           0.3578, 0.3151, 0.3578, 0.3578, 0.3578, 0.3151, 0.3578, 0.3578, 0.3578,\n",
       "           0.3151, 0.3151, 0.3578, 0.3151, 0.3578, 0.3151, 0.3151, 0.3578, 0.3151,\n",
       "           0.3578, 0.3151, 0.3578, 0.3151, 0.3151, 0.3151, 0.3578, 0.3578, 0.3151,\n",
       "           0.3578, 0.3151, 0.3578, 0.3151, 0.3151, 0.3578, 0.3151, 0.3578, 0.3632,\n",
       "           0.3632, 0.3355, 0.3632, 0.3632, 0.3632, 0.3632, 0.3570, 0.3396, 0.3379,\n",
       "           0.3194, 0.3372, 0.3153, 0.3263, 0.3226, 0.3101, 0.2963, 0.2949, 0.2899,\n",
       "           0.2829, 0.2792, 0.2771, 0.2748, 0.2811, 0.2805, 0.2752, 0.2750, 0.2756,\n",
       "           0.2759, 0.2822, 0.2824, 0.2831, 0.2851, 0.2893, 0.2856, 0.2894, 0.2945,\n",
       "           0.3025, 0.3017, 0.3123, 0.3165, 0.3017, 0.3017, 0.3229, 0.3434, 0.3017,\n",
       "           0.3494, 0.3017, 0.3017, 0.3017, 0.3017, 0.3017, 0.3017, 0.3017, 0.3784,\n",
       "           0.3784, 0.3784, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017,\n",
       "           0.3017, 0.3017, 0.3784, 0.3017, 0.3017, 0.3784, 0.3017, 0.3017, 0.3017,\n",
       "           0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3017, 0.3784,\n",
       "           0.3784, 0.3784, 0.3784, 0.3784, 0.3784, 0.3784, 0.3017, 0.3784, 0.3017,\n",
       "           0.3017, 0.4610, 0.4610, 0.3484, 0.4610, 0.3484, 0.4610, 0.4610, 0.4610,\n",
       "           0.4352, 0.3484, 0.4214, 0.3835, 0.3245, 0.3356, 0.3331, 0.3173, 0.3161,\n",
       "           0.3128, 0.3110, 0.3098, 0.3111, 0.3037, 0.2954, 0.2920, 0.3008, 0.3012,\n",
       "           0.2899, 0.2902, 0.2903, 0.2909, 0.3035, 0.3054, 0.3075, 0.3150, 0.2965,\n",
       "           0.3164, 0.3156, 0.2999, 0.3271, 0.3032, 0.3041, 0.3284, 0.3222, 0.3284,\n",
       "           0.3284, 0.3284, 0.3323, 0.3335, 0.3284, 0.3284, 0.3443, 0.3511, 0.3284,\n",
       "           0.3284, 0.3563, 0.3689, 0.3284, 0.3284, 0.3678, 0.3678, 0.3678, 0.3284,\n",
       "           0.3678, 0.3284, 0.3678, 0.3532, 0.3532, 0.3306, 0.3532, 0.3439, 0.3532,\n",
       "           0.3496, 0.3976, 0.3716, 0.3346, 0.3605, 0.3439, 0.3260, 0.3109, 0.3226,\n",
       "           0.3096, 0.3047, 0.3021, 0.3030, 0.2977, 0.2967, 0.2965, 0.2939, 0.2891,\n",
       "           0.2838, 0.2848, 0.2870, 0.2803, 0.2878, 0.2769, 0.2834, 0.2861, 0.2746,\n",
       "           0.2861, 0.2755, 0.2883, 0.2761, 0.2859, 0.2773, 0.2866, 0.2802, 0.2879,\n",
       "           0.2990, 0.2888, 0.3016, 0.2943, 0.3016, 0.2969, 0.3016, 0.3020, 0.3016,\n",
       "           0.3016, 0.3205, 0.3016, 0.3016, 0.3016, 0.3378, 0.3016, 0.3468, 0.3016,\n",
       "           0.3514, 0.3543, 0.3016, 0.3016, 0.3634, 0.3016, 0.3701, 0.3016, 0.3016,\n",
       "           0.3016, 0.3701, 0.3701, 0.3701, 0.3016, 0.3701, 0.3016, 0.3701, 0.3701,\n",
       "           0.3016, 0.3016, 0.3701, 0.3016, 0.3701, 0.3016, 0.3701, 0.3016, 0.3701,\n",
       "           0.3016, 0.3701, 0.3701, 0.3701, 0.3016, 0.3016, 0.3701, 0.3016, 0.3016,\n",
       "           0.3701, 0.3701, 0.3792, 0.3792, 0.3370, 0.3370, 0.3792, 0.3792, 0.3792,\n",
       "           0.3396, 0.3792, 0.3317, 0.3646, 0.3232, 0.3353, 0.3108, 0.3024, 0.2949,\n",
       "           0.2960, 0.2912, 0.2892, 0.2854, 0.2883, 0.2882, 0.2810, 0.2792, 0.2779,\n",
       "           0.2822, 0.2813, 0.2804, 0.2774, 0.2708, 0.2776, 0.2806, 0.2712, 0.2810,\n",
       "           0.2725, 0.2717, 0.2726, 0.2839, 0.2728, 0.2844, 0.2910, 0.2922, 0.3057,\n",
       "           0.2926, 0.2963, 0.2988, 0.3015, 0.3033, 0.3048, 0.3048, 0.3048, 0.3054,\n",
       "           0.3048, 0.3048, 0.3151, 0.3163, 0.3184, 0.3048, 0.3237, 0.3048, 0.3270,\n",
       "           0.3048, 0.3048, 0.3270, 0.3270, 0.3270, 0.3270, 0.3048, 0.3048, 0.3048,\n",
       "           0.3048, 0.3270, 0.3048, 0.3048, 0.3270, 0.3048, 0.3270, 0.3270, 0.3048,\n",
       "           0.3048, 0.3270, 0.3270, 0.3048, 0.3048, 0.3270, 0.3048, 0.3048, 0.3270,\n",
       "           0.3048, 0.3270, 0.3270, 0.3048, 0.3270, 0.3353, 0.4503, 0.3976, 0.3554,\n",
       "           0.3313, 0.3114, 0.3227, 0.3004, 0.3160, 0.3030, 0.3108, 0.2994, 0.3003,\n",
       "           0.2927, 0.2922, 0.2859, 0.2833, 0.2873, 0.2784, 0.2795, 0.2796, 0.2671,\n",
       "           0.2767, 0.2678, 0.2733, 0.2733, 0.2736, 0.2738, 0.2758, 0.2655, 0.2632,\n",
       "           0.2650, 0.2648, 0.2740, 0.2646, 0.2733, 0.2788, 0.2654, 0.2667, 0.2674,\n",
       "           0.2800, 0.2654, 0.2663, 0.2717, 0.2693, 0.2933, 0.2736, 0.2938, 0.2710,\n",
       "           0.2938, 0.2938, 0.2938, 0.2768, 0.2938, 0.2938, 0.2768, 0.2768, 0.2768,\n",
       "           0.2938, 0.2768, 0.2938, 0.2938, 0.2768, 0.2938, 0.2768, 0.2938, 0.2768,\n",
       "           0.2938, 0.2938, 0.2768, 0.3342, 0.3372, 0.3342, 0.3372, 0.3372, 0.3358,\n",
       "           0.3372, 0.3372, 0.3372, 0.3249, 0.3230, 0.3372, 0.3163, 0.3372, 0.3168,\n",
       "           0.3372, 0.3153, 0.3152, 0.3371, 0.3165, 0.2974, 0.2988, 0.2975, 0.2848,\n",
       "           0.2836, 0.2914, 0.2810, 0.2888, 0.2811, 0.2795, 0.2785, 0.2798, 0.2858,\n",
       "           0.2748, 0.2819, 0.2708, 0.2707, 0.2804, 0.2700, 0.2693, 0.2781, 0.2813,\n",
       "           0.2773, 0.2774, 0.2671, 0.2769, 0.2775, 0.2758, 0.2832, 0.2672, 0.2676,\n",
       "           0.2839, 0.2695, 0.2829, 0.2695, 0.2864, 0.2839, 0.2675, 0.2876, 0.2929,\n",
       "           0.2945, 0.2675, 0.2675, 0.2675, 0.2675, 0.2675, 0.2950, 0.2950, 0.2675,\n",
       "           0.2950, 0.2675, 0.2950, 0.2675, 0.2950, 0.2675, 0.2950, 0.2675, 0.2675,\n",
       "           0.2950, 0.2675, 0.2675, 0.2675, 0.2675, 0.2950, 0.2675, 0.2950, 0.2950,\n",
       "           0.2950, 0.2675, 0.2950, 0.2950, 0.2675, 0.2675, 0.2950, 0.2950, 0.2675,\n",
       "           0.2675, 0.2950, 0.2675, 0.3212, 0.3202, 0.3227, 0.3557, 0.3240, 0.3173,\n",
       "           0.3066, 0.3065, 0.3026, 0.2992, 0.2980, 0.3065, 0.3047, 0.3008, 0.3022,\n",
       "           0.2924, 0.2913, 0.2924, 0.2901, 0.2987, 0.2901, 0.2985, 0.3005, 0.2996,\n",
       "           0.2899, 0.2895, 0.3009, 0.2889, 0.3008, 0.2866, 0.3024, 0.2901, 0.3017,\n",
       "           0.3054, 0.2897, 0.2923, 0.3134, 0.2913, 0.3084, 0.2950, 0.3084, 0.2916,\n",
       "           0.2916, 0.2916, 0.3084, 0.2916, 0.2916]),\n",
       "   tensor([0.3347, 0.3682, 0.3347,  ..., 0.2350, 0.2302, 0.2355]),\n",
       "   tensor([0.5199, 0.7914, 0.5199,  ..., 0.3147, 0.2958, 0.3147])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([0.6480], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.6336], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-1.6213], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([0.0316], grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.1697], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.8198], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([0.3179], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.6747], grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.2849]),\n",
       "   tensor([0.6361]),\n",
       "   tensor([0.2944]),\n",
       "   tensor([0.2965])]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "class SurfaceBatchNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features=1, \n",
    "        momentum=0.1\n",
    "    ):\n",
    "        super(SurfaceBatchNorm, self).__init__()\n",
    "        self.log_moneyness_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.time_to_maturity_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_return_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_volatility_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.treasury_rate_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.iv_mean_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.iv_std_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Concatenate all tensors from the Input Surface into one tensor for each feature\n",
    "        input_surface_log_moneyness = torch.cat([x for x in batch['Input Surface']['Log Moneyness']])\n",
    "        input_surface_time_to_maturity = torch.cat([x for x in batch['Input Surface']['Time to Maturity']])\n",
    "\n",
    "        # Concatenate Input Surface tensors with Query Points tensors\n",
    "        total_log_moneyness = torch.cat([input_surface_log_moneyness] + [x for x in batch['Query Points']['Log Moneyness']])\n",
    "        total_time_to_maturity = torch.cat([input_surface_time_to_maturity] + [x for x in batch['Query Points']['Time to Maturity']])\n",
    "\n",
    "        # Normalize Log Moneyness and Time to Maturity\n",
    "        norm_log_moneyness = self.log_moneyness_bn(total_log_moneyness.unsqueeze(1)).squeeze(1)\n",
    "        norm_time_to_maturity = self.time_to_maturity_bn(total_time_to_maturity.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Split the normalized results back to corresponding structures\n",
    "        input_surface_sizes = [len(x) for x in batch['Input Surface']['Log Moneyness']]\n",
    "        query_points_sizes = [len(x) for x in batch['Query Points']['Log Moneyness']]\n",
    "        total_input_size = sum(input_surface_sizes)\n",
    "\n",
    "        # Normalizing Market Features\n",
    "        market_features = batch['Market Features']\n",
    "        norm_market_return = self.market_return_bn(market_features['Market Return'].unsqueeze(1)).squeeze(1)\n",
    "        norm_market_volatility = self.market_volatility_bn(market_features['Market Volatility'].unsqueeze(1)).squeeze(1)\n",
    "        norm_treasury_rate = self.treasury_rate_bn(market_features['Treasury Rate'].unsqueeze(1)).squeeze(1)\n",
    "        norm_iv_mean = self.iv_mean_bn(market_features['IV Mean'].unsqueeze(1)).squeeze(1)\n",
    "        norm_iv_std = self.iv_std_bn(market_features['IV Std.'].unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Reconstructing the batch with normalized data\n",
    "        output = {\n",
    "            'Datetime': batch['Datetime'],\n",
    "            'Symbol': batch['Symbol'],\n",
    "            'Mask Proportion': batch['Mask Proportion'],\n",
    "            'Market Features': {\n",
    "                'Market Return': norm_market_return,\n",
    "                'Market Volatility': norm_market_volatility,\n",
    "                'Treasury Rate': norm_treasury_rate,\n",
    "                'IV Mean': norm_iv_mean,\n",
    "                'IV Std.': norm_iv_std\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[:total_input_size], input_surface_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[:total_input_size], input_surface_sizes)),\n",
    "                'Implied Volatility': batch['Input Surface']['Implied Volatility']\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[total_input_size:], query_points_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[total_input_size:], query_points_sizes)),\n",
    "                'Implied Volatility': batch['Query Points']['Implied Volatility']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Ensure requires_grad is True for query point values\n",
    "        for key in output['Query Points']:\n",
    "            if key != 'Implied Volatility':  # We only set requires_grad for Log Moneyness and Time to Maturity\n",
    "                for tensor in output['Query Points'][key]:\n",
    "                    tensor.requires_grad_()\n",
    "\n",
    "        return output\n",
    "\n",
    "# Usage\n",
    "surfacebatchnorm = SurfaceBatchNorm()\n",
    "processed_batch = surfacebatchnorm(batch)\n",
    "processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class EllipticalRBFKernel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        bandwidth, \n",
    "        remove_kernel=False\n",
    "    ):\n",
    "        super(EllipticalRBFKernel, self).__init__()\n",
    "        self.bandwidth = bandwidth\n",
    "        # Initialize the log of the scale vector to zero, which corresponds to scale factors of one\n",
    "        self.log_scale = nn.Parameter(torch.zeros(input_dim))\n",
    "        self.remove_kernel = remove_kernel\n",
    "\n",
    "    def forward(self, distances):\n",
    "        if self.remove_kernel:\n",
    "            # Create a mask for the condition check\n",
    "            all_zeros = torch.all(distances==0.0, dim=-1)\n",
    "            result = torch.where(\n",
    "                all_zeros, \n",
    "                torch.full(distances.shape[:-1], 1.0, device=distances.device),\n",
    "                torch.full(distances.shape[:-1], 1e-10, device=distances.device)\n",
    "            )\n",
    "            return result\n",
    "        # Convert log scale to actual scale values\n",
    "        scale = torch.exp(self.log_scale)\n",
    "        \n",
    "        # Calculate the scaled distances\n",
    "        scaled_distances = (distances ** 2) * scale  # Element-wise multiplication by scale\n",
    "\n",
    "        # Normalize by the trace of the scale matrix\n",
    "        trace_scale_matrix = torch.sum(scale)\n",
    "        normalized_distances = torch.sum(scaled_distances, dim=-1) / trace_scale_matrix\n",
    "\n",
    "        # Compute the RBF kernel output using the normalized distances\n",
    "        kernel_values = torch.exp(-normalized_distances / (2 * self.bandwidth ** 2))\n",
    "\n",
    "        return kernel_values\n",
    "\n",
    "class SurfaceContinuousKernelPositionalEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceContinuousKernelPositionalEmbedding, self).__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "        self.remove_positional_embedding = remove_positional_embedding\n",
    "\n",
    "        # Initialize multiple RBF kernels, each with a different fixed bandwidth\n",
    "        self.kernels = nn.ModuleList()\n",
    "        for i in range(1, d_embedding + 1):\n",
    "            bandwidth_value = torch.erfinv(torch.tensor(i / (d_embedding + 1))) * np.sqrt(2)\n",
    "            self.kernels.append(\n",
    "                EllipticalRBFKernel(\n",
    "                    bandwidth=bandwidth_value, \n",
    "                    input_dim=2, \n",
    "                    remove_kernel=remove_kernel\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.input_surface_layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.query_points_layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "        # Initialize learnable scaling parameter (the base for positional embedding)\n",
    "        self.log_scale = nn.Parameter(torch.log(torch.tensor(10000.0)))\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_surface_batch, \n",
    "        query_points_batch\n",
    "    ):\n",
    "        batch_size = len(input_surface_batch['Log Moneyness'])\n",
    "\n",
    "        input_surface_embeddings = []\n",
    "        query_points_embeddings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Extract the coordinates and implied volatilities for each surface in the batch\n",
    "            surface_coords = torch.stack([\n",
    "                input_surface_batch['Log Moneyness'][i], \n",
    "                input_surface_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "            surface_ivs = input_surface_batch['Implied Volatility'][i]\n",
    "\n",
    "            query_coords = torch.stack([\n",
    "                query_points_batch['Log Moneyness'][i], \n",
    "                query_points_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "\n",
    "            all_coords = torch.cat((surface_coords, query_coords), dim=0)\n",
    "\n",
    "            # Compute the pairwise differences between all points and the input surface points\n",
    "            point_differences = all_coords.unsqueeze(1) - surface_coords.unsqueeze(0)  # (n+m, n, 2)\n",
    "\n",
    "            # Initialize the output embeddings for the current surface with d_embedding channels\n",
    "            all_embedded = torch.zeros((all_coords.shape[0], self.d_embedding), dtype=torch.float32, device=surface_coords.device)\n",
    "\n",
    "            for kernel_idx, kernel in enumerate(self.kernels):\n",
    "                # Apply the RBF kernel to each distance vector \n",
    "                kernel_outputs = kernel(point_differences)\n",
    "\n",
    "                # Compute the weighted sum of IVs based on the kernel outputs\n",
    "                weighted_sum = (kernel_outputs * surface_ivs.unsqueeze(0)).sum(dim=1)\n",
    "                normalization_factor = kernel_outputs.sum(dim=1)\n",
    "\n",
    "                all_embedded[:, kernel_idx] = weighted_sum / normalization_factor    \n",
    "\n",
    "            # Split the embeddings into input surface and query points embeddings\n",
    "            input_surface_embedded = all_embedded[:surface_coords.shape[0], :]\n",
    "            query_points_embedded = all_embedded[surface_coords.shape[0]:, :]\n",
    "\n",
    "            # Normalize the embedded surfaces\n",
    "            input_surface_embedded = self.input_surface_layer_norm(input_surface_embedded)\n",
    "            query_points_embedded = self.query_points_layer_norm(query_points_embedded)\n",
    "\n",
    "            # Positional embedding for input surface points\n",
    "            input_surface_pe = self._compute_positional_embedding(surface_coords)\n",
    "\n",
    "            # Positional embedding for query points\n",
    "            query_points_pe = self._compute_positional_embedding(query_coords)\n",
    "\n",
    "            # Add positional embeddings with a factor of sqrt(2)\n",
    "            input_surface_final = input_surface_embedded + input_surface_pe * np.sqrt(2)\n",
    "            query_points_final = query_points_embedded + query_points_pe * np.sqrt(2)\n",
    "\n",
    "            # Append the encoded surface for this input surface to the batch list\n",
    "            input_surface_embeddings.append(input_surface_final)\n",
    "            query_points_embeddings.append(query_points_final)\n",
    "\n",
    "        # Keep all encoded surfaces as lists to handle variable lengths\n",
    "        return {\n",
    "            'Input Surface': input_surface_embeddings,\n",
    "            'Query Points': query_points_embeddings\n",
    "        }\n",
    "\n",
    "    def _compute_positional_embedding(\n",
    "        self, \n",
    "        coords, \n",
    "    ):\n",
    "        positional_embedding = torch.zeros(coords.size(0), self.d_embedding, device=coords.device)\n",
    "\n",
    "        if not self.remove_positional_embedding:\n",
    "            for i in range(self.d_embedding // 4):\n",
    "                div_factor = torch.exp(self.log_scale) ** (4 * i / self.d_embedding)\n",
    "                positional_embedding[:, 4 * i] = torch.sin(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 1] = torch.cos(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 2] = torch.sin(coords[:, 1] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 3] = torch.cos(coords[:, 1] / div_factor)\n",
    "\n",
    "        return positional_embedding\n",
    "\n",
    "# Example of initializing and using this module\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "\n",
    "# continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "# kernel_positional_embedded_batch = continuous_kernel_positional_embedding(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "# kernel_positional_embedded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import psutil\n",
    "# import os\n",
    "# def print_memory_usage(stage):\n",
    "#     process = psutil.Process(os.getpid())\n",
    "#     print(f\"[{stage}] Memory Usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "# def initialize_model(d_embedding):\n",
    "#     model = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "#     return model\n",
    "\n",
    "# def run_model(model, processed_batch):\n",
    "#     return model(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "\n",
    "# print_memory_usage(\"Before initialization\")\n",
    "# model = initialize_model(d_embedding)\n",
    "# print_memory_usage(\"After initialization\")\n",
    "\n",
    "# kernel_positional_embedded_batch = run_model(model, processed_batch)\n",
    "# print_memory_usage(\"After model run\")\n",
    "\n",
    "# # Clear references and collect garbage\n",
    "# del model, kernel_positional_embedded_batch\n",
    "# gc.collect()\n",
    "# print_memory_usage(\"After cleanup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "# import os\n",
    "\n",
    "# def print_memory_usage():\n",
    "#     process = psutil.Process(os.getpid())\n",
    "#     mem_info = process.memory_info()\n",
    "#     print(f\"RSS: {mem_info.rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# print_memory_usage()\n",
    "\n",
    "# # Cleanup to free up RAM\n",
    "# del continuous_kernel_positional_embedding\n",
    "# del kernel_positional_embedded_batch\n",
    "# del processed_batch\n",
    "\n",
    "# print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def run_model(processed_batch, HYPERPARAMETERS):\n",
    "#     d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']\n",
    "#     continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "#     kernel_positional_embedded_batch = continuous_kernel_positional_embedding(\n",
    "#         processed_batch['Input Surface'], processed_batch['Query Points']\n",
    "#     )\n",
    "#     return kernel_positional_embedded_batch\n",
    "\n",
    "# import torch\n",
    "\n",
    "# def hook(module, input, output):\n",
    "#     print(f\"Memory allocated: {torch.cuda.memory_allocated()}\")\n",
    "#     print(f\"Max memory allocated: {torch.cuda.max_memory_allocated()}\")\n",
    "#     print(f\"Memory reserved: {torch.cuda.memory_reserved()}\")\n",
    "#     print(f\"Max memory reserved: {torch.cuda.max_memory_reserved()}\")\n",
    "\n",
    "# continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "\n",
    "# # Register the hook to your model layers\n",
    "# for name, module in continuous_kernel_positional_embedding.named_modules():\n",
    "#     module.register_forward_hook(hook)\n",
    "    \n",
    "# kernel_positional_embedded_batch = continuous_kernel_positional_embedding(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SurfaceEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        momentum=0.1,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceEmbedding, self).__init__()\n",
    "        self.batch_norm = SurfaceBatchNorm(num_features=1, momentum=momentum)\n",
    "        self.kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding, remove_kernel, remove_positional_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.mask_token = nn.Parameter(torch.randn(d_embedding))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Apply batch normalization\n",
    "        norm_batch = self.batch_norm(batch)\n",
    "\n",
    "        # Extract market features from processed batch and create external_features_batch tensor\n",
    "        market_features = norm_batch['Market Features']\n",
    "        external_features_batch = torch.stack([\n",
    "            market_features['Market Return'],\n",
    "            market_features['Market Volatility'],\n",
    "            market_features['Treasury Rate'],\n",
    "            market_features['IV Mean'],\n",
    "            market_features['IV Std.']\n",
    "        ], dim=-1)  # (batch, features)\n",
    "\n",
    "        # Compute kernel and positional embeddings\n",
    "        embeddings = self.kernel_positional_embedding(norm_batch['Input Surface'], norm_batch['Query Points'])\n",
    "\n",
    "        input_surface_embeddings = embeddings['Input Surface']\n",
    "        query_points_embeddings = embeddings['Query Points']\n",
    "\n",
    "        embedded_sequences = []\n",
    "\n",
    "        for input_surface_embedding, query_points_embedding in zip(input_surface_embeddings, query_points_embeddings):\n",
    "            # Add mask token to the query point embeddings\n",
    "            masked_query_points_embedding = query_points_embedding + self.mask_token\n",
    "\n",
    "            # Combine input surface embeddings and masked query points embeddings\n",
    "            combined_sequence = torch.cat((input_surface_embedding, masked_query_points_embedding), dim=0)\n",
    "\n",
    "            # Apply layer normalization\n",
    "            combined_sequence = self.layer_norm(combined_sequence)\n",
    "\n",
    "            embedded_sequences.append(combined_sequence)\n",
    "\n",
    "        return embedded_sequences, external_features_batch\n",
    "\n",
    "\n",
    "# # Example of initializing and using this module\n",
    "# d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "# surface_embedding = SurfaceEmbedding(d_embedding=d_embedding)\n",
    "# embedded_sequences_batch, external_features_batch = surface_embedding(batch)\n",
    "# embedded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualNorm(nn.Module):\n",
    "    def __init__(self, d_embedding):\n",
    "        super(ResidualNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x, \n",
    "        sublayer_output\n",
    "    ):\n",
    "        return self.norm(x + sublayer_output)\n",
    "    \n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        gate_dropout,\n",
    "        weight_initializer_std=0.02,\n",
    "        bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(GatedAttentionFusion, self).__init__()\n",
    "        self.gate_layer = nn.Sequential(\n",
    "            nn.Linear(d_embedding * 2, d_embedding),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(gate_dropout)\n",
    "        )\n",
    "        self.remove_external_attention = remove_external_attention\n",
    "        self.remove_gate = remove_gate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for module in self.gate_layer:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        self_attention_output, \n",
    "        external_attention_output\n",
    "    ):\n",
    "        if self.remove_external_attention:\n",
    "\n",
    "            return self_attention_output\n",
    "\n",
    "        if self.remove_gate:  \n",
    "\n",
    "            return self_attention_output + external_attention_output\n",
    "        # Concatenate self-attention and external attention outputs\n",
    "        concatenated_output = torch.cat((self_attention_output, external_attention_output), dim=-1)\n",
    "        # Compute gate values\n",
    "        gate_values = self.gate_layer(concatenated_output)\n",
    "        # Calculate gated embedding\n",
    "        gated_embedding = gate_values * self_attention_output + (1 - gate_values) * external_attention_output\n",
    "\n",
    "        return gated_embedding\n",
    "    \n",
    "    \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        ffn_hidden_dim, \n",
    "        ffn_dropout, \n",
    "        layer_depth, \n",
    "        weight_initializer_std=0.02, \n",
    "        bias_initializer_value=0,\n",
    "    ):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_embedding, ffn_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ffn_dropout),\n",
    "            nn.Linear(ffn_hidden_dim, d_embedding),\n",
    "            nn.Dropout(ffn_dropout)\n",
    "        )\n",
    "\n",
    "        self.layer_depth = layer_depth\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feedforward(x)\n",
    "    \n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for i, module in enumerate(self.feedforward):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "                \n",
    "                # Rescale the output matrices of the last linear projection\n",
    "                if i == len(self.feedforward) - 2:\n",
    "                    scale_factor = 1 / (2 * self.layer_depth) ** 0.5\n",
    "                    module.weight.data *= scale_factor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        n_heads, \n",
    "        ffn_hidden_dim, \n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        layer_depth,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.residual_norm_self_attention = ResidualNorm(d_embedding)\n",
    "        self.external_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            kdim=external_dim, \n",
    "            vdim=external_dim, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.residual_norm_external_attention = ResidualNorm(d_embedding)\n",
    "        self.gated_attention_fusion = GatedAttentionFusion(\n",
    "            d_embedding, \n",
    "            gate_dropout,\n",
    "            weight_initializer_std,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention, \n",
    "            remove_gate,\n",
    "        )\n",
    "        self.residual_norm_fusion = ResidualNorm(d_embedding)\n",
    "        self.feed_forward = FeedForwardNetwork(\n",
    "            d_embedding, \n",
    "            ffn_hidden_dim, \n",
    "            ffn_dropout, \n",
    "            layer_depth, \n",
    "            weight_initializer_std, \n",
    "            linear_bias_initializer_value\n",
    "        )\n",
    "        self.residual_norm_ffn = ResidualNorm(d_embedding)\n",
    "        # Initialize self-attention\n",
    "        self._initialize_attention_weights(self.self_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "        # Initialize external-attention\n",
    "        self._initialize_attention_weights(self.external_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "\n",
    "    def _initialize_attention_weights(\n",
    "        self, \n",
    "        attention_module, \n",
    "        weight_initializer_std, \n",
    "        linear_bias_initializer_value, \n",
    "        layer_depth\n",
    "    ):\n",
    "        if attention_module._qkv_same_embed_dim:\n",
    "            nn.init.normal_(attention_module.in_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "        else:\n",
    "            nn.init.normal_(attention_module.q_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.k_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.v_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "\n",
    "        if attention_module.in_proj_bias is not None:\n",
    "            nn.init.constant_(attention_module.in_proj_bias, linear_bias_initializer_value)\n",
    "            nn.init.constant_(attention_module.out_proj.bias, linear_bias_initializer_value)\n",
    "        \n",
    "        if attention_module.bias_k is not None:\n",
    "            nn.init.constant_(attention_module.bias_k, linear_bias_initializer_value)\n",
    "        if attention_module.bias_v is not None:\n",
    "            nn.init.constant_(attention_module.bias_v, linear_bias_initializer_value)\n",
    "        \n",
    "        # Transformer layer rescaling for output weights\n",
    "        scale_factor = 1 / (2 * layer_depth) ** 0.5\n",
    "        nn.init.normal_(attention_module.out_proj.weight, mean=0.0, std=weight_initializer_std * scale_factor)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        surface_embeddings, \n",
    "        external_features,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Self-Attention\n",
    "        self_attention_output, self_attention_weights = self.self_attention(surface_embeddings, surface_embeddings, surface_embeddings)\n",
    "        self_attention_output = self.residual_norm_self_attention(surface_embeddings, self_attention_output)\n",
    "        # External Attention\n",
    "        external_attention_output, external_attention_weights = self.external_attention(surface_embeddings, external_features, external_features) \n",
    "        external_attention_output = self.residual_norm_external_attention(surface_embeddings, external_attention_output)\n",
    "        # Gated Attention Fusion\n",
    "        gated_embedding = self.gated_attention_fusion(self_attention_output, external_attention_output)\n",
    "        gated_embedding = self.residual_norm_fusion(surface_embeddings, gated_embedding)\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.feed_forward(gated_embedding)\n",
    "        # Final Residual Connection and Layer Normalization\n",
    "        surface_embeddings = self.residual_norm_ffn(gated_embedding, ffn_output)\n",
    "\n",
    "        if output_attention_map:\n",
    "            # Remove the batch dimension for attention weights\n",
    "            return surface_embeddings, self_attention_weights.squeeze(0), external_attention_weights.squeeze(0)\n",
    "        \n",
    "        return surface_embeddings, None, None\n",
    "\n",
    "class SurfaceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(SurfaceEncoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            Encoder(\n",
    "                d_embedding, \n",
    "                n_heads, \n",
    "                ffn_hidden_dim, \n",
    "                attention_dropout, \n",
    "                gate_dropout,\n",
    "                ffn_dropout,\n",
    "                external_dim,\n",
    "                (i + 1),\n",
    "                weight_initializer_std,\n",
    "                linear_bias_initializer_value,\n",
    "                gate_bias_initializer_value,\n",
    "                remove_external_attention,\n",
    "                remove_gate\n",
    "            )\n",
    "            for i in range(num_encoder_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        embedded_sequences_batch, \n",
    "        external_features_batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        batch_size = len(embedded_sequences_batch)\n",
    "        encoded_sequences_batch = []\n",
    "        self_attention_maps = []\n",
    "        external_attention_maps = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            surface_embeddings = embedded_sequences_batch[i].unsqueeze(1) \n",
    "            external_features = external_features_batch[i].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            for j, encoder in enumerate(self.encoders):\n",
    "                if j == len(self.encoders) - 1 and output_attention_map:\n",
    "                    surface_embeddings, self_attention_map, external_attention_map = encoder(surface_embeddings, external_features, output_attention_map)\n",
    "                    \n",
    "                else:\n",
    "                    surface_embeddings, _, _ = encoder(surface_embeddings, external_features)\n",
    "                \n",
    "            encoded_sequences_batch.append(surface_embeddings.squeeze(1))\n",
    "            if output_attention_map:\n",
    "                self_attention_maps.append(self_attention_map)\n",
    "                external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return encoded_sequences_batch, self_attention_maps, external_attention_maps\n",
    "        \n",
    "        return encoded_sequences_batch, None, None    \n",
    "\n",
    "# Example of initializing and using these modules\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "external_dim = 5\n",
    "\n",
    "surface_encoder = SurfaceEncoder(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim, \n",
    "    attention_dropout, \n",
    "    gate_dropout, \n",
    "    ffn_dropout, \n",
    "    external_dim, \n",
    ")\n",
    "\n",
    "# Assume embedded_sequences_batch is the output of the SurfaceEmbedding module and\n",
    "# external_features is the formatted external market features batch\n",
    "# encoded_sequences_batch, self_attention_map_batch, external_attention_map_batch = surface_encoder(embedded_sequences_batch, external_features_batch)\n",
    "# encoded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IvySPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0168], grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0115], grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0085], grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0136], grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class IvySPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(IvySPT, self).__init__()\n",
    "        self.surface_embedding = SurfaceEmbedding(\n",
    "            d_embedding, \n",
    "            remove_kernel, \n",
    "            remove_positional_embedding\n",
    "        )\n",
    "        self.surface_encoder = SurfaceEncoder(\n",
    "            d_embedding, \n",
    "            num_encoder_blocks,\n",
    "            n_heads, \n",
    "            ffn_hidden_dim,\n",
    "            attention_dropout, \n",
    "            gate_dropout,\n",
    "            ffn_dropout,\n",
    "            external_dim,\n",
    "            weight_initializer_std,\n",
    "            linear_bias_initializer_value,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention,\n",
    "            remove_gate\n",
    "        )\n",
    "        self.final_layer = nn.Linear(d_embedding, 1)\n",
    "        nn.init.normal_(self.final_layer.weight, mean=0.0, std=weight_initializer_std * (1 / (2 * (num_encoder_blocks + 1)) ** 0.5))\n",
    "        nn.init.constant_(self.final_layer.bias, linear_bias_initializer_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Obtain the embedded sequences and external features from the SurfaceEmbedding module\n",
    "        embedded_sequences_batch, external_features_batch = self.surface_embedding(batch)\n",
    "\n",
    "        # Encode the sequences using the SurfaceEncoder module\n",
    "        encoded_sequences_batch, self_attention_maps, external_attention_maps = self.surface_encoder(\n",
    "            embedded_sequences_batch, \n",
    "            external_features_batch, \n",
    "            output_attention_map\n",
    "        )\n",
    "\n",
    "        # List to hold the implied volatility estimates for each query point in the batch\n",
    "        iv_estimates_batch = []\n",
    "\n",
    "        query_self_attention_maps = []\n",
    "        query_external_attention_maps = []\n",
    "\n",
    "        for i in range(len(encoded_sequences_batch)):\n",
    "            # Extract the encoded sequence\n",
    "            encoded_sequence = encoded_sequences_batch[i]\n",
    "\n",
    "            # Determine the number of query points for this sequence\n",
    "            num_query_points = len(batch['Query Points']['Log Moneyness'][i])\n",
    "\n",
    "            # Extract the encoded query points (last num_query_points elements in the sequence)\n",
    "            encoded_query_points = encoded_sequence[-num_query_points:]\n",
    "\n",
    "            # Estimate the implied volatility for each query point using the fully connected layer\n",
    "            iv_estimates = self.final_layer(encoded_query_points).squeeze(-1)\n",
    "\n",
    "            # Append the estimates to the batch list\n",
    "            iv_estimates_batch.append(iv_estimates)\n",
    "\n",
    "            if output_attention_map:\n",
    "                # Extract the attention maps for the query points\n",
    "                self_attention_map = self_attention_maps[i][-num_query_points:]\n",
    "                external_attention_map = external_attention_maps[i][-num_query_points:]\n",
    "\n",
    "                query_self_attention_maps.append(self_attention_map)\n",
    "                query_external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return iv_estimates_batch, self_attention_maps, external_attention_maps\n",
    "        \n",
    "        return iv_estimates_batch, None, None\n",
    "\n",
    "# Example of initializing and using this module\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "external_dim = 5\n",
    "\n",
    "ivy_spt = IvySPT(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim,\n",
    "    attention_dropout, \n",
    "    gate_dropout,\n",
    "    ffn_dropout,\n",
    "    external_dim\n",
    ")\n",
    "\n",
    "# Pass the batch through the IvySPT model to get implied volatility estimates\n",
    "iv_estimates_batch, self_attention_maps, external_attention_maps = ivy_spt(batch, output_attention_map=False)\n",
    "gc.collect()\n",
    "iv_estimates_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.2849]), tensor([0.6361]), tensor([0.2944]), tensor([0.2965])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['Query Points']['Implied Volatility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1559, grad_fn=<AddBackward0>),\n",
       " tensor(0.1559, grad_fn=<MeanBackward0>),\n",
       " tensor(0., grad_fn=<MeanBackward0>),\n",
       " tensor(0., grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SurfaceArbitrageFreeLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mse_coeff=1,\n",
    "        calendar_coeff=1,\n",
    "        butterfly_coeff=1\n",
    "    ):\n",
    "        super(SurfaceArbitrageFreeLoss, self).__init__()\n",
    "        self.mse_coeff = mse_coeff\n",
    "        self.calendar_coeff = calendar_coeff\n",
    "        self.butterfly_coeff = butterfly_coeff\n",
    "\n",
    "    def forward(self, iv_estimates_batch, batch):\n",
    "        mse_losses = []\n",
    "        calendar_arbitrage_losses = []\n",
    "        butterfly_arbitrage_losses = []\n",
    "\n",
    "        for iv_estimates, target_volatility, time_to_maturity, log_moneyness in zip(\n",
    "            iv_estimates_batch, \n",
    "            batch['Query Points']['Implied Volatility'], \n",
    "            batch['Query Points']['Time to Maturity'], \n",
    "            batch['Query Points']['Log Moneyness']\n",
    "        ):\n",
    "            # Calculate mean squared error between model estimates and target volatilities\n",
    "            mse_loss = F.mse_loss(iv_estimates, target_volatility, reduction='none')\n",
    "            mse_losses.append(mse_loss)\n",
    "\n",
    "            # Calculate the total implied variance\n",
    "            total_implied_variance = time_to_maturity * iv_estimates.pow(2)\n",
    "\n",
    "            # Compute gradients needed for arbitrage conditions\n",
    "            w_t = torch.autograd.grad(total_implied_variance.sum(), time_to_maturity, create_graph=True)[0] \n",
    "            w_x = torch.autograd.grad(total_implied_variance.sum(), log_moneyness, create_graph=True)[0]\n",
    "            w_xx = torch.autograd.grad(w_x.sum(), log_moneyness, create_graph=True)[0]\n",
    "\n",
    "            # Calculate Calendar Arbitrage Loss\n",
    "            calendar_arbitrage_loss = torch.clamp(-w_t, min=0) ** 2\n",
    "            calendar_arbitrage_losses.append(calendar_arbitrage_loss)\n",
    "\n",
    "            # Calculate Butterfly Arbitrage Loss\n",
    "            w = total_implied_variance\n",
    "            g = (1 - log_moneyness * w_x / (2 * w)) ** 2 - w_x / 4 * (1 / w + 1 / 4) + w_xx / 2\n",
    "            butterfly_arbitrage_loss = torch.clamp(-g, min=0) ** 2\n",
    "            butterfly_arbitrage_losses.append(butterfly_arbitrage_loss)\n",
    "\n",
    "        # Combine all losses\n",
    "        mse_loss = torch.cat(mse_losses).mean()\n",
    "        calendar_arbitrage_loss = torch.cat(calendar_arbitrage_losses).mean()\n",
    "        butterfly_arbitrage_loss = torch.cat(butterfly_arbitrage_losses).mean()\n",
    "\n",
    "        total_loss = self.mse_coeff * mse_loss + self.calendar_coeff * calendar_arbitrage_loss + \\\n",
    "            self.butterfly_coeff * butterfly_arbitrage_loss\n",
    "\n",
    "        return total_loss, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss\n",
    "\n",
    "surface_arbitrage_free_loss = SurfaceArbitrageFreeLoss()  \n",
    "total_loss, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss = surface_arbitrage_free_loss(iv_estimates_batch, batch)\n",
    "total_loss, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
