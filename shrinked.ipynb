{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Number of Query Points' : None,\n",
    "        'Batch Size' : 3\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 8,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 16,\n",
    "        'Attention Dropout' : 0.1,\n",
    "        'Gate Dropout' : 0.1,\n",
    "        'FFN Dropout' : 0.1,\n",
    "        'Number of Blocks' : 2,\n",
    "        'External Feature Dimension' : 3,\n",
    "    },\n",
    "    'No-Arbitrage' : {\n",
    "        'Butterfly' : 1,\n",
    "        'Calendar' : 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Log Moneyness</th>\n",
       "      <th>Time to Maturity</th>\n",
       "      <th>Implied Volatility</th>\n",
       "      <th>Market Return</th>\n",
       "      <th>Market Volatility</th>\n",
       "      <th>Treasury Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.157580</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.157580</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.145163</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.145163</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.132898</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-06-28</th>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.401026</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.407931</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.407931</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.414785</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.414785</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574326 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Log Moneyness  Time to Maturity  Implied Volatility  \\\n",
       "Datetime   Symbol                                                        \n",
       "2013-01-02 AAPL        -0.157580          0.007937              0.3726   \n",
       "           AAPL        -0.157580          0.007937              0.6095   \n",
       "           AAPL        -0.145163          0.007937              0.3726   \n",
       "           AAPL        -0.145163          0.007937              0.6095   \n",
       "           AAPL        -0.132898          0.007937              0.3726   \n",
       "...                          ...               ...                 ...   \n",
       "2013-06-28 GOOGL        0.401026          2.253968              0.2430   \n",
       "           GOOGL        0.407931          2.253968              0.2383   \n",
       "           GOOGL        0.407931          2.253968              0.2426   \n",
       "           GOOGL        0.414785          2.253968              0.2402   \n",
       "           GOOGL        0.414785          2.253968              0.2433   \n",
       "\n",
       "                   Market Return  Market Volatility  Treasury Rate  \n",
       "Datetime   Symbol                                                   \n",
       "2013-01-02 AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "...                          ...                ...            ...  \n",
       "2013-06-28 GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "\n",
       "[574326 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_googl_data = pd.read_csv('volatility_surface_AAPL_GOOGL_2013_01_2013_06.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "aapl_googl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yfinance as yf\n",
    "# # Load the data\n",
    "# aapl_googl_data = pd.read_csv('volatility_surface_AAPL_GOOGL_2013_01_2013_06.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "# # Fetch historical close and adjusted close prices for AAPL and GOOGL\n",
    "# aapl = yf.download('AAPL', start='2013-01-01', end='2013-06-30')\n",
    "# googl = yf.download('GOOG', start='2013-01-01', end='2013-06-30')\n",
    "# # Create a dictionary to hold close and adjusted close prices for easy access\n",
    "# prices = {\n",
    "#     'AAPL': {'Close': aapl['Close'], 'Adj Close': aapl['Adj Close']},\n",
    "#     'GOOGL': {'Close': googl['Close'], 'Adj Close': googl['Adj Close']}\n",
    "# }\n",
    "# # Define a function to calculate the modified log moneyness\n",
    "# def modified_log_moneyness(row):\n",
    "#     symbol = row.name[1]\n",
    "#     date = row.name[0]\n",
    "#     close_price = prices[symbol]['Close'][date]\n",
    "#     adj_close_price = prices[symbol]['Adj Close'][date]\n",
    "#     log_moneyness = row['Log Moneyness'] + np.log(close_price / adj_close_price)\n",
    "    \n",
    "#     treasury_rate = row['Treasury Rate']\n",
    "#     time_to_maturity = row['Time to Maturity']\n",
    "#     exponential_treasury_rate = np.log(1 + treasury_rate)\n",
    "#     discount_factor = np.exp(-exponential_treasury_rate * time_to_maturity)\n",
    "    \n",
    "#     return log_moneyness * discount_factor\n",
    "# # Apply the function to each row\n",
    "# aapl_googl_data['Log Moneyness'] = aapl_googl_data.apply(modified_log_moneyness, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': Timestamp('2013-01-02 00:00:00'),\n",
       " 'Symbol': 'AAPL',\n",
       " 'Market Features': {'Market Return': 0.0250861159586972,\n",
       "  'Market Volatility': 14.68000030517578,\n",
       "  'Treasury Rate': 0.0549999997019767},\n",
       " 'Surface': {'Log Moneyness': array([-0.31668849, -0.31668849, -0.30426597, ...,  0.63882295,\n",
       "          0.6483924 ,  0.6483924 ]),\n",
       "  'Time to Maturity': array([0.00793651, 0.00793651, 0.00793651, ..., 2.95634921, 2.95634921,\n",
       "         2.95634921]),\n",
       "  'Implied Volatility': array([0.3726, 0.6095, 0.3726, ..., 0.3387, 0.3342, 0.3389])}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def implied_volatility_surfaces(options_market_data):\n",
    "    # Group the data by Datetime and Symbol\n",
    "    grouped_data = options_market_data.groupby(level=['Datetime', 'Symbol'])\n",
    "\n",
    "    surfaces = []\n",
    "    for (date, symbol), surface in grouped_data:\n",
    "        surface_dict = {\n",
    "            'Datetime': date,\n",
    "            'Symbol': symbol,\n",
    "            'Market Features': {\n",
    "                'Market Return': surface['Market Return'].values[0],\n",
    "                'Market Volatility': surface['Market Volatility'].values[0],\n",
    "                'Treasury Rate': surface['Treasury Rate'].values[0],\n",
    "            },\n",
    "            'Surface': {\n",
    "                'Log Moneyness': surface['Log Moneyness'].values,\n",
    "                'Time to Maturity': surface['Time to Maturity'].values,\n",
    "                'Implied Volatility': surface['Implied Volatility'].values,\n",
    "            }\n",
    "        }\n",
    "        surfaces.append(surface_dict)\n",
    "\n",
    "    return surfaces\n",
    "\n",
    "surfaces = implied_volatility_surfaces(aapl_googl_data)\n",
    "surfaces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL'],\n",
       " 'Mask Proportion': [0.7, 0.5, 0.7],\n",
       " 'Market Features': {'Market Return': tensor([-0.0003, -0.0019, -0.0007]),\n",
       "  'Market Volatility': tensor([15.4400, 13.5700, 13.0200]),\n",
       "  'Treasury Rate': tensor([0.0400, 0.0600, 0.0350]),\n",
       "  'IV Mean': tensor([0.3143, 0.3350, 0.2761]),\n",
       "  'IV Std.': tensor([0.0559, 0.0697, 0.0507])},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-0.2852, -0.2553, -0.1573, -0.1310, -0.1181, -0.0928, -0.0804, -0.0804,\n",
       "           -0.0681, -0.0560, -0.0440, -0.0205,  0.0025,  0.0138,  0.0470,  0.0578,\n",
       "            0.0791,  0.0999,  0.1102,  0.1203,  0.1304,  0.1304,  0.1403,  0.1696,\n",
       "            0.1886,  0.1886,  0.1980,  0.2073, -0.7859, -0.7859, -0.7371, -0.6906,\n",
       "           -0.6906, -0.6462, -0.6247, -0.6036, -0.5430, -0.5236, -0.5045, -0.4858,\n",
       "           -0.4318, -0.4144, -0.3973, -0.3805, -0.3639, -0.3159, -0.3004, -0.3004,\n",
       "           -0.2553, -0.2263, -0.1981, -0.1707, -0.1707, -0.1573, -0.1181, -0.1054,\n",
       "           -0.0928, -0.0928, -0.0804, -0.0560, -0.0440, -0.0322, -0.0089,  0.0360,\n",
       "            0.0470,  0.0578,  0.0685,  0.0895,  0.1203,  0.1304,  0.1502,  0.1599,\n",
       "            0.1696,  0.1886,  0.1980,  0.2165,  0.2347,  0.2701,  0.2788,  0.2874,\n",
       "            0.3292,  0.3694,  0.3850,  0.3850,  0.3927,  0.3927,  0.4004,  0.4230,\n",
       "            0.4230,  0.4378,  0.4739,  0.4739,  0.5088,  0.5156,  0.5224,  0.5291,\n",
       "            0.5291,  0.5358,  0.5491,  0.5686,  0.5750,  0.5941,  0.6004,  0.6004,\n",
       "            0.6432,  0.6551,  0.6610,  0.6669,  0.6785,  0.6842,  0.6900,  0.6957,\n",
       "            0.6957,  0.7070,  0.7126,  0.7181,  0.7237,  0.7347,  0.7401,  0.7455,\n",
       "            0.7509,  0.7616,  0.7722,  0.7775,  0.7879, -0.3159, -0.2553, -0.2407,\n",
       "           -0.2263, -0.1707, -0.1441, -0.1310, -0.1181, -0.0928, -0.0804, -0.0681,\n",
       "           -0.0560, -0.0322, -0.0205, -0.0089,  0.0025,  0.0138,  0.0250,  0.0578,\n",
       "            0.0685,  0.1102,  0.1696,  0.1791,  0.1980,  0.2073,  0.2257,  0.2257,\n",
       "            0.2347,  0.2437,  0.2526,  0.2701, -0.1981, -0.1707, -0.1573, -0.1181,\n",
       "           -0.0928, -0.0928, -0.0804, -0.0089, -0.0089,  0.0025,  0.0138,  0.0250,\n",
       "            0.0791,  0.0895,  0.1102,  0.1203,  0.1304,  0.1403,  0.1403,  0.1502,\n",
       "            0.1886,  0.2073,  0.2165,  0.2257,  0.2257,  0.2347,  0.2526,  0.2614,\n",
       "           -0.6247, -0.6036, -0.5830, -0.5628, -0.5045, -0.4495, -0.4318, -0.4318,\n",
       "           -0.4144, -0.3477, -0.3477, -0.3159, -0.2852, -0.2701, -0.2701, -0.2553,\n",
       "           -0.2407, -0.2263, -0.1981, -0.1844, -0.1707, -0.1707, -0.1310, -0.1181,\n",
       "           -0.1181, -0.1054, -0.0928, -0.0560, -0.0560,  0.0025,  0.0138,  0.1304,\n",
       "            0.1403,  0.1980,  0.2347,  0.2701,  0.3043,  0.3127,  0.3455,  0.3694,\n",
       "            0.3850,  0.3927,  0.3927,  0.4155,  0.4305,  0.4524,  0.4524,  0.4810,\n",
       "            0.4880,  0.4950,  0.4950,  0.5019,  0.5088,  0.5156,  0.5224,  0.5358,\n",
       "            0.5425,  0.5491,  0.5491,  0.5750,  0.5814,  0.5878,  0.6066,  0.6066,\n",
       "            0.6128,  0.6189,  0.6251,  0.6372,  0.6372,  0.6492,  0.6551,  0.6669,\n",
       "            0.6727,  0.6785,  0.6785,  0.7126,  0.7237,  0.7292,  0.7401,  0.7455,\n",
       "            0.7509,  0.7563,  0.7616,  0.7616,  0.7669,  0.7722,  0.7775,  0.7930,\n",
       "            0.8084,  0.8135,  0.8185,  0.8335,  0.8433,  0.8433, -0.7859, -0.7371,\n",
       "           -0.7371, -0.7136, -0.6906, -0.6462, -0.6036, -0.5628, -0.5430, -0.4858,\n",
       "           -0.4675, -0.4495, -0.2407, -0.2263, -0.1981, -0.1844, -0.1310, -0.1181,\n",
       "           -0.1054, -0.0928, -0.0804, -0.0804, -0.0681, -0.0560, -0.0089, -0.0089,\n",
       "            0.0025,  0.0138,  0.0250,  0.0360,  0.0578,  0.0999,  0.1304,  0.1403,\n",
       "            0.1403,  0.1599,  0.1886,  0.1980,  0.2437,  0.2614,  0.2788,  0.3127,\n",
       "            0.3210,  0.3292,  0.3615,  0.3694,  0.3927, -0.6906, -0.6681, -0.6462,\n",
       "           -0.6036, -0.5830, -0.5628, -0.4495, -0.4318, -0.3805, -0.3639, -0.3317,\n",
       "           -0.3159, -0.3159, -0.2553, -0.2407, -0.2263, -0.1981, -0.1844, -0.1707,\n",
       "           -0.0804, -0.0681, -0.0322, -0.0205,  0.0025,  0.0025,  0.0138,  0.0360,\n",
       "            0.0470,  0.0578,  0.0685,  0.0791,  0.0895,  0.0895,  0.0999,  0.1102,\n",
       "            0.1502,  0.1886,  0.1980,  0.2073,  0.2165,  0.2347,  0.2437,  0.2959,\n",
       "            0.2959,  0.3043,  0.3127,  0.3210,  0.3694,  0.3772,  0.3927,  0.4155,\n",
       "            0.4155,  0.4230,  0.4230, -0.6036, -0.5236, -0.4675, -0.4495, -0.4495,\n",
       "           -0.3973, -0.3805, -0.3639, -0.3477, -0.3317, -0.3159, -0.3159, -0.3004,\n",
       "           -0.2701, -0.2553, -0.2263, -0.2263, -0.1981, -0.1844, -0.1844, -0.1707,\n",
       "           -0.1707, -0.1441, -0.1441, -0.1181, -0.0681, -0.0560, -0.0440, -0.0322,\n",
       "           -0.0205, -0.0205, -0.0089,  0.0025,  0.0578,  0.0895,  0.1102,  0.1304,\n",
       "            0.1403,  0.1502,  0.1599,  0.1696,  0.1791,  0.1980,  0.2257,  0.2347,\n",
       "            0.2614,  0.3043,  0.3127,  0.3292,  0.3374,  0.3374,  0.3455,  0.3535,\n",
       "            0.3615,  0.3772,  0.3850,  0.3927,  0.4080,  0.4080,  0.4155,  0.4305,\n",
       "            0.4378,  0.4452,  0.4524,  0.4524,  0.4668,  0.4739,  0.4810,  0.4880,\n",
       "            0.5019,  0.5088,  0.5088,  0.5224,  0.5491,  0.5556,  0.5621,  0.5621,\n",
       "            0.5750,  0.5750,  0.5814,  0.5878,  0.6066,  0.6128,  0.6189,  0.6311,\n",
       "            0.6372,  0.6372,  0.6492,  0.6842,  0.6842,  0.6900,  0.6957, -0.7612,\n",
       "           -0.7136, -0.6681, -0.6462, -0.6247, -0.6036, -0.5628, -0.5430, -0.5045,\n",
       "           -0.4858, -0.4675, -0.4675, -0.4318, -0.4144, -0.3805, -0.3805, -0.3317,\n",
       "           -0.3159, -0.3004, -0.2852, -0.2701, -0.2407, -0.2263, -0.2121, -0.1981,\n",
       "           -0.1844, -0.1844, -0.1573, -0.1310, -0.0804, -0.0440, -0.0205,  0.0025,\n",
       "            0.0250,  0.0999,  0.1203,  0.1304,  0.1696,  0.1886,  0.1980,  0.2257,\n",
       "            0.2347,  0.2614,  0.2701,  0.2788,  0.2874,  0.3374,  0.3455,  0.3694,\n",
       "            0.3772,  0.3927,  0.4004,  0.4230,  0.4305,  0.4378,  0.4452,  0.4452,\n",
       "            0.4524,  0.4597,  0.4597,  0.4880,  0.5019,  0.5088,  0.5224,  0.5358,\n",
       "            0.6128,  0.6189,  0.6251,  0.6311,  0.6372,  0.6432,  0.6432,  0.6727,\n",
       "            0.6785,  0.6842,  0.6957,  0.7237,  0.7347,  0.7401,  0.7401,  0.7509,\n",
       "            0.7509,  0.7563,  0.7616,  0.7669,  0.7879,  0.7982,  0.8033,  0.8033,\n",
       "            0.8335,  0.8433,  0.8531,  0.8627, -0.7612, -0.6247, -0.5236, -0.5045,\n",
       "           -0.5045, -0.4858, -0.4495, -0.4495, -0.3805, -0.3639, -0.3639, -0.3317,\n",
       "           -0.3159, -0.3159, -0.3004, -0.2701, -0.2701, -0.2553, -0.2263, -0.1981,\n",
       "           -0.1981, -0.1573, -0.1310, -0.1181, -0.0681, -0.0560, -0.0440, -0.0205,\n",
       "           -0.0089,  0.0025,  0.0138,  0.0250,  0.0250,  0.0360,  0.0578,  0.0685,\n",
       "            0.0791,  0.0999,  0.1403,  0.1599,  0.1791,  0.1980,  0.2073,  0.2165,\n",
       "            0.2257,  0.2347,  0.2437,  0.2526,  0.2614,  0.2788,  0.2788,  0.2959,\n",
       "            0.2959,  0.3043,  0.3210,  0.3292,  0.3615,  0.3694,  0.3850,  0.3927,\n",
       "            0.4004,  0.4004,  0.4080,  0.4230, -0.8112, -0.8112, -0.7371, -0.7136,\n",
       "           -0.6036, -0.5628, -0.5628, -0.4495, -0.3477, -0.2263, -0.1981, -0.1844,\n",
       "           -0.1573, -0.1573, -0.1441, -0.1054, -0.0804, -0.0440, -0.0205,  0.0250,\n",
       "            0.0360,  0.0685,  0.0791,  0.0999,  0.1102,  0.1403,  0.1502,  0.1599,\n",
       "            0.1599,  0.1886,  0.1980,  0.2257,  0.2614,  0.3772,  0.3927,  0.4080,\n",
       "            0.4230,  0.4378,  0.4524,  0.4668,  0.5224,  0.5358,  0.5358,  0.6372,\n",
       "            0.6727,  0.6957,  0.7070,  0.7181,  0.7616,  0.7722,  0.7827,  0.7827,\n",
       "            0.7930,  0.8033,  0.8235,  0.8335,  0.8433,  0.8723,  0.8723]),\n",
       "   tensor([-0.2509, -0.2509, -0.2368,  ...,  0.8284,  0.8381,  0.8477]),\n",
       "   tensor([-0.3741, -0.3582, -0.3504, -0.3504, -0.3426, -0.3046, -0.2897, -0.2679,\n",
       "           -0.2679, -0.2536, -0.2466, -0.2395, -0.2257, -0.2257, -0.2188, -0.2052,\n",
       "           -0.1918, -0.1851, -0.1851, -0.1785, -0.1654, -0.1590, -0.1525, -0.1398,\n",
       "           -0.1335, -0.1210, -0.1148, -0.1025, -0.0844, -0.0784, -0.0725, -0.0607,\n",
       "           -0.0607, -0.0549, -0.0376, -0.0319, -0.0150, -0.0039,  0.0071,  0.0180,\n",
       "            0.0287,  0.0551,  0.0603,  0.0655,  0.0655,  0.0859, -0.3661, -0.3582,\n",
       "           -0.3349, -0.3272, -0.3120, -0.3120, -0.3046, -0.2971, -0.2607, -0.2536,\n",
       "           -0.2466, -0.2257, -0.1918, -0.1851, -0.1654, -0.1398, -0.1335, -0.1272,\n",
       "           -0.1210, -0.1210, -0.1148, -0.1086, -0.1086, -0.1025, -0.0784, -0.0725,\n",
       "           -0.0549, -0.0491, -0.0433, -0.0319, -0.0262,  0.0125,  0.0180,  0.0287,\n",
       "           -0.2466, -0.2188, -0.2052, -0.2052, -0.1785, -0.1525, -0.1525, -0.1398,\n",
       "           -0.1148, -0.1025, -0.1025, -0.0433, -0.0319,  0.0016,  0.0071,  0.0287,\n",
       "           -0.1025, -0.0904, -0.0784, -0.0094, -0.0094,  0.0125,  0.0341,  0.0655,\n",
       "            0.0859, -1.0127, -0.9977, -0.9829, -0.9257, -0.9119, -0.9119, -0.8849,\n",
       "           -0.8849, -0.8717, -0.8457, -0.8329, -0.8079, -0.7716, -0.7716, -0.7597,\n",
       "           -0.7480, -0.7251, -0.7138, -0.7026, -0.7026, -0.6698, -0.6591, -0.6485,\n",
       "           -0.6485, -0.6380, -0.6277, -0.6174, -0.6073, -0.5873, -0.5774, -0.5484,\n",
       "           -0.5110, -0.5110, -0.5019, -0.4839, -0.4839, -0.4488, -0.4317, -0.4149,\n",
       "           -0.3741, -0.3504, -0.3272, -0.3120, -0.2824, -0.2824, -0.2751, -0.2679,\n",
       "           -0.2536, -0.2466, -0.2395, -0.2188, -0.2188, -0.2052, -0.1984, -0.1984,\n",
       "           -0.1918, -0.1851, -0.1335, -0.1086, -0.0964, -0.0844, -0.0784, -0.0666,\n",
       "           -0.0607, -0.0549, -0.0491, -0.0319, -0.0262, -0.0262, -0.0094, -0.0039,\n",
       "            0.0016,  0.0180,  0.0287,  0.0499,  0.0551,  0.0603,  0.0808,  0.0859,\n",
       "            0.0909,  0.0959,  0.1059,  0.1351,  0.1542,  0.1729,  0.2181,  0.2270,\n",
       "            0.2357,  0.2443,  0.2529,  0.2529,  0.2782, -0.6073, -0.5873, -0.5873,\n",
       "           -0.5774, -0.5677, -0.5677, -0.5484, -0.5390, -0.5203, -0.5110, -0.4929,\n",
       "           -0.4839, -0.4750, -0.4662, -0.4575, -0.4488, -0.4488, -0.4402, -0.3984,\n",
       "           -0.3984, -0.3902, -0.3661, -0.3426, -0.3349, -0.3349, -0.3272, -0.3120,\n",
       "           -0.2971, -0.2897, -0.2897, -0.2824, -0.2679, -0.2607, -0.2395, -0.2326,\n",
       "           -0.2257, -0.2052, -0.1984, -0.1918, -0.1851, -0.1785, -0.1654, -0.1590,\n",
       "           -0.1590, -0.1525, -0.1525, -0.1398, -0.1398, -0.1025, -0.0964, -0.0784,\n",
       "           -0.0725, -0.0607, -0.0549, -0.0491, -0.0433, -0.0433, -0.0319, -0.0262,\n",
       "           -0.0262, -0.0150, -0.0094,  0.0071,  0.0071,  0.0180,  0.0287,  0.0341,\n",
       "            0.0394,  0.0551,  0.0655,  0.0706,  0.0706,  0.0859,  0.0909,  0.0909,\n",
       "            0.0959,  0.1108,  0.1206,  0.1255,  0.1255,  0.1303,  0.1303,  0.1495,\n",
       "            0.1542,  0.1589,  0.1729,  0.1729,  0.1821,  0.1958,  0.2093,  0.2181,\n",
       "            0.2226,  0.2226,  0.2270,  0.2313,  0.2357,  0.2400,  0.2400,  0.2443,\n",
       "            0.2443, -0.8849, -0.8329, -0.8204, -0.8079, -0.7957, -0.7957, -0.7597,\n",
       "           -0.7480, -0.7251, -0.7026, -0.6915, -0.6806, -0.6698, -0.6591, -0.6485,\n",
       "           -0.6380, -0.6277, -0.6174, -0.6174, -0.5972, -0.5580, -0.5580, -0.5484,\n",
       "           -0.5296, -0.5203, -0.5203, -0.5019, -0.4750, -0.4402, -0.4317, -0.4149,\n",
       "           -0.4066, -0.4066, -0.3741, -0.3582, -0.3582, -0.3426, -0.3349, -0.3272,\n",
       "           -0.3196, -0.2971, -0.2971, -0.2824, -0.2679, -0.2607, -0.2536, -0.2466,\n",
       "           -0.2326, -0.2188, -0.2120, -0.2052, -0.1984, -0.1918, -0.1851, -0.1785,\n",
       "           -0.1590, -0.1461, -0.1272, -0.1025, -0.0964, -0.0964, -0.0904, -0.0725,\n",
       "           -0.0666, -0.0491, -0.0376, -0.0262, -0.0206, -0.0094,  0.0180,  0.0394,\n",
       "            0.0446,  0.0499,  0.0551,  0.0859,  0.0909,  0.1157,  0.1729,  0.2003,\n",
       "            0.2093,  0.2093,  0.2181,  0.2270,  0.2357,  0.2529,  0.2699,  0.2782,\n",
       "           -0.7957, -0.7251, -0.7251, -0.6915, -0.6698, -0.6698, -0.6380, -0.6174,\n",
       "           -0.5972, -0.5580, -0.5484, -0.5390, -0.5296, -0.5110, -0.4839, -0.4839,\n",
       "           -0.4662, -0.4575, -0.4402, -0.4233, -0.4233, -0.3984, -0.3902, -0.3902,\n",
       "           -0.3821, -0.3821, -0.3741, -0.3504, -0.3196, -0.3120, -0.2971, -0.2824,\n",
       "           -0.2679, -0.2607, -0.2536, -0.2257, -0.2188, -0.2120, -0.2052, -0.1918,\n",
       "           -0.1918, -0.1851, -0.1720, -0.1654, -0.1654, -0.1398, -0.1335, -0.1335,\n",
       "           -0.1210, -0.1025, -0.0844, -0.0844, -0.0784, -0.0784, -0.0725, -0.0666,\n",
       "           -0.0607, -0.0376, -0.0262, -0.0150, -0.0039,  0.0180,  0.0287,  0.0394,\n",
       "            0.0551,  0.0603,  0.0706,  0.0909,  0.1009,  0.1157,  0.1255,  0.2003,\n",
       "            0.2093,  0.2357,  0.2443,  0.2699, -1.0915, -1.0753, -1.0593, -1.0280,\n",
       "           -0.9977, -0.9829, -0.9683, -0.9539, -0.9539, -0.9397, -0.9257, -0.9119,\n",
       "           -0.9119, -0.8849, -0.8717, -0.8586, -0.8457, -0.8204, -0.8204, -0.8079,\n",
       "           -0.8079, -0.7957, -0.7716, -0.7365, -0.7138, -0.6915, -0.6698, -0.6591,\n",
       "           -0.6380, -0.6277, -0.6073, -0.6073, -0.5972, -0.5774, -0.5677, -0.5484,\n",
       "           -0.5390, -0.5019, -0.4929, -0.4750, -0.4662, -0.4575, -0.4317, -0.4149,\n",
       "           -0.3984, -0.3741, -0.3741, -0.3582, -0.3196, -0.2971, -0.2897, -0.2897,\n",
       "           -0.2607, -0.2607, -0.2466, -0.2257, -0.2120, -0.2052, -0.1984, -0.1785,\n",
       "           -0.1785, -0.1720, -0.1720, -0.1654, -0.1590, -0.1525, -0.1086, -0.1025,\n",
       "           -0.0964, -0.0904, -0.0607, -0.0549, -0.0491, -0.0491, -0.0376, -0.0376,\n",
       "           -0.0262, -0.0094, -0.0039,  0.0016,  0.0180,  0.0446,  0.0551,  0.0551,\n",
       "            0.0603,  0.0603,  0.0859,  0.0909,  0.0959,  0.1157,  0.1351,  0.1447,\n",
       "            0.1821,  0.1912,  0.2357,  0.2357,  0.2529,  0.2529, -1.0435, -1.0435,\n",
       "           -0.9829, -0.9539, -0.8204, -0.7716, -0.7716, -0.7480, -0.6380, -0.6380,\n",
       "           -0.6174, -0.5774, -0.5580, -0.5203, -0.4662, -0.4488, -0.3821, -0.3661,\n",
       "           -0.3196, -0.3046, -0.2751, -0.2326, -0.2326, -0.2188, -0.2188, -0.1785,\n",
       "           -0.1272, -0.1148, -0.0904, -0.0904, -0.0666, -0.0319, -0.0206,  0.0125,\n",
       "            0.0234,  0.0341,  0.0655,  0.0655,  0.0757,  0.0757,  0.0859,  0.1059,\n",
       "            0.1059,  0.1157,  0.1255,  0.1351,  0.1351,  0.1542,  0.1636,  0.1729,\n",
       "            0.1912,  0.2093,  0.2181,  0.2181,  0.2270,  0.2614,  0.3029,  0.3110,\n",
       "            0.3191,  0.3270,  0.3349,  0.3506,  0.4034])],\n",
       "  'Time to Maturity': [tensor([0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254]),\n",
       "   tensor([0.0159, 0.0159, 0.0159,  ..., 2.8532, 2.8532, 2.8532]),\n",
       "   tensor([0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437,\n",
       "           0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437,\n",
       "           0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437,\n",
       "           0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310,\n",
       "           0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.1310, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421, 0.2421,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921,\n",
       "           0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.4921, 0.8532, 0.8532, 0.8532,\n",
       "           0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532,\n",
       "           0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532,\n",
       "           0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532,\n",
       "           0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532,\n",
       "           0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532,\n",
       "           0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532,\n",
       "           0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532,\n",
       "           0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532, 0.8532,\n",
       "           0.8532, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643, 0.9643,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087])],\n",
       "  'Implied Volatility': [tensor([0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.4187, 0.3716,\n",
       "           0.3716, 0.3617, 0.3363, 0.3212, 0.3147, 0.3133, 0.3133, 0.3716, 0.3133,\n",
       "           0.3716, 0.3133, 0.3716, 0.3133, 0.3133, 0.3133, 0.3716, 0.3133, 0.3716,\n",
       "           0.3716, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473,\n",
       "           0.3473, 0.3258, 0.3258, 0.3258, 0.3258, 0.3473, 0.3473, 0.3258, 0.3258,\n",
       "           0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3473, 0.3473,\n",
       "           0.3258, 0.3258, 0.3180, 0.3207, 0.3086, 0.2900, 0.2851, 0.2781, 0.2698,\n",
       "           0.2607, 0.2637, 0.2668, 0.2688, 0.2859, 0.3138, 0.2561, 0.2561, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295,\n",
       "           0.3295, 0.3295, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295,\n",
       "           0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.3295, 0.2561, 0.3295, 0.2561,\n",
       "           0.2561, 0.3295, 0.2561, 0.3295, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.3295, 0.3381,\n",
       "           0.3381, 0.3381, 0.3381, 0.3265, 0.3632, 0.3381, 0.3216, 0.3165, 0.3037,\n",
       "           0.3004, 0.2912, 0.2812, 0.2803, 0.2746, 0.2723, 0.2720, 0.2671, 0.2695,\n",
       "           0.2617, 0.2847, 0.2702, 0.2702, 0.3356, 0.2702, 0.3356, 0.2702, 0.3356,\n",
       "           0.2702, 0.2702, 0.3356, 0.3564, 0.3187, 0.3187, 0.3002, 0.2968, 0.2920,\n",
       "           0.2829, 0.2660, 0.2642, 0.2607, 0.2606, 0.2599, 0.2623, 0.2655, 0.2648,\n",
       "           0.2754, 0.2797, 0.2871, 0.2661, 0.2920, 0.2661, 0.3215, 0.2661, 0.3215,\n",
       "           0.2661, 0.3215, 0.3215, 0.2661, 0.3369, 0.3510, 0.3510, 0.3369, 0.3369,\n",
       "           0.3369, 0.3510, 0.3369, 0.3369, 0.3510, 0.3369, 0.3510, 0.3510, 0.3510,\n",
       "           0.3369, 0.3369, 0.3369, 0.3369, 0.3295, 0.3397, 0.3226, 0.3172, 0.3031,\n",
       "           0.2975, 0.2933, 0.2938, 0.2866, 0.2749, 0.2728, 0.2622, 0.2596, 0.2658,\n",
       "           0.2692, 0.2912, 0.3146, 0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.3302, 0.3302, 0.3302,\n",
       "           0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302,\n",
       "           0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.2695, 0.3302, 0.3302, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.3302, 0.3302, 0.2695, 0.3695,\n",
       "           0.5483, 0.3695, 0.3695, 0.5483, 0.5483, 0.5483, 0.5483, 0.5483, 0.3695,\n",
       "           0.5483, 0.3695, 0.3469, 0.4183, 0.3310, 0.3267, 0.3105, 0.3345, 0.3049,\n",
       "           0.3031, 0.3168, 0.3001, 0.2975, 0.3078, 0.2949, 0.2905, 0.2926, 0.2908,\n",
       "           0.2887, 0.2866, 0.2852, 0.2911, 0.2942, 0.2847, 0.2983, 0.2876, 0.2911,\n",
       "           0.3135, 0.3053, 0.3091, 0.3461, 0.3318, 0.3461, 0.3386, 0.3461, 0.3415,\n",
       "           0.3461, 0.3459, 0.6848, 0.3459, 0.6848, 0.6642, 0.3459, 0.5333, 0.5164,\n",
       "           0.4740, 0.4519, 0.4324, 0.4189, 0.3381, 0.3202, 0.3164, 0.3511, 0.3059,\n",
       "           0.3018, 0.2989, 0.2883, 0.2808, 0.2774, 0.2753, 0.2727, 0.2728, 0.2728,\n",
       "           0.2691, 0.2710, 0.2698, 0.2669, 0.2671, 0.2664, 0.2710, 0.2712, 0.2666,\n",
       "           0.2672, 0.2711, 0.2715, 0.2728, 0.2745, 0.2770, 0.2782, 0.2864, 0.3104,\n",
       "           0.3104, 0.2921, 0.3104, 0.3076, 0.3133, 0.3104, 0.3256, 0.3104, 0.3273,\n",
       "           0.3104, 0.3472, 0.3472, 0.3472, 0.3822, 0.3472, 0.3472, 0.3822, 0.3422,\n",
       "           0.3822, 0.3320, 0.3775, 0.3280, 0.3672, 0.3508, 0.3103, 0.3265, 0.3036,\n",
       "           0.2970, 0.3053, 0.2938, 0.3040, 0.2912, 0.2958, 0.2864, 0.2824, 0.2764,\n",
       "           0.2747, 0.2725, 0.2711, 0.2698, 0.2707, 0.2691, 0.2686, 0.2639, 0.2656,\n",
       "           0.2663, 0.2630, 0.2670, 0.2628, 0.2658, 0.2692, 0.2642, 0.2661, 0.2684,\n",
       "           0.2775, 0.2869, 0.2849, 0.2849, 0.2853, 0.2869, 0.2849, 0.2849, 0.2919,\n",
       "           0.2849, 0.3020, 0.3043, 0.2849, 0.3108, 0.2849, 0.3125, 0.2849, 0.3207,\n",
       "           0.2849, 0.3229, 0.2849, 0.3229, 0.3229, 0.3124, 0.2849, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.6576, 0.6189, 0.5936, 0.3469, 0.3469, 0.5412,\n",
       "           0.3469, 0.3469, 0.3469, 0.4520, 0.4388, 0.3434, 0.4219, 0.3313, 0.3866,\n",
       "           0.3244, 0.3141, 0.3109, 0.3080, 0.3051, 0.3263, 0.3153, 0.2955, 0.2940,\n",
       "           0.3014, 0.2983, 0.2901, 0.2870, 0.2879, 0.2810, 0.2771, 0.2740, 0.2726,\n",
       "           0.2743, 0.2690, 0.2725, 0.2678, 0.2671, 0.2746, 0.2683, 0.2779, 0.2691,\n",
       "           0.2702, 0.2804, 0.2854, 0.2824, 0.2965, 0.2942, 0.2804, 0.3037, 0.2857,\n",
       "           0.3085, 0.3089, 0.2902, 0.2918, 0.2932, 0.3089, 0.3089, 0.2964, 0.3089,\n",
       "           0.3042, 0.3057, 0.3089, 0.3089, 0.3116, 0.3116, 0.3089, 0.3116, 0.3089,\n",
       "           0.3116, 0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3116, 0.3116,\n",
       "           0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116,\n",
       "           0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3439, 0.3439, 0.4487,\n",
       "           0.4355, 0.3331, 0.4234, 0.4020, 0.3242, 0.3154, 0.3551, 0.3130, 0.3100,\n",
       "           0.3391, 0.3078, 0.3058, 0.3209, 0.3031, 0.3157, 0.2985, 0.3037, 0.2965,\n",
       "           0.2930, 0.2948, 0.2915, 0.2874, 0.2867, 0.2854, 0.2849, 0.2848, 0.2818,\n",
       "           0.2839, 0.2803, 0.2834, 0.2811, 0.2824, 0.2822, 0.2819, 0.2779, 0.2814,\n",
       "           0.2811, 0.2763, 0.2819, 0.2824, 0.2767, 0.2829, 0.2817, 0.2824, 0.2765,\n",
       "           0.2772, 0.2783, 0.2858, 0.2779, 0.2890, 0.2871, 0.2882, 0.2902, 0.2938,\n",
       "           0.2975, 0.3004, 0.2849, 0.2853, 0.2980, 0.3009, 0.2999, 0.5636, 0.3514,\n",
       "           0.3463, 0.5072, 0.3388, 0.4173, 0.3351, 0.3262, 0.3204, 0.3153, 0.3135,\n",
       "           0.3134, 0.3101, 0.3126, 0.3115, 0.3105, 0.3050, 0.3026, 0.3019, 0.3061,\n",
       "           0.3052, 0.2983, 0.3050, 0.2977, 0.3053, 0.3043, 0.2967, 0.2968, 0.3043,\n",
       "           0.3043, 0.3040, 0.2955, 0.2960, 0.3071, 0.3091, 0.3101, 0.2967, 0.3110,\n",
       "           0.2989, 0.3118, 0.3167, 0.3020, 0.3136, 0.3194, 0.3105, 0.3119, 0.3129,\n",
       "           0.3137, 0.3182, 0.3192, 0.3202, 0.3194, 0.3194, 0.3232, 0.3194, 0.3194,\n",
       "           0.3284, 0.3320, 0.3194]),\n",
       "   tensor([0.4101, 0.4484, 0.4484,  ..., 0.3084, 0.3084, 0.3084]),\n",
       "   tensor([0.3682, 0.3347, 0.3347, 0.3682, 0.3682, 0.3682, 0.3347, 0.3347, 0.3682,\n",
       "           0.3682, 0.3682, 0.3682, 0.3347, 0.3682, 0.3347, 0.3347, 0.3347, 0.3347,\n",
       "           0.3682, 0.3347, 0.3682, 0.3682, 0.3682, 0.3347, 0.3682, 0.3682, 0.3682,\n",
       "           0.3347, 0.3682, 0.3347, 0.3347, 0.3347, 0.3567, 0.3347, 0.3327, 0.2965,\n",
       "           0.2824, 0.2700, 0.2548, 0.2666, 0.2585, 0.2842, 0.2842, 0.3289, 0.2842,\n",
       "           0.2842, 0.3506, 0.3506, 0.2507, 0.3506, 0.2507, 0.3506, 0.2507, 0.3506,\n",
       "           0.3506, 0.2507, 0.3506, 0.2507, 0.3506, 0.2507, 0.2507, 0.3506, 0.3506,\n",
       "           0.2507, 0.2507, 0.3506, 0.2507, 0.2507, 0.3496, 0.2507, 0.2507, 0.2904,\n",
       "           0.2517, 0.2561, 0.2562, 0.2342, 0.2294, 0.2164, 0.2091, 0.2190, 0.3352,\n",
       "           0.4354, 0.3190, 0.4266, 0.3190, 0.3190, 0.3534, 0.3190, 0.3091, 0.2880,\n",
       "           0.2944, 0.2443, 0.2243, 0.2071, 0.2117, 0.2099, 0.2826, 0.2755, 0.2582,\n",
       "           0.2145, 0.2090, 0.2061, 0.2037, 0.2176, 0.2022, 0.2970, 0.3306, 0.3306,\n",
       "           0.3306, 0.2970, 0.3306, 0.2970, 0.3306, 0.3306, 0.2970, 0.2970, 0.3306,\n",
       "           0.2970, 0.3306, 0.3306, 0.2970, 0.2970, 0.2970, 0.2970, 0.3306, 0.2970,\n",
       "           0.3306, 0.2970, 0.3306, 0.3306, 0.3306, 0.2970, 0.3306, 0.2970, 0.3306,\n",
       "           0.2970, 0.2970, 0.3306, 0.2970, 0.2970, 0.3306, 0.3306, 0.2970, 0.3306,\n",
       "           0.3306, 0.3306, 0.3306, 0.3306, 0.2970, 0.3306, 0.3306, 0.2970, 0.3306,\n",
       "           0.2970, 0.2970, 0.2970, 0.3306, 0.3306, 0.2970, 0.3306, 0.2970, 0.3306,\n",
       "           0.2891, 0.2746, 0.2631, 0.2611, 0.2497, 0.2467, 0.2365, 0.2337, 0.2293,\n",
       "           0.2227, 0.2196, 0.2141, 0.2058, 0.2041, 0.2020, 0.2034, 0.1964, 0.1938,\n",
       "           0.1956, 0.2039, 0.2089, 0.2125, 0.2141, 0.2055, 0.2199, 0.2352, 0.2055,\n",
       "           0.2055, 0.2359, 0.2359, 0.2055, 0.2359, 0.2359, 0.2055, 0.2055, 0.3246,\n",
       "           0.3246, 0.5744, 0.3246, 0.3246, 0.5568, 0.5395, 0.5309, 0.3246, 0.3246,\n",
       "           0.3246, 0.4978, 0.3246, 0.4812, 0.3246, 0.3246, 0.4898, 0.3246, 0.3246,\n",
       "           0.4176, 0.3246, 0.3175, 0.3175, 0.3246, 0.3175, 0.3246, 0.3246, 0.3175,\n",
       "           0.3215, 0.3175, 0.3246, 0.3175, 0.3246, 0.3175, 0.3240, 0.3246, 0.3170,\n",
       "           0.3122, 0.3094, 0.3018, 0.3002, 0.3004, 0.2959, 0.2921, 0.2943, 0.2908,\n",
       "           0.2839, 0.2849, 0.2721, 0.2683, 0.2667, 0.2629, 0.2564, 0.2549, 0.2592,\n",
       "           0.2558, 0.2515, 0.2486, 0.2514, 0.2470, 0.2476, 0.2479, 0.2430, 0.2400,\n",
       "           0.2417, 0.2399, 0.2355, 0.2346, 0.2344, 0.2315, 0.2366, 0.2316, 0.2308,\n",
       "           0.2357, 0.2254, 0.2370, 0.2333, 0.2379, 0.2391, 0.2335, 0.2386, 0.2335,\n",
       "           0.2348, 0.2314, 0.2331, 0.2449, 0.2191, 0.2191, 0.2191, 0.2191, 0.2191,\n",
       "           0.2519, 0.2191, 0.2519, 0.2519, 0.2519, 0.2519, 0.2191, 0.2519, 0.2191,\n",
       "           0.2952, 0.2919, 0.2919, 0.2919, 0.2919, 0.2952, 0.2919, 0.2919, 0.2919,\n",
       "           0.2952, 0.2919, 0.2919, 0.2919, 0.2952, 0.2952, 0.2919, 0.2919, 0.2919,\n",
       "           0.2952, 0.2919, 0.2919, 0.2952, 0.2919, 0.2919, 0.2919, 0.2952, 0.2952,\n",
       "           0.2919, 0.2919, 0.2919, 0.2919, 0.2919, 0.2952, 0.2919, 0.2919, 0.2952,\n",
       "           0.2952, 0.2919, 0.2919, 0.2950, 0.2919, 0.2887, 0.2853, 0.2921, 0.2816,\n",
       "           0.2791, 0.2773, 0.2767, 0.2738, 0.2676, 0.2649, 0.2632, 0.2624, 0.2642,\n",
       "           0.2611, 0.2557, 0.2495, 0.2513, 0.2412, 0.2415, 0.2382, 0.2405, 0.2336,\n",
       "           0.2323, 0.2297, 0.2274, 0.2258, 0.2250, 0.2253, 0.2228, 0.2180, 0.2165,\n",
       "           0.2205, 0.2155, 0.2146, 0.2190, 0.2194, 0.2225, 0.2246, 0.2279, 0.2139,\n",
       "           0.2198, 0.2137, 0.2137, 0.2330, 0.2137, 0.2403, 0.3009, 0.3009, 0.2944,\n",
       "           0.3009, 0.3009, 0.2944, 0.2944, 0.2944, 0.2944, 0.3009, 0.3009, 0.3009,\n",
       "           0.2944, 0.2944, 0.3009, 0.2944, 0.3009, 0.2944, 0.2944, 0.3009, 0.2944,\n",
       "           0.2980, 0.2989, 0.2955, 0.3009, 0.2947, 0.2932, 0.2955, 0.2756, 0.2723,\n",
       "           0.2693, 0.2765, 0.2650, 0.2723, 0.2702, 0.2626, 0.2631, 0.2620, 0.2552,\n",
       "           0.2574, 0.2528, 0.2570, 0.2537, 0.2523, 0.2479, 0.2469, 0.2480, 0.2426,\n",
       "           0.2412, 0.2426, 0.2399, 0.2363, 0.2391, 0.2347, 0.2381, 0.2371, 0.2367,\n",
       "           0.2294, 0.2307, 0.2281, 0.2257, 0.2237, 0.2247, 0.2236, 0.2226, 0.2228,\n",
       "           0.2199, 0.2224, 0.2212, 0.2220, 0.2183, 0.2229, 0.2236, 0.2272, 0.2280,\n",
       "           0.2267, 0.2949, 0.2949, 0.2642, 0.2949, 0.2642, 0.2949, 0.2642, 0.2949,\n",
       "           0.2642, 0.2642, 0.2642, 0.2949, 0.2642, 0.2949, 0.2949, 0.2642, 0.2949,\n",
       "           0.2949, 0.2642, 0.2949, 0.2642, 0.2642, 0.2949, 0.2642, 0.2949, 0.2642,\n",
       "           0.2642, 0.2642, 0.2949, 0.2949, 0.2949, 0.2642, 0.2949, 0.2949, 0.2642,\n",
       "           0.2642, 0.2642, 0.2949, 0.2949, 0.2949, 0.2642, 0.2642, 0.2642, 0.2642,\n",
       "           0.2949, 0.2910, 0.2642, 0.2884, 0.2789, 0.2745, 0.2731, 0.2642, 0.2646,\n",
       "           0.2607, 0.2567, 0.2591, 0.2524, 0.2502, 0.2548, 0.2513, 0.2460, 0.2483,\n",
       "           0.2453, 0.2438, 0.2491, 0.2421, 0.2352, 0.2384, 0.2335, 0.2324, 0.2333,\n",
       "           0.2283, 0.2297, 0.2277, 0.2287, 0.2265, 0.2251, 0.2256, 0.2258, 0.2221,\n",
       "           0.2207, 0.2181, 0.2214, 0.2181, 0.2211, 0.2174, 0.2176, 0.2166, 0.2166,\n",
       "           0.2146, 0.2152, 0.2151, 0.2179, 0.2163, 0.2217, 0.2193, 0.2220, 0.2198,\n",
       "           0.2960, 0.3628, 0.2960, 0.2960, 0.2957, 0.3032, 0.3190, 0.3177, 0.2983,\n",
       "           0.3045, 0.3007, 0.2977, 0.2939, 0.2892, 0.2861, 0.2844, 0.2781, 0.2780,\n",
       "           0.2698, 0.2695, 0.2666, 0.2616, 0.2627, 0.2605, 0.2610, 0.2582, 0.2532,\n",
       "           0.2528, 0.2501, 0.2503, 0.2478, 0.2450, 0.2441, 0.2433, 0.2424, 0.2416,\n",
       "           0.2404, 0.2398, 0.2380, 0.2389, 0.2390, 0.2380, 0.2372, 0.2371, 0.2370,\n",
       "           0.2365, 0.2363, 0.2359, 0.2352, 0.2347, 0.2337, 0.2336, 0.2334, 0.2327,\n",
       "           0.2333, 0.2319, 0.2316, 0.2318, 0.2314, 0.2318, 0.2308, 0.2310, 0.2302])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([ 0.0999,  0.4080,  0.2257,  ...,  0.3127, -0.3477, -0.0928],\n",
       "          requires_grad=True),\n",
       "   tensor([-0.0927,  0.0855,  0.0957,  ...,  0.6935,  0.6596,  0.7370],\n",
       "          requires_grad=True),\n",
       "   tensor([-0.2120,  0.2443,  0.1009,  ..., -0.2466, -0.5774, -0.6174],\n",
       "          requires_grad=True)],\n",
       "  'Time to Maturity': [tensor([0.0714, 0.0476, 0.0476,  ..., 2.3254, 1.2421, 1.2421],\n",
       "          requires_grad=True),\n",
       "   tensor([1.4087, 1.0476, 0.0754,  ..., 0.5754, 1.0476, 1.4087],\n",
       "          requires_grad=True),\n",
       "   tensor([0.8532, 0.8532, 0.8532,  ..., 2.4087, 2.4087, 2.4087],\n",
       "          requires_grad=True)],\n",
       "  'Implied Volatility': [tensor([0.2806, 0.3295, 0.2561,  ..., 0.2955, 0.3111, 0.2893]),\n",
       "   tensor([0.2795, 0.2733, 0.3101,  ..., 0.3701, 0.2938, 0.2950]),\n",
       "   tensor([0.2561, 0.2259, 0.2187,  ..., 0.2626, 0.2929, 0.2982])]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "class IVSurfaceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data, \n",
    "        mask_proportions, \n",
    "        random_state=0,\n",
    "        n_query_points=None\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.mask_proportions = mask_proportions\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        self.n_query_points = n_query_points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        surface_data = self.data[idx]\n",
    "        \n",
    "        # Extract the surface coordinates and volatilities\n",
    "        points_coordinates = np.stack([\n",
    "            surface_data['Surface']['Log Moneyness'], \n",
    "            surface_data['Surface']['Time to Maturity']\n",
    "        ], axis=1)\n",
    "        points_volatilities = surface_data['Surface']['Implied Volatility']\n",
    "\n",
    "        # Select a random mask proportion\n",
    "        proportion = self.rng.choice(self.mask_proportions)\n",
    "\n",
    "        # Perform clustering\n",
    "        n_clusters = int(np.ceil(1 / proportion))\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('kmeans', KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init='auto'))\n",
    "        ])\n",
    "        labels = pipeline.fit_predict(points_coordinates)\n",
    "        masked_indices = np.array([], dtype=int)\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_indices = np.where(labels == cluster)[0]\n",
    "            num_to_mask = int(np.ceil(len(cluster_indices) * proportion))\n",
    "            masked_indices = np.append(masked_indices, [self.rng.choice(cluster_indices, size=num_to_mask, replace=False)])\n",
    "        \n",
    "        unmasked_indices = np.setdiff1d(range(len(labels)), masked_indices)\n",
    "\n",
    "        # Calculate IV mean and std for unmasked points\n",
    "        iv_mean = np.mean(points_volatilities[unmasked_indices])\n",
    "        iv_std = np.std(points_volatilities[unmasked_indices])\n",
    "\n",
    "        # Define query indices based on n_query_points\n",
    "        if self.n_query_points is None:\n",
    "            query_indices = masked_indices\n",
    "        else:\n",
    "            query_indices = self.rng.choice(masked_indices, size=self.n_query_points, replace=False)\n",
    "            \n",
    "        data_item = {\n",
    "            'Datetime': surface_data['Datetime'],\n",
    "            'Symbol': surface_data['Symbol'],\n",
    "            'Mask Proportion': proportion,\n",
    "            'Market Features': {\n",
    "                'Market Return': torch.tensor(surface_data['Market Features']['Market Return'], dtype=torch.float32),\n",
    "                'Market Volatility': torch.tensor(surface_data['Market Features']['Market Volatility'], dtype=torch.float32),\n",
    "                'Treasury Rate': torch.tensor(surface_data['Market Features']['Treasury Rate'], dtype=torch.float32),\n",
    "                'IV Mean': torch.tensor(iv_mean, dtype=torch.float32),\n",
    "                'IV Std.': torch.tensor(iv_std, dtype=torch.float32),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[unmasked_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[unmasked_indices, 1], dtype=torch.float32),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[unmasked_indices], dtype=torch.float32)\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[query_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[query_indices, 1], dtype=torch.float32),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[query_indices], dtype=torch.float32)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return data_item\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batched_data = {\n",
    "            'Datetime': [item['Datetime'] for item in batch],\n",
    "            'Symbol': [item['Symbol'] for item in batch],\n",
    "            'Mask Proportion': [item['Mask Proportion'] for item in batch],\n",
    "            'Market Features': {\n",
    "                'Market Return': default_collate([item['Market Features']['Market Return'] for item in batch]),\n",
    "                'Market Volatility': default_collate([item['Market Features']['Market Volatility'] for item in batch]),\n",
    "                'Treasury Rate': default_collate([item['Market Features']['Treasury Rate'] for item in batch]),\n",
    "                'IV Mean': default_collate([item['Market Features']['IV Mean'] for item in batch]),\n",
    "                'IV Std.': default_collate([item['Market Features']['IV Std.'] for item in batch]),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': [item['Input Surface']['Log Moneyness'].clone().detach() for item in batch],\n",
    "                'Time to Maturity': [item['Input Surface']['Time to Maturity'].clone().detach() for item in batch],\n",
    "                'Implied Volatility': [item['Input Surface']['Implied Volatility'].clone().detach() for item in batch],\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': [item['Query Points']['Log Moneyness'].clone().detach().requires_grad_(True) for item in batch],\n",
    "                'Time to Maturity': [item['Query Points']['Time to Maturity'].clone().detach().requires_grad_(True) for item in batch],\n",
    "                'Implied Volatility': [item['Query Points']['Implied Volatility'].clone().detach() for item in batch],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "# Assuming surfaces is the output from the implied_volatility_surfaces function\n",
    "mask_proportions = HYPERPARAMETERS['Input Preprocessing']['Mask Proportions']  \n",
    "n_query_points = HYPERPARAMETERS['Input Preprocessing']['Number of Query Points']  \n",
    "dataset = IVSurfaceDataset(surfaces, mask_proportions, RANDOM_STATE, n_query_points)\n",
    "data_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=HYPERPARAMETERS['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Fetch one batch from the DataLoader\n",
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL'],\n",
       " 'Mask Proportion': [0.7, 0.5, 0.7],\n",
       " 'Market Features': {'Market Return': tensor([ 0.1927, -0.2735,  0.0808], grad_fn=<SqueezeBackward1>),\n",
       "  'Market Volatility': tensor([ 1.3806, -0.4248, -0.9558], grad_fn=<SqueezeBackward1>),\n",
       "  'Treasury Rate': tensor([-0.4443,  1.3328, -0.8885], grad_fn=<SqueezeBackward1>),\n",
       "  'IV Mean': tensor([ 0.2376,  1.0783, -1.3158], grad_fn=<SqueezeBackward1>),\n",
       "  'IV Std.': tensor([-0.3337,  1.2685, -0.9348], grad_fn=<SqueezeBackward1>)},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-0.8491, -0.7758, -0.5352, -0.4706, -0.4389, -0.3768, -0.3463, -0.3463,\n",
       "           -0.3162, -0.2864, -0.2570, -0.1992, -0.1428, -0.1150, -0.0336, -0.0071,\n",
       "            0.0452,  0.0963,  0.1215,  0.1464,  0.1711,  0.1711,  0.1956,  0.2674,\n",
       "            0.3142,  0.3142,  0.3373,  0.3601, -2.0787, -2.0787, -1.9589, -1.8447,\n",
       "           -1.8447, -1.7356, -1.6827, -1.6311, -1.4822, -1.4345, -1.3877, -1.3419,\n",
       "           -1.2091, -1.1664, -1.1244, -1.0832, -1.0426, -0.9247, -0.8866, -0.8866,\n",
       "           -0.7758, -0.7047, -0.6355, -0.5682, -0.5682, -0.5352, -0.4389, -0.4077,\n",
       "           -0.3768, -0.3768, -0.3463, -0.2864, -0.2570, -0.2279, -0.1708, -0.0604,\n",
       "           -0.0336, -0.0071,  0.0192,  0.0709,  0.1464,  0.1711,  0.2197,  0.2437,\n",
       "            0.2674,  0.3142,  0.3373,  0.3827,  0.4274,  0.5143,  0.5356,  0.5566,\n",
       "            0.6594,  0.7580,  0.7964,  0.7964,  0.8153,  0.8153,  0.8341,  0.8897,\n",
       "            0.8897,  0.9261,  1.0148,  1.0148,  1.1003,  1.1171,  1.1337,  1.1503,\n",
       "            1.1503,  1.1667,  1.1992,  1.2472,  1.2630,  1.3098,  1.3252,  1.3252,\n",
       "            1.4303,  1.4595,  1.4740,  1.4884,  1.5170,  1.5311,  1.5452,  1.5592,\n",
       "            1.5592,  1.5869,  1.6007,  1.6144,  1.6280,  1.6550,  1.6683,  1.6816,\n",
       "            1.6949,  1.7211,  1.7471,  1.7600,  1.7856, -0.9247, -0.7758, -0.7400,\n",
       "           -0.7047, -0.5682, -0.5027, -0.4706, -0.4389, -0.3768, -0.3463, -0.3162,\n",
       "           -0.2864, -0.2279, -0.1992, -0.1708, -0.1428, -0.1150, -0.0876, -0.0071,\n",
       "            0.0192,  0.1215,  0.2674,  0.2909,  0.3373,  0.3601,  0.4051,  0.4051,\n",
       "            0.4274,  0.4494,  0.4712,  0.5143, -0.6355, -0.5682, -0.5352, -0.4389,\n",
       "           -0.3768, -0.3768, -0.3463, -0.1708, -0.1708, -0.1428, -0.1150, -0.0876,\n",
       "            0.0452,  0.0709,  0.1215,  0.1464,  0.1711,  0.1956,  0.1956,  0.2197,\n",
       "            0.3142,  0.3601,  0.3827,  0.4051,  0.4051,  0.4274,  0.4712,  0.4928,\n",
       "           -1.6827, -1.6311, -1.5804, -1.5308, -1.3877, -1.2526, -1.2091, -1.2091,\n",
       "           -1.1664, -1.0026, -1.0026, -0.9247, -0.8491, -0.8122, -0.8122, -0.7758,\n",
       "           -0.7400, -0.7047, -0.6355, -0.6016, -0.5682, -0.5682, -0.4706, -0.4389,\n",
       "           -0.4389, -0.4077, -0.3768, -0.2864, -0.2864, -0.1428, -0.1150,  0.1711,\n",
       "            0.1956,  0.3373,  0.4274,  0.5143,  0.5982,  0.6188,  0.6993,  0.7580,\n",
       "            0.7964,  0.8153,  0.8153,  0.8713,  0.9080,  0.9620,  0.9620,  1.0321,\n",
       "            1.0494,  1.0665,  1.0665,  1.0835,  1.1003,  1.1171,  1.1337,  1.1667,\n",
       "            1.1830,  1.1992,  1.1992,  1.2630,  1.2787,  1.2943,  1.3405,  1.3405,\n",
       "            1.3557,  1.3708,  1.3858,  1.4156,  1.4156,  1.4450,  1.4595,  1.4884,\n",
       "            1.5027,  1.5170,  1.5170,  1.6007,  1.6280,  1.6415,  1.6683,  1.6816,\n",
       "            1.6949,  1.7080,  1.7211,  1.7211,  1.7342,  1.7471,  1.7600,  1.7983,\n",
       "            1.8360,  1.8484,  1.8608,  1.8975,  1.9217,  1.9217, -2.0787, -1.9589,\n",
       "           -1.9589, -1.9011, -1.8447, -1.7356, -1.6311, -1.5308, -1.4822, -1.3419,\n",
       "           -1.2968, -1.2526, -0.7400, -0.7047, -0.6355, -0.6016, -0.4706, -0.4389,\n",
       "           -0.4077, -0.3768, -0.3463, -0.3463, -0.3162, -0.2864, -0.1708, -0.1708,\n",
       "           -0.1428, -0.1150, -0.0876, -0.0604, -0.0071,  0.0963,  0.1711,  0.1956,\n",
       "            0.1956,  0.2437,  0.3142,  0.3373,  0.4494,  0.4928,  0.5356,  0.6188,\n",
       "            0.6392,  0.6594,  0.7386,  0.7580,  0.8153, -1.8447, -1.7895, -1.7356,\n",
       "           -1.6311, -1.5804, -1.5308, -1.2526, -1.2091, -1.0832, -1.0426, -0.9634,\n",
       "           -0.9247, -0.9247, -0.7758, -0.7400, -0.7047, -0.6355, -0.6016, -0.5682,\n",
       "           -0.3463, -0.3162, -0.2279, -0.1992, -0.1428, -0.1428, -0.1150, -0.0604,\n",
       "           -0.0336, -0.0071,  0.0192,  0.0452,  0.0709,  0.0709,  0.0963,  0.1215,\n",
       "            0.2197,  0.3142,  0.3373,  0.3601,  0.3827,  0.4274,  0.4494,  0.5775,\n",
       "            0.5775,  0.5982,  0.6188,  0.6392,  0.7580,  0.7773,  0.8153,  0.8713,\n",
       "            0.8713,  0.8897,  0.8897, -1.6311, -1.4345, -1.2968, -1.2526, -1.2526,\n",
       "           -1.1244, -1.0832, -1.0426, -1.0026, -0.9634, -0.9247, -0.9247, -0.8866,\n",
       "           -0.8122, -0.7758, -0.7047, -0.7047, -0.6355, -0.6016, -0.6016, -0.5682,\n",
       "           -0.5682, -0.5027, -0.5027, -0.4389, -0.3162, -0.2864, -0.2570, -0.2279,\n",
       "           -0.1992, -0.1992, -0.1708, -0.1428, -0.0071,  0.0709,  0.1215,  0.1711,\n",
       "            0.1956,  0.2197,  0.2437,  0.2674,  0.2909,  0.3373,  0.4051,  0.4274,\n",
       "            0.4928,  0.5982,  0.6188,  0.6594,  0.6794,  0.6794,  0.6993,  0.7190,\n",
       "            0.7386,  0.7773,  0.7964,  0.8153,  0.8528,  0.8528,  0.8713,  0.9080,\n",
       "            0.9261,  0.9441,  0.9620,  0.9620,  0.9973,  1.0148,  1.0321,  1.0494,\n",
       "            1.0835,  1.1003,  1.1003,  1.1337,  1.1992,  1.2153,  1.2313,  1.2313,\n",
       "            1.2630,  1.2630,  1.2787,  1.2943,  1.3405,  1.3557,  1.3708,  1.4007,\n",
       "            1.4156,  1.4156,  1.4450,  1.5311,  1.5311,  1.5452,  1.5592, -2.0181,\n",
       "           -1.9011, -1.7895, -1.7356, -1.6827, -1.6311, -1.5308, -1.4822, -1.3877,\n",
       "           -1.3419, -1.2968, -1.2968, -1.2091, -1.1664, -1.0832, -1.0832, -0.9634,\n",
       "           -0.9247, -0.8866, -0.8491, -0.8122, -0.7400, -0.7047, -0.6698, -0.6355,\n",
       "           -0.6016, -0.6016, -0.5352, -0.4706, -0.3463, -0.2570, -0.1992, -0.1428,\n",
       "           -0.0876,  0.0963,  0.1464,  0.1711,  0.2674,  0.3142,  0.3373,  0.4051,\n",
       "            0.4274,  0.4928,  0.5143,  0.5356,  0.5566,  0.6794,  0.6993,  0.7580,\n",
       "            0.7773,  0.8153,  0.8341,  0.8897,  0.9080,  0.9261,  0.9441,  0.9441,\n",
       "            0.9620,  0.9797,  0.9797,  1.0494,  1.0835,  1.1003,  1.1337,  1.1667,\n",
       "            1.3557,  1.3708,  1.3858,  1.4007,  1.4156,  1.4303,  1.4303,  1.5027,\n",
       "            1.5170,  1.5311,  1.5592,  1.6280,  1.6550,  1.6683,  1.6683,  1.6949,\n",
       "            1.6949,  1.7080,  1.7211,  1.7342,  1.7856,  1.8109,  1.8235,  1.8235,\n",
       "            1.8975,  1.9217,  1.9456,  1.9694, -2.0181, -1.6827, -1.4345, -1.3877,\n",
       "           -1.3877, -1.3419, -1.2526, -1.2526, -1.0832, -1.0426, -1.0426, -0.9634,\n",
       "           -0.9247, -0.9247, -0.8866, -0.8122, -0.8122, -0.7758, -0.7047, -0.6355,\n",
       "           -0.6355, -0.5352, -0.4706, -0.4389, -0.3162, -0.2864, -0.2570, -0.1992,\n",
       "           -0.1708, -0.1428, -0.1150, -0.0876, -0.0876, -0.0604, -0.0071,  0.0192,\n",
       "            0.0452,  0.0963,  0.1956,  0.2437,  0.2909,  0.3373,  0.3601,  0.3827,\n",
       "            0.4051,  0.4274,  0.4494,  0.4712,  0.4928,  0.5356,  0.5356,  0.5775,\n",
       "            0.5775,  0.5982,  0.6392,  0.6594,  0.7386,  0.7580,  0.7964,  0.8153,\n",
       "            0.8341,  0.8341,  0.8528,  0.8897, -2.1409, -2.1409, -1.9589, -1.9011,\n",
       "           -1.6311, -1.5308, -1.5308, -1.2526, -1.0026, -0.7047, -0.6355, -0.6016,\n",
       "           -0.5352, -0.5352, -0.5027, -0.4077, -0.3463, -0.2570, -0.1992, -0.0876,\n",
       "           -0.0604,  0.0192,  0.0452,  0.0963,  0.1215,  0.1956,  0.2197,  0.2437,\n",
       "            0.2437,  0.3142,  0.3373,  0.4051,  0.4928,  0.7773,  0.8153,  0.8528,\n",
       "            0.8897,  0.9261,  0.9620,  0.9973,  1.1337,  1.1667,  1.1667,  1.4156,\n",
       "            1.5027,  1.5592,  1.5869,  1.6144,  1.7211,  1.7471,  1.7728,  1.7728,\n",
       "            1.7983,  1.8235,  1.8731,  1.8975,  1.9217,  1.9929,  1.9929],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.7651, -0.7651, -0.7303,  ...,  1.8852,  1.9089,  1.9324],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-1.0675e+00, -1.0285e+00, -1.0092e+00, -1.0092e+00, -9.9012e-01,\n",
       "           -8.9675e-01, -8.6037e-01, -8.0680e-01, -8.0680e-01, -7.7172e-01,\n",
       "           -7.5437e-01, -7.3714e-01, -7.0303e-01, -7.0303e-01, -6.8616e-01,\n",
       "           -6.5275e-01, -6.1979e-01, -6.0348e-01, -6.0348e-01, -5.8727e-01,\n",
       "           -5.5517e-01, -5.3928e-01, -5.2349e-01, -4.9221e-01, -4.7672e-01,\n",
       "           -4.4603e-01, -4.3082e-01, -4.0069e-01, -3.5618e-01, -3.4153e-01,\n",
       "           -3.2695e-01, -2.9807e-01, -2.9807e-01, -2.8375e-01, -2.4129e-01,\n",
       "           -2.2730e-01, -1.8580e-01, -1.5852e-01, -1.3154e-01, -1.0485e-01,\n",
       "           -7.8444e-02, -1.3655e-02, -8.9943e-04,  1.1790e-02,  1.1790e-02,\n",
       "            6.1902e-02, -1.0479e+00, -1.0285e+00, -9.7116e-01, -9.5234e-01,\n",
       "           -9.1514e-01, -9.1514e-01, -8.9675e-01, -8.7849e-01, -7.8919e-01,\n",
       "           -7.7172e-01, -7.5437e-01, -7.0303e-01, -6.1979e-01, -6.0348e-01,\n",
       "           -5.5517e-01, -4.9221e-01, -4.7672e-01, -4.6132e-01, -4.4603e-01,\n",
       "           -4.4603e-01, -4.3082e-01, -4.1571e-01, -4.1571e-01, -4.0069e-01,\n",
       "           -3.4153e-01, -3.2695e-01, -2.8375e-01, -2.6951e-01, -2.5536e-01,\n",
       "           -2.2730e-01, -2.1339e-01, -1.1815e-01, -1.0485e-01, -7.8444e-02,\n",
       "           -7.5437e-01, -6.8616e-01, -6.5275e-01, -6.5275e-01, -5.8727e-01,\n",
       "           -5.2349e-01, -5.2349e-01, -4.9221e-01, -4.3082e-01, -4.0069e-01,\n",
       "           -4.0069e-01, -2.5536e-01, -2.2730e-01, -1.4499e-01, -1.3154e-01,\n",
       "           -7.8444e-02, -4.0069e-01, -3.7093e-01, -3.4153e-01, -1.7212e-01,\n",
       "           -1.7212e-01, -1.1815e-01, -6.5349e-02,  1.1790e-02,  6.1902e-02,\n",
       "           -2.6356e+00, -2.5987e+00, -2.5623e+00, -2.4220e+00, -2.3881e+00,\n",
       "           -2.3881e+00, -2.3217e+00, -2.3217e+00, -2.2892e+00, -2.2254e+00,\n",
       "           -2.1942e+00, -2.1328e+00, -2.0435e+00, -2.0435e+00, -2.0144e+00,\n",
       "           -1.9857e+00, -1.9292e+00, -1.9015e+00, -1.8741e+00, -1.8741e+00,\n",
       "           -1.7936e+00, -1.7673e+00, -1.7413e+00, -1.7413e+00, -1.7156e+00,\n",
       "           -1.6902e+00, -1.6650e+00, -1.6400e+00, -1.5909e+00, -1.5667e+00,\n",
       "           -1.4956e+00, -1.4038e+00, -1.4038e+00, -1.3813e+00, -1.3371e+00,\n",
       "           -1.3371e+00, -1.2509e+00, -1.2090e+00, -1.1677e+00, -1.0675e+00,\n",
       "           -1.0092e+00, -9.5234e-01, -9.1514e-01, -8.4238e-01, -8.4238e-01,\n",
       "           -8.2452e-01, -8.0680e-01, -7.7172e-01, -7.5437e-01, -7.3714e-01,\n",
       "           -6.8616e-01, -6.8616e-01, -6.5275e-01, -6.3621e-01, -6.3621e-01,\n",
       "           -6.1979e-01, -6.0348e-01, -4.7672e-01, -4.1571e-01, -3.8577e-01,\n",
       "           -3.5618e-01, -3.4153e-01, -3.1247e-01, -2.9807e-01, -2.8375e-01,\n",
       "           -2.6951e-01, -2.2730e-01, -2.1339e-01, -2.1339e-01, -1.7212e-01,\n",
       "           -1.5852e-01, -1.4499e-01, -1.0485e-01, -7.8444e-02, -2.6477e-02,\n",
       "           -1.3655e-02, -8.9943e-04,  4.9469e-02,  6.1902e-02,  7.4271e-02,\n",
       "            8.6579e-02,  1.1101e-01,  1.8288e-01,  2.2965e-01,  2.7555e-01,\n",
       "            3.8667e-01,  4.0830e-01,  4.2975e-01,  4.5101e-01,  4.7208e-01,\n",
       "            4.7208e-01,  5.3425e-01, -1.6400e+00, -1.5909e+00, -1.5909e+00,\n",
       "           -1.5667e+00, -1.5428e+00, -1.5428e+00, -1.4956e+00, -1.4723e+00,\n",
       "           -1.4264e+00, -1.4038e+00, -1.3591e+00, -1.3371e+00, -1.3153e+00,\n",
       "           -1.2936e+00, -1.2722e+00, -1.2509e+00, -1.2509e+00, -1.2299e+00,\n",
       "           -1.1271e+00, -1.1271e+00, -1.1071e+00, -1.0479e+00, -9.9012e-01,\n",
       "           -9.7116e-01, -9.7116e-01, -9.5234e-01, -9.1514e-01, -8.7849e-01,\n",
       "           -8.6037e-01, -8.6037e-01, -8.4238e-01, -8.0680e-01, -7.8919e-01,\n",
       "           -7.3714e-01, -7.2002e-01, -7.0303e-01, -6.5275e-01, -6.3621e-01,\n",
       "           -6.1979e-01, -6.0348e-01, -5.8727e-01, -5.5517e-01, -5.3928e-01,\n",
       "           -5.3928e-01, -5.2349e-01, -5.2349e-01, -4.9221e-01, -4.9221e-01,\n",
       "           -4.0069e-01, -3.8577e-01, -3.4153e-01, -3.2695e-01, -2.9807e-01,\n",
       "           -2.8375e-01, -2.6951e-01, -2.5536e-01, -2.5536e-01, -2.2730e-01,\n",
       "           -2.1339e-01, -2.1339e-01, -1.8580e-01, -1.7212e-01, -1.3154e-01,\n",
       "           -1.3154e-01, -1.0485e-01, -7.8444e-02, -6.5349e-02, -5.2323e-02,\n",
       "           -1.3655e-02,  1.1790e-02,  2.4414e-02,  2.4414e-02,  6.1902e-02,\n",
       "            7.4271e-02,  7.4271e-02,  8.6579e-02,  1.2314e-01,  1.4721e-01,\n",
       "            1.5916e-01,  1.5916e-01,  1.7105e-01,  1.7105e-01,  2.1804e-01,\n",
       "            2.2965e-01,  2.4121e-01,  2.7555e-01,  2.7555e-01,  2.9818e-01,\n",
       "            3.3174e-01,  3.6484e-01,  3.8667e-01,  3.9751e-01,  3.9751e-01,\n",
       "            4.0830e-01,  4.1905e-01,  4.2975e-01,  4.4040e-01,  4.4040e-01,\n",
       "            4.5101e-01,  4.5101e-01, -2.3217e+00, -2.1942e+00, -2.1633e+00,\n",
       "           -2.1328e+00, -2.1026e+00, -2.1026e+00, -2.0144e+00, -1.9857e+00,\n",
       "           -1.9292e+00, -1.8741e+00, -1.8469e+00, -1.8201e+00, -1.7936e+00,\n",
       "           -1.7673e+00, -1.7413e+00, -1.7156e+00, -1.6902e+00, -1.6650e+00,\n",
       "           -1.6650e+00, -1.6154e+00, -1.5191e+00, -1.5191e+00, -1.4956e+00,\n",
       "           -1.4492e+00, -1.4264e+00, -1.4264e+00, -1.3813e+00, -1.3153e+00,\n",
       "           -1.2299e+00, -1.2090e+00, -1.1677e+00, -1.1473e+00, -1.1473e+00,\n",
       "           -1.0675e+00, -1.0285e+00, -1.0285e+00, -9.9012e-01, -9.7116e-01,\n",
       "           -9.5234e-01, -9.3367e-01, -8.7849e-01, -8.7849e-01, -8.4238e-01,\n",
       "           -8.0680e-01, -7.8919e-01, -7.7172e-01, -7.5437e-01, -7.2002e-01,\n",
       "           -6.8616e-01, -6.6940e-01, -6.5275e-01, -6.3621e-01, -6.1979e-01,\n",
       "           -6.0348e-01, -5.8727e-01, -5.3928e-01, -5.0780e-01, -4.6132e-01,\n",
       "           -4.0069e-01, -3.8577e-01, -3.8577e-01, -3.7093e-01, -3.2695e-01,\n",
       "           -3.1247e-01, -2.6951e-01, -2.4129e-01, -2.1339e-01, -1.9956e-01,\n",
       "           -1.7212e-01, -1.0485e-01, -5.2323e-02, -3.9366e-02, -2.6477e-02,\n",
       "           -1.3655e-02,  6.1902e-02,  7.4271e-02,  1.3520e-01,  2.7555e-01,\n",
       "            3.4282e-01,  3.6484e-01,  3.6484e-01,  3.8667e-01,  4.0830e-01,\n",
       "            4.2975e-01,  4.7208e-01,  5.1370e-01,  5.3425e-01, -2.1026e+00,\n",
       "           -1.9292e+00, -1.9292e+00, -1.8469e+00, -1.7936e+00, -1.7936e+00,\n",
       "           -1.7156e+00, -1.6650e+00, -1.6154e+00, -1.5191e+00, -1.4956e+00,\n",
       "           -1.4723e+00, -1.4492e+00, -1.4038e+00, -1.3371e+00, -1.3371e+00,\n",
       "           -1.2936e+00, -1.2722e+00, -1.2299e+00, -1.1882e+00, -1.1882e+00,\n",
       "           -1.1271e+00, -1.1071e+00, -1.1071e+00, -1.0872e+00, -1.0872e+00,\n",
       "           -1.0675e+00, -1.0092e+00, -9.3367e-01, -9.1514e-01, -8.7849e-01,\n",
       "           -8.4238e-01, -8.0680e-01, -7.8919e-01, -7.7172e-01, -7.0303e-01,\n",
       "           -6.8616e-01, -6.6940e-01, -6.5275e-01, -6.1979e-01, -6.1979e-01,\n",
       "           -6.0348e-01, -5.7117e-01, -5.5517e-01, -5.5517e-01, -4.9221e-01,\n",
       "           -4.7672e-01, -4.7672e-01, -4.4603e-01, -4.0069e-01, -3.5618e-01,\n",
       "           -3.5618e-01, -3.4153e-01, -3.4153e-01, -3.2695e-01, -3.1247e-01,\n",
       "           -2.9807e-01, -2.4129e-01, -2.1339e-01, -1.8580e-01, -1.5852e-01,\n",
       "           -1.0485e-01, -7.8444e-02, -5.2323e-02, -1.3655e-02, -8.9943e-04,\n",
       "            2.4414e-02,  7.4271e-02,  9.8825e-02,  1.3520e-01,  1.5916e-01,\n",
       "            3.4282e-01,  3.6484e-01,  4.2975e-01,  4.5101e-01,  5.1370e-01,\n",
       "           -2.8291e+00, -2.7891e+00, -2.7498e+00, -2.6731e+00, -2.5987e+00,\n",
       "           -2.5623e+00, -2.5265e+00, -2.4911e+00, -2.4911e+00, -2.4563e+00,\n",
       "           -2.4220e+00, -2.3881e+00, -2.3881e+00, -2.3217e+00, -2.2892e+00,\n",
       "           -2.2571e+00, -2.2254e+00, -2.1633e+00, -2.1633e+00, -2.1328e+00,\n",
       "           -2.1328e+00, -2.1026e+00, -2.0435e+00, -1.9573e+00, -1.9015e+00,\n",
       "           -1.8469e+00, -1.7936e+00, -1.7673e+00, -1.7156e+00, -1.6902e+00,\n",
       "           -1.6400e+00, -1.6400e+00, -1.6154e+00, -1.5667e+00, -1.5428e+00,\n",
       "           -1.4956e+00, -1.4723e+00, -1.3813e+00, -1.3591e+00, -1.3153e+00,\n",
       "           -1.2936e+00, -1.2722e+00, -1.2090e+00, -1.1677e+00, -1.1271e+00,\n",
       "           -1.0675e+00, -1.0675e+00, -1.0285e+00, -9.3367e-01, -8.7849e-01,\n",
       "           -8.6037e-01, -8.6037e-01, -7.8919e-01, -7.8919e-01, -7.5437e-01,\n",
       "           -7.0303e-01, -6.6940e-01, -6.5275e-01, -6.3621e-01, -5.8727e-01,\n",
       "           -5.8727e-01, -5.7117e-01, -5.7117e-01, -5.5517e-01, -5.3928e-01,\n",
       "           -5.2349e-01, -4.1571e-01, -4.0069e-01, -3.8577e-01, -3.7093e-01,\n",
       "           -2.9807e-01, -2.8375e-01, -2.6951e-01, -2.6951e-01, -2.4129e-01,\n",
       "           -2.4129e-01, -2.1339e-01, -1.7212e-01, -1.5852e-01, -1.4499e-01,\n",
       "           -1.0485e-01, -3.9366e-02, -1.3655e-02, -1.3655e-02, -8.9943e-04,\n",
       "           -8.9943e-04,  6.1902e-02,  7.4271e-02,  8.6579e-02,  1.3520e-01,\n",
       "            1.8288e-01,  2.0638e-01,  2.9818e-01,  3.2060e-01,  4.2975e-01,\n",
       "            4.2975e-01,  4.7208e-01,  4.7208e-01, -2.7112e+00, -2.7112e+00,\n",
       "           -2.5623e+00, -2.4911e+00, -2.1633e+00, -2.0435e+00, -2.0435e+00,\n",
       "           -1.9857e+00, -1.7156e+00, -1.7156e+00, -1.6650e+00, -1.5667e+00,\n",
       "           -1.5191e+00, -1.4264e+00, -1.2936e+00, -1.2509e+00, -1.0872e+00,\n",
       "           -1.0479e+00, -9.3367e-01, -8.9675e-01, -8.2452e-01, -7.2002e-01,\n",
       "           -7.2002e-01, -6.8616e-01, -6.8616e-01, -5.8727e-01, -4.6132e-01,\n",
       "           -4.3082e-01, -3.7093e-01, -3.7093e-01, -3.1247e-01, -2.2730e-01,\n",
       "           -1.9956e-01, -1.1815e-01, -9.1610e-02, -6.5349e-02,  1.1790e-02,\n",
       "            1.1790e-02,  3.6974e-02,  3.6974e-02,  6.1902e-02,  1.1101e-01,\n",
       "            1.1101e-01,  1.3520e-01,  1.5916e-01,  1.8288e-01,  1.8288e-01,\n",
       "            2.2965e-01,  2.5271e-01,  2.7555e-01,  3.2060e-01,  3.6484e-01,\n",
       "            3.8667e-01,  3.8667e-01,  4.0830e-01,  4.9298e-01,  5.9488e-01,\n",
       "            6.1476e-01,  6.3448e-01,  6.5405e-01,  6.7346e-01,  7.1182e-01,\n",
       "            8.4157e-01], grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147,\n",
       "           -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147,\n",
       "           -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147,\n",
       "           -0.9147, -0.9147, -0.9147, -0.9147, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8685,\n",
       "           -0.8685, -0.8685, -0.8685, -0.8685, -0.8685, -0.8338, -0.8338, -0.8338,\n",
       "           -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338,\n",
       "           -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338,\n",
       "           -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338,\n",
       "           -0.8338, -0.8338, -0.8338, -0.8338, -0.7934, -0.7934, -0.7934, -0.7934,\n",
       "           -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934,\n",
       "           -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934,\n",
       "           -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067,\n",
       "           -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.7067, -0.5449, -0.5449,\n",
       "           -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449,\n",
       "           -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449,\n",
       "           -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449,\n",
       "           -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449,\n",
       "           -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.5449,\n",
       "           -0.5449, -0.5449, -0.5449, -0.5449, -0.5449, -0.3426, -0.3426, -0.3426,\n",
       "           -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426,\n",
       "           -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426,\n",
       "           -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426,\n",
       "           -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426,\n",
       "           -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426,\n",
       "           -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426, -0.3426,\n",
       "           -0.3426, -0.3426, -0.3426, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,\n",
       "           -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807, -0.1807,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,  0.3452,\n",
       "            0.3452,  0.3452,  0.3452,  0.3452,  0.8711,  0.8711,  0.8711,  0.8711,\n",
       "            0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,\n",
       "            0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,\n",
       "            0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,\n",
       "            0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,\n",
       "            0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,\n",
       "            0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,\n",
       "            0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,  0.8711,\n",
       "            0.8711,  0.8711,  0.8711,  0.8711,  2.4490,  2.4490,  2.4490,  2.4490,\n",
       "            2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,\n",
       "            2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,\n",
       "            2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,\n",
       "            2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,\n",
       "            2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,\n",
       "            2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,\n",
       "            2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490,  2.4490],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9147, -0.9147, -0.9147,  ...,  3.2176,  3.2176,  3.2176],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147,\n",
       "           -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147,\n",
       "           -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147,\n",
       "           -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147,\n",
       "           -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147,\n",
       "           -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.9147, -0.8743, -0.8743,\n",
       "           -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743,\n",
       "           -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743,\n",
       "           -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743,\n",
       "           -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743, -0.8743,\n",
       "           -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338,\n",
       "           -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338, -0.8338,\n",
       "           -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934, -0.7934,\n",
       "           -0.7934, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.7471,\n",
       "           -0.7471, -0.7471, -0.7471, -0.7471, -0.7471, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853, -0.5853,\n",
       "           -0.5853, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "           -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212, -0.2212,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,  0.3047,\n",
       "            0.3047,  0.3047,  0.3047,  0.3047,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,\n",
       "            0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  0.4666,  2.5703,  2.5703,\n",
       "            2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,\n",
       "            2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,\n",
       "            2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,\n",
       "            2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,\n",
       "            2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,\n",
       "            2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,\n",
       "            2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,  2.5703,\n",
       "            2.5703,  2.5703,  2.5703,  2.5703,  2.5703],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.4187, 0.3716,\n",
       "           0.3716, 0.3617, 0.3363, 0.3212, 0.3147, 0.3133, 0.3133, 0.3716, 0.3133,\n",
       "           0.3716, 0.3133, 0.3716, 0.3133, 0.3133, 0.3133, 0.3716, 0.3133, 0.3716,\n",
       "           0.3716, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473,\n",
       "           0.3473, 0.3258, 0.3258, 0.3258, 0.3258, 0.3473, 0.3473, 0.3258, 0.3258,\n",
       "           0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3473, 0.3473,\n",
       "           0.3258, 0.3258, 0.3180, 0.3207, 0.3086, 0.2900, 0.2851, 0.2781, 0.2698,\n",
       "           0.2607, 0.2637, 0.2668, 0.2688, 0.2859, 0.3138, 0.2561, 0.2561, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295,\n",
       "           0.3295, 0.3295, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295,\n",
       "           0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.3295, 0.2561, 0.3295, 0.2561,\n",
       "           0.2561, 0.3295, 0.2561, 0.3295, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.3295, 0.3381,\n",
       "           0.3381, 0.3381, 0.3381, 0.3265, 0.3632, 0.3381, 0.3216, 0.3165, 0.3037,\n",
       "           0.3004, 0.2912, 0.2812, 0.2803, 0.2746, 0.2723, 0.2720, 0.2671, 0.2695,\n",
       "           0.2617, 0.2847, 0.2702, 0.2702, 0.3356, 0.2702, 0.3356, 0.2702, 0.3356,\n",
       "           0.2702, 0.2702, 0.3356, 0.3564, 0.3187, 0.3187, 0.3002, 0.2968, 0.2920,\n",
       "           0.2829, 0.2660, 0.2642, 0.2607, 0.2606, 0.2599, 0.2623, 0.2655, 0.2648,\n",
       "           0.2754, 0.2797, 0.2871, 0.2661, 0.2920, 0.2661, 0.3215, 0.2661, 0.3215,\n",
       "           0.2661, 0.3215, 0.3215, 0.2661, 0.3369, 0.3510, 0.3510, 0.3369, 0.3369,\n",
       "           0.3369, 0.3510, 0.3369, 0.3369, 0.3510, 0.3369, 0.3510, 0.3510, 0.3510,\n",
       "           0.3369, 0.3369, 0.3369, 0.3369, 0.3295, 0.3397, 0.3226, 0.3172, 0.3031,\n",
       "           0.2975, 0.2933, 0.2938, 0.2866, 0.2749, 0.2728, 0.2622, 0.2596, 0.2658,\n",
       "           0.2692, 0.2912, 0.3146, 0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.3302, 0.3302, 0.3302,\n",
       "           0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302,\n",
       "           0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.2695, 0.3302, 0.3302, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.3302, 0.3302, 0.2695, 0.3695,\n",
       "           0.5483, 0.3695, 0.3695, 0.5483, 0.5483, 0.5483, 0.5483, 0.5483, 0.3695,\n",
       "           0.5483, 0.3695, 0.3469, 0.4183, 0.3310, 0.3267, 0.3105, 0.3345, 0.3049,\n",
       "           0.3031, 0.3168, 0.3001, 0.2975, 0.3078, 0.2949, 0.2905, 0.2926, 0.2908,\n",
       "           0.2887, 0.2866, 0.2852, 0.2911, 0.2942, 0.2847, 0.2983, 0.2876, 0.2911,\n",
       "           0.3135, 0.3053, 0.3091, 0.3461, 0.3318, 0.3461, 0.3386, 0.3461, 0.3415,\n",
       "           0.3461, 0.3459, 0.6848, 0.3459, 0.6848, 0.6642, 0.3459, 0.5333, 0.5164,\n",
       "           0.4740, 0.4519, 0.4324, 0.4189, 0.3381, 0.3202, 0.3164, 0.3511, 0.3059,\n",
       "           0.3018, 0.2989, 0.2883, 0.2808, 0.2774, 0.2753, 0.2727, 0.2728, 0.2728,\n",
       "           0.2691, 0.2710, 0.2698, 0.2669, 0.2671, 0.2664, 0.2710, 0.2712, 0.2666,\n",
       "           0.2672, 0.2711, 0.2715, 0.2728, 0.2745, 0.2770, 0.2782, 0.2864, 0.3104,\n",
       "           0.3104, 0.2921, 0.3104, 0.3076, 0.3133, 0.3104, 0.3256, 0.3104, 0.3273,\n",
       "           0.3104, 0.3472, 0.3472, 0.3472, 0.3822, 0.3472, 0.3472, 0.3822, 0.3422,\n",
       "           0.3822, 0.3320, 0.3775, 0.3280, 0.3672, 0.3508, 0.3103, 0.3265, 0.3036,\n",
       "           0.2970, 0.3053, 0.2938, 0.3040, 0.2912, 0.2958, 0.2864, 0.2824, 0.2764,\n",
       "           0.2747, 0.2725, 0.2711, 0.2698, 0.2707, 0.2691, 0.2686, 0.2639, 0.2656,\n",
       "           0.2663, 0.2630, 0.2670, 0.2628, 0.2658, 0.2692, 0.2642, 0.2661, 0.2684,\n",
       "           0.2775, 0.2869, 0.2849, 0.2849, 0.2853, 0.2869, 0.2849, 0.2849, 0.2919,\n",
       "           0.2849, 0.3020, 0.3043, 0.2849, 0.3108, 0.2849, 0.3125, 0.2849, 0.3207,\n",
       "           0.2849, 0.3229, 0.2849, 0.3229, 0.3229, 0.3124, 0.2849, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.6576, 0.6189, 0.5936, 0.3469, 0.3469, 0.5412,\n",
       "           0.3469, 0.3469, 0.3469, 0.4520, 0.4388, 0.3434, 0.4219, 0.3313, 0.3866,\n",
       "           0.3244, 0.3141, 0.3109, 0.3080, 0.3051, 0.3263, 0.3153, 0.2955, 0.2940,\n",
       "           0.3014, 0.2983, 0.2901, 0.2870, 0.2879, 0.2810, 0.2771, 0.2740, 0.2726,\n",
       "           0.2743, 0.2690, 0.2725, 0.2678, 0.2671, 0.2746, 0.2683, 0.2779, 0.2691,\n",
       "           0.2702, 0.2804, 0.2854, 0.2824, 0.2965, 0.2942, 0.2804, 0.3037, 0.2857,\n",
       "           0.3085, 0.3089, 0.2902, 0.2918, 0.2932, 0.3089, 0.3089, 0.2964, 0.3089,\n",
       "           0.3042, 0.3057, 0.3089, 0.3089, 0.3116, 0.3116, 0.3089, 0.3116, 0.3089,\n",
       "           0.3116, 0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3116, 0.3116,\n",
       "           0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116,\n",
       "           0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3439, 0.3439, 0.4487,\n",
       "           0.4355, 0.3331, 0.4234, 0.4020, 0.3242, 0.3154, 0.3551, 0.3130, 0.3100,\n",
       "           0.3391, 0.3078, 0.3058, 0.3209, 0.3031, 0.3157, 0.2985, 0.3037, 0.2965,\n",
       "           0.2930, 0.2948, 0.2915, 0.2874, 0.2867, 0.2854, 0.2849, 0.2848, 0.2818,\n",
       "           0.2839, 0.2803, 0.2834, 0.2811, 0.2824, 0.2822, 0.2819, 0.2779, 0.2814,\n",
       "           0.2811, 0.2763, 0.2819, 0.2824, 0.2767, 0.2829, 0.2817, 0.2824, 0.2765,\n",
       "           0.2772, 0.2783, 0.2858, 0.2779, 0.2890, 0.2871, 0.2882, 0.2902, 0.2938,\n",
       "           0.2975, 0.3004, 0.2849, 0.2853, 0.2980, 0.3009, 0.2999, 0.5636, 0.3514,\n",
       "           0.3463, 0.5072, 0.3388, 0.4173, 0.3351, 0.3262, 0.3204, 0.3153, 0.3135,\n",
       "           0.3134, 0.3101, 0.3126, 0.3115, 0.3105, 0.3050, 0.3026, 0.3019, 0.3061,\n",
       "           0.3052, 0.2983, 0.3050, 0.2977, 0.3053, 0.3043, 0.2967, 0.2968, 0.3043,\n",
       "           0.3043, 0.3040, 0.2955, 0.2960, 0.3071, 0.3091, 0.3101, 0.2967, 0.3110,\n",
       "           0.2989, 0.3118, 0.3167, 0.3020, 0.3136, 0.3194, 0.3105, 0.3119, 0.3129,\n",
       "           0.3137, 0.3182, 0.3192, 0.3202, 0.3194, 0.3194, 0.3232, 0.3194, 0.3194,\n",
       "           0.3284, 0.3320, 0.3194]),\n",
       "   tensor([0.4101, 0.4484, 0.4484,  ..., 0.3084, 0.3084, 0.3084]),\n",
       "   tensor([0.3682, 0.3347, 0.3347, 0.3682, 0.3682, 0.3682, 0.3347, 0.3347, 0.3682,\n",
       "           0.3682, 0.3682, 0.3682, 0.3347, 0.3682, 0.3347, 0.3347, 0.3347, 0.3347,\n",
       "           0.3682, 0.3347, 0.3682, 0.3682, 0.3682, 0.3347, 0.3682, 0.3682, 0.3682,\n",
       "           0.3347, 0.3682, 0.3347, 0.3347, 0.3347, 0.3567, 0.3347, 0.3327, 0.2965,\n",
       "           0.2824, 0.2700, 0.2548, 0.2666, 0.2585, 0.2842, 0.2842, 0.3289, 0.2842,\n",
       "           0.2842, 0.3506, 0.3506, 0.2507, 0.3506, 0.2507, 0.3506, 0.2507, 0.3506,\n",
       "           0.3506, 0.2507, 0.3506, 0.2507, 0.3506, 0.2507, 0.2507, 0.3506, 0.3506,\n",
       "           0.2507, 0.2507, 0.3506, 0.2507, 0.2507, 0.3496, 0.2507, 0.2507, 0.2904,\n",
       "           0.2517, 0.2561, 0.2562, 0.2342, 0.2294, 0.2164, 0.2091, 0.2190, 0.3352,\n",
       "           0.4354, 0.3190, 0.4266, 0.3190, 0.3190, 0.3534, 0.3190, 0.3091, 0.2880,\n",
       "           0.2944, 0.2443, 0.2243, 0.2071, 0.2117, 0.2099, 0.2826, 0.2755, 0.2582,\n",
       "           0.2145, 0.2090, 0.2061, 0.2037, 0.2176, 0.2022, 0.2970, 0.3306, 0.3306,\n",
       "           0.3306, 0.2970, 0.3306, 0.2970, 0.3306, 0.3306, 0.2970, 0.2970, 0.3306,\n",
       "           0.2970, 0.3306, 0.3306, 0.2970, 0.2970, 0.2970, 0.2970, 0.3306, 0.2970,\n",
       "           0.3306, 0.2970, 0.3306, 0.3306, 0.3306, 0.2970, 0.3306, 0.2970, 0.3306,\n",
       "           0.2970, 0.2970, 0.3306, 0.2970, 0.2970, 0.3306, 0.3306, 0.2970, 0.3306,\n",
       "           0.3306, 0.3306, 0.3306, 0.3306, 0.2970, 0.3306, 0.3306, 0.2970, 0.3306,\n",
       "           0.2970, 0.2970, 0.2970, 0.3306, 0.3306, 0.2970, 0.3306, 0.2970, 0.3306,\n",
       "           0.2891, 0.2746, 0.2631, 0.2611, 0.2497, 0.2467, 0.2365, 0.2337, 0.2293,\n",
       "           0.2227, 0.2196, 0.2141, 0.2058, 0.2041, 0.2020, 0.2034, 0.1964, 0.1938,\n",
       "           0.1956, 0.2039, 0.2089, 0.2125, 0.2141, 0.2055, 0.2199, 0.2352, 0.2055,\n",
       "           0.2055, 0.2359, 0.2359, 0.2055, 0.2359, 0.2359, 0.2055, 0.2055, 0.3246,\n",
       "           0.3246, 0.5744, 0.3246, 0.3246, 0.5568, 0.5395, 0.5309, 0.3246, 0.3246,\n",
       "           0.3246, 0.4978, 0.3246, 0.4812, 0.3246, 0.3246, 0.4898, 0.3246, 0.3246,\n",
       "           0.4176, 0.3246, 0.3175, 0.3175, 0.3246, 0.3175, 0.3246, 0.3246, 0.3175,\n",
       "           0.3215, 0.3175, 0.3246, 0.3175, 0.3246, 0.3175, 0.3240, 0.3246, 0.3170,\n",
       "           0.3122, 0.3094, 0.3018, 0.3002, 0.3004, 0.2959, 0.2921, 0.2943, 0.2908,\n",
       "           0.2839, 0.2849, 0.2721, 0.2683, 0.2667, 0.2629, 0.2564, 0.2549, 0.2592,\n",
       "           0.2558, 0.2515, 0.2486, 0.2514, 0.2470, 0.2476, 0.2479, 0.2430, 0.2400,\n",
       "           0.2417, 0.2399, 0.2355, 0.2346, 0.2344, 0.2315, 0.2366, 0.2316, 0.2308,\n",
       "           0.2357, 0.2254, 0.2370, 0.2333, 0.2379, 0.2391, 0.2335, 0.2386, 0.2335,\n",
       "           0.2348, 0.2314, 0.2331, 0.2449, 0.2191, 0.2191, 0.2191, 0.2191, 0.2191,\n",
       "           0.2519, 0.2191, 0.2519, 0.2519, 0.2519, 0.2519, 0.2191, 0.2519, 0.2191,\n",
       "           0.2952, 0.2919, 0.2919, 0.2919, 0.2919, 0.2952, 0.2919, 0.2919, 0.2919,\n",
       "           0.2952, 0.2919, 0.2919, 0.2919, 0.2952, 0.2952, 0.2919, 0.2919, 0.2919,\n",
       "           0.2952, 0.2919, 0.2919, 0.2952, 0.2919, 0.2919, 0.2919, 0.2952, 0.2952,\n",
       "           0.2919, 0.2919, 0.2919, 0.2919, 0.2919, 0.2952, 0.2919, 0.2919, 0.2952,\n",
       "           0.2952, 0.2919, 0.2919, 0.2950, 0.2919, 0.2887, 0.2853, 0.2921, 0.2816,\n",
       "           0.2791, 0.2773, 0.2767, 0.2738, 0.2676, 0.2649, 0.2632, 0.2624, 0.2642,\n",
       "           0.2611, 0.2557, 0.2495, 0.2513, 0.2412, 0.2415, 0.2382, 0.2405, 0.2336,\n",
       "           0.2323, 0.2297, 0.2274, 0.2258, 0.2250, 0.2253, 0.2228, 0.2180, 0.2165,\n",
       "           0.2205, 0.2155, 0.2146, 0.2190, 0.2194, 0.2225, 0.2246, 0.2279, 0.2139,\n",
       "           0.2198, 0.2137, 0.2137, 0.2330, 0.2137, 0.2403, 0.3009, 0.3009, 0.2944,\n",
       "           0.3009, 0.3009, 0.2944, 0.2944, 0.2944, 0.2944, 0.3009, 0.3009, 0.3009,\n",
       "           0.2944, 0.2944, 0.3009, 0.2944, 0.3009, 0.2944, 0.2944, 0.3009, 0.2944,\n",
       "           0.2980, 0.2989, 0.2955, 0.3009, 0.2947, 0.2932, 0.2955, 0.2756, 0.2723,\n",
       "           0.2693, 0.2765, 0.2650, 0.2723, 0.2702, 0.2626, 0.2631, 0.2620, 0.2552,\n",
       "           0.2574, 0.2528, 0.2570, 0.2537, 0.2523, 0.2479, 0.2469, 0.2480, 0.2426,\n",
       "           0.2412, 0.2426, 0.2399, 0.2363, 0.2391, 0.2347, 0.2381, 0.2371, 0.2367,\n",
       "           0.2294, 0.2307, 0.2281, 0.2257, 0.2237, 0.2247, 0.2236, 0.2226, 0.2228,\n",
       "           0.2199, 0.2224, 0.2212, 0.2220, 0.2183, 0.2229, 0.2236, 0.2272, 0.2280,\n",
       "           0.2267, 0.2949, 0.2949, 0.2642, 0.2949, 0.2642, 0.2949, 0.2642, 0.2949,\n",
       "           0.2642, 0.2642, 0.2642, 0.2949, 0.2642, 0.2949, 0.2949, 0.2642, 0.2949,\n",
       "           0.2949, 0.2642, 0.2949, 0.2642, 0.2642, 0.2949, 0.2642, 0.2949, 0.2642,\n",
       "           0.2642, 0.2642, 0.2949, 0.2949, 0.2949, 0.2642, 0.2949, 0.2949, 0.2642,\n",
       "           0.2642, 0.2642, 0.2949, 0.2949, 0.2949, 0.2642, 0.2642, 0.2642, 0.2642,\n",
       "           0.2949, 0.2910, 0.2642, 0.2884, 0.2789, 0.2745, 0.2731, 0.2642, 0.2646,\n",
       "           0.2607, 0.2567, 0.2591, 0.2524, 0.2502, 0.2548, 0.2513, 0.2460, 0.2483,\n",
       "           0.2453, 0.2438, 0.2491, 0.2421, 0.2352, 0.2384, 0.2335, 0.2324, 0.2333,\n",
       "           0.2283, 0.2297, 0.2277, 0.2287, 0.2265, 0.2251, 0.2256, 0.2258, 0.2221,\n",
       "           0.2207, 0.2181, 0.2214, 0.2181, 0.2211, 0.2174, 0.2176, 0.2166, 0.2166,\n",
       "           0.2146, 0.2152, 0.2151, 0.2179, 0.2163, 0.2217, 0.2193, 0.2220, 0.2198,\n",
       "           0.2960, 0.3628, 0.2960, 0.2960, 0.2957, 0.3032, 0.3190, 0.3177, 0.2983,\n",
       "           0.3045, 0.3007, 0.2977, 0.2939, 0.2892, 0.2861, 0.2844, 0.2781, 0.2780,\n",
       "           0.2698, 0.2695, 0.2666, 0.2616, 0.2627, 0.2605, 0.2610, 0.2582, 0.2532,\n",
       "           0.2528, 0.2501, 0.2503, 0.2478, 0.2450, 0.2441, 0.2433, 0.2424, 0.2416,\n",
       "           0.2404, 0.2398, 0.2380, 0.2389, 0.2390, 0.2380, 0.2372, 0.2371, 0.2370,\n",
       "           0.2365, 0.2363, 0.2359, 0.2352, 0.2347, 0.2337, 0.2336, 0.2334, 0.2327,\n",
       "           0.2333, 0.2319, 0.2316, 0.2318, 0.2314, 0.2318, 0.2308, 0.2310, 0.2302])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([ 0.0963,  0.8528,  0.4051,  ...,  0.6188, -1.0026, -0.3768],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.3766,  0.0611,  0.0860,  ...,  1.5539,  1.4707,  1.6607],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.6694,  0.4510,  0.0988,  ..., -0.7544, -1.5667, -1.6650],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.8338, -0.8685, -0.8685,  ...,  2.4490,  0.8711,  0.8711],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([ 1.1139,  0.5879, -0.8281,  ..., -0.0998,  0.5879,  1.1139],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([0.3047, 0.3047, 0.3047,  ..., 2.5703, 2.5703, 2.5703],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.2806, 0.3295, 0.2561,  ..., 0.2955, 0.3111, 0.2893]),\n",
       "   tensor([0.2795, 0.2733, 0.3101,  ..., 0.3701, 0.2938, 0.2950]),\n",
       "   tensor([0.2561, 0.2259, 0.2187,  ..., 0.2626, 0.2929, 0.2982])]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "class SurfaceBatchNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features=1, \n",
    "        momentum=0.1\n",
    "    ):\n",
    "        super(SurfaceBatchNorm, self).__init__()\n",
    "        self.log_moneyness_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.time_to_maturity_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_return_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_volatility_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.treasury_rate_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.iv_mean_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.iv_std_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Concatenate all tensors from the Input Surface into one tensor for each feature\n",
    "        input_surface_log_moneyness = torch.cat([x for x in batch['Input Surface']['Log Moneyness']])\n",
    "        input_surface_time_to_maturity = torch.cat([x for x in batch['Input Surface']['Time to Maturity']])\n",
    "\n",
    "        # Concatenate Input Surface tensors with Query Points tensors\n",
    "        total_log_moneyness = torch.cat([input_surface_log_moneyness] + [x for x in batch['Query Points']['Log Moneyness']])\n",
    "        total_time_to_maturity = torch.cat([input_surface_time_to_maturity] + [x for x in batch['Query Points']['Time to Maturity']])\n",
    "\n",
    "        # Normalize Log Moneyness and Time to Maturity\n",
    "        norm_log_moneyness = self.log_moneyness_bn(total_log_moneyness.unsqueeze(1)).squeeze(1)\n",
    "        norm_time_to_maturity = self.time_to_maturity_bn(total_time_to_maturity.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Split the normalized results back to corresponding structures\n",
    "        input_surface_sizes = [len(x) for x in batch['Input Surface']['Log Moneyness']]\n",
    "        query_points_sizes = [len(x) for x in batch['Query Points']['Log Moneyness']]\n",
    "        total_input_size = sum(input_surface_sizes)\n",
    "\n",
    "        # Normalizing Market Features\n",
    "        market_features = batch['Market Features']\n",
    "        norm_market_return = self.market_return_bn(market_features['Market Return'].unsqueeze(1)).squeeze(1)\n",
    "        norm_market_volatility = self.market_volatility_bn(market_features['Market Volatility'].unsqueeze(1)).squeeze(1)\n",
    "        norm_treasury_rate = self.treasury_rate_bn(market_features['Treasury Rate'].unsqueeze(1)).squeeze(1)\n",
    "        norm_iv_mean = self.iv_mean_bn(market_features['IV Mean'].unsqueeze(1)).squeeze(1)\n",
    "        norm_iv_std = self.iv_std_bn(market_features['IV Std.'].unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Reconstructing the batch with normalized data\n",
    "        output = {\n",
    "            'Datetime': batch['Datetime'],\n",
    "            'Symbol': batch['Symbol'],\n",
    "            'Mask Proportion': batch['Mask Proportion'],\n",
    "            'Market Features': {\n",
    "                'Market Return': norm_market_return,\n",
    "                'Market Volatility': norm_market_volatility,\n",
    "                'Treasury Rate': norm_treasury_rate,\n",
    "                'IV Mean': norm_iv_mean,\n",
    "                'IV Std.': norm_iv_std\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[:total_input_size], input_surface_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[:total_input_size], input_surface_sizes)),\n",
    "                'Implied Volatility': batch['Input Surface']['Implied Volatility']\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[total_input_size:], query_points_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[total_input_size:], query_points_sizes)),\n",
    "                'Implied Volatility': batch['Query Points']['Implied Volatility']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Ensure requires_grad is True for query point values\n",
    "        for key in output['Query Points']:\n",
    "            if key != 'Implied Volatility':  # We only set requires_grad for Log Moneyness and Time to Maturity\n",
    "                for tensor in output['Query Points'][key]:\n",
    "                    tensor.requires_grad_()\n",
    "\n",
    "        return output\n",
    "\n",
    "# Usage\n",
    "surfacebatchnorm = SurfaceBatchNorm()\n",
    "processed_batch = surfacebatchnorm(batch)\n",
    "processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class EllipticalRBFKernel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        bandwidth, \n",
    "        remove_kernel=False\n",
    "    ):\n",
    "        super(EllipticalRBFKernel, self).__init__()\n",
    "        self.bandwidth = bandwidth\n",
    "        # Initialize the log of the scale vector to zero, which corresponds to scale factors of one\n",
    "        self.log_scale = nn.Parameter(torch.zeros(input_dim))\n",
    "        self.remove_kernel = remove_kernel\n",
    "\n",
    "    def forward(self, distances):\n",
    "        if self.remove_kernel:\n",
    "            # Create a mask for the condition check\n",
    "            all_zeros = torch.all(distances==0.0, dim=-1)\n",
    "            result = torch.where(\n",
    "                all_zeros, \n",
    "                torch.full(distances.shape[:-1], 1.0, device=distances.device),\n",
    "                torch.full(distances.shape[:-1], 1e-10, device=distances.device)\n",
    "            )\n",
    "            return result\n",
    "        # Convert log scale to actual scale values\n",
    "        scale = torch.exp(self.log_scale)\n",
    "        \n",
    "        # Calculate the scaled distances\n",
    "        scaled_distances = (distances ** 2) * scale  # Element-wise multiplication by scale\n",
    "\n",
    "        # Normalize by the trace of the scale matrix\n",
    "        trace_scale_matrix = torch.sum(scale)\n",
    "        normalized_distances = torch.sum(scaled_distances, dim=-1) / trace_scale_matrix\n",
    "\n",
    "        # Compute the RBF kernel output using the normalized distances\n",
    "        kernel_values = torch.exp(-normalized_distances / (2 * self.bandwidth ** 2))\n",
    "\n",
    "        return kernel_values\n",
    "\n",
    "class SurfaceContinuousKernelPositionalEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceContinuousKernelPositionalEmbedding, self).__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "        self.remove_positional_embedding = remove_positional_embedding\n",
    "\n",
    "        # Initialize multiple RBF kernels, each with a different fixed bandwidth\n",
    "        self.kernels = nn.ModuleList()\n",
    "        for i in range(1, d_embedding + 1):\n",
    "            bandwidth_value = torch.erfinv(torch.tensor(i / (d_embedding + 1))) * np.sqrt(2)\n",
    "            self.kernels.append(\n",
    "                EllipticalRBFKernel(\n",
    "                    bandwidth=bandwidth_value, \n",
    "                    input_dim=2, \n",
    "                    remove_kernel=remove_kernel\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.input_surface_layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.query_points_layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "        # Initialize learnable scaling parameter (the base for positional embedding)\n",
    "        self.log_scale = nn.Parameter(torch.log(torch.tensor(10000.0)))\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_surface_batch, \n",
    "        query_points_batch\n",
    "    ):\n",
    "        batch_size = len(input_surface_batch['Log Moneyness'])\n",
    "\n",
    "        input_surface_embeddings = []\n",
    "        query_points_embeddings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Extract the coordinates and implied volatilities for each surface in the batch\n",
    "            surface_coords = torch.stack([\n",
    "                input_surface_batch['Log Moneyness'][i], \n",
    "                input_surface_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "            surface_ivs = input_surface_batch['Implied Volatility'][i]\n",
    "\n",
    "            query_coords = torch.stack([\n",
    "                query_points_batch['Log Moneyness'][i], \n",
    "                query_points_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "\n",
    "            all_coords = torch.cat((surface_coords, query_coords), dim=0)\n",
    "\n",
    "            # Compute the pairwise differences between all points and the input surface points\n",
    "            point_differences = all_coords.unsqueeze(1) - surface_coords.unsqueeze(0)  # (n+m, n, 2)\n",
    "\n",
    "            # Initialize the output embeddings for the current surface with d_embedding channels\n",
    "            all_embedded = torch.zeros((all_coords.shape[0], self.d_embedding), dtype=torch.float32, device=surface_coords.device)\n",
    "\n",
    "            for kernel_idx, kernel in enumerate(self.kernels):\n",
    "                # Apply the RBF kernel to each distance vector \n",
    "                kernel_outputs = kernel(point_differences)\n",
    "\n",
    "                # Compute the weighted sum of IVs based on the kernel outputs\n",
    "                weighted_sum = (kernel_outputs * surface_ivs.unsqueeze(0)).sum(dim=1)\n",
    "                normalization_factor = kernel_outputs.sum(dim=1)\n",
    "\n",
    "                all_embedded[:, kernel_idx] = weighted_sum / normalization_factor    \n",
    "\n",
    "            # Split the embeddings into input surface and query points embeddings\n",
    "            input_surface_embedded = all_embedded[:surface_coords.shape[0], :]\n",
    "            query_points_embedded = all_embedded[surface_coords.shape[0]:, :]\n",
    "\n",
    "            # Normalize the embedded surfaces\n",
    "            input_surface_embedded = self.input_surface_layer_norm(input_surface_embedded)\n",
    "            query_points_embedded = self.query_points_layer_norm(query_points_embedded)\n",
    "\n",
    "            # Positional embedding for input surface points\n",
    "            input_surface_pe = self._compute_positional_embedding(surface_coords)\n",
    "\n",
    "            # Positional embedding for query points\n",
    "            query_points_pe = self._compute_positional_embedding(query_coords)\n",
    "\n",
    "            # Add positional embeddings with a factor of sqrt(2)\n",
    "            input_surface_final = input_surface_embedded + input_surface_pe * np.sqrt(2)\n",
    "            query_points_final = query_points_embedded + query_points_pe * np.sqrt(2)\n",
    "\n",
    "            # Append the encoded surface for this input surface to the batch list\n",
    "            input_surface_embeddings.append(input_surface_final)\n",
    "            query_points_embeddings.append(query_points_final)\n",
    "\n",
    "        # Keep all encoded surfaces as lists to handle variable lengths\n",
    "        return {\n",
    "            'Input Surface': input_surface_embeddings,\n",
    "            'Query Points': query_points_embeddings\n",
    "        }\n",
    "\n",
    "    def _compute_positional_embedding(\n",
    "        self, \n",
    "        coords, \n",
    "    ):\n",
    "        positional_embedding = torch.zeros(coords.size(0), self.d_embedding, device=coords.device)\n",
    "\n",
    "        if not self.remove_positional_embedding:\n",
    "            for i in range(self.d_embedding // 4):\n",
    "                div_factor = torch.exp(self.log_scale) ** (4 * i / self.d_embedding)\n",
    "                positional_embedding[:, 4 * i] = torch.sin(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 1] = torch.cos(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 2] = torch.sin(coords[:, 1] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 3] = torch.cos(coords[:, 1] / div_factor)\n",
    "\n",
    "        return positional_embedding\n",
    "\n",
    "# Example of initializing and using this module\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "\n",
    "# continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "# kernel_positional_embedded_batch = continuous_kernel_positional_embedding(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "# kernel_positional_embedded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SurfaceEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        momentum=0.1,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceEmbedding, self).__init__()\n",
    "        self.batch_norm = SurfaceBatchNorm(num_features=1, momentum=momentum)\n",
    "        self.kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding, remove_kernel, remove_positional_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.mask_token = nn.Parameter(torch.randn(d_embedding))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Apply batch normalization\n",
    "        norm_batch = self.batch_norm(batch)\n",
    "\n",
    "        # Extract market features from processed batch and create external_features_batch tensor\n",
    "        market_features = norm_batch['Market Features']\n",
    "        external_features_batch = torch.stack([\n",
    "            market_features['Market Return'],\n",
    "            market_features['Market Volatility'],\n",
    "            market_features['Treasury Rate'],\n",
    "            market_features['IV Mean'],\n",
    "            market_features['IV Std.']\n",
    "        ], dim=-1)  # (batch, features)\n",
    "\n",
    "        # Compute kernel and positional embeddings\n",
    "        embeddings = self.kernel_positional_embedding(norm_batch['Input Surface'], norm_batch['Query Points'])\n",
    "\n",
    "        input_surface_embeddings = embeddings['Input Surface']\n",
    "        query_points_embeddings = embeddings['Query Points']\n",
    "\n",
    "        embedded_sequences = []\n",
    "\n",
    "        for input_surface_embedding, query_points_embedding in zip(input_surface_embeddings, query_points_embeddings):\n",
    "            # Add mask token to the query point embeddings\n",
    "            masked_query_points_embedding = query_points_embedding + self.mask_token\n",
    "\n",
    "            # Combine input surface embeddings and masked query points embeddings\n",
    "            combined_sequence = torch.cat((input_surface_embedding, masked_query_points_embedding), dim=0)\n",
    "\n",
    "            # Apply layer normalization\n",
    "            combined_sequence = self.layer_norm(combined_sequence)\n",
    "\n",
    "            embedded_sequences.append(combined_sequence)\n",
    "\n",
    "        return embedded_sequences, external_features_batch\n",
    "\n",
    "\n",
    "# # Example of initializing and using this module\n",
    "# d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "# surface_embedding = SurfaceEmbedding(d_embedding=d_embedding)\n",
    "# embedded_sequences_batch, external_features_batch = surface_embedding(batch)\n",
    "# embedded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualNorm(nn.Module):\n",
    "    def __init__(self, d_embedding):\n",
    "        super(ResidualNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x, \n",
    "        sublayer_output\n",
    "    ):\n",
    "        return self.norm(x + sublayer_output)\n",
    "    \n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        gate_dropout,\n",
    "        weight_initializer_std=0.02,\n",
    "        bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(GatedAttentionFusion, self).__init__()\n",
    "        self.gate_layer = nn.Sequential(\n",
    "            nn.Linear(d_embedding * 2, d_embedding),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(gate_dropout)\n",
    "        )\n",
    "        self.remove_external_attention = remove_external_attention\n",
    "        self.remove_gate = remove_gate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for module in self.gate_layer:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        self_attention_output, \n",
    "        external_attention_output\n",
    "    ):\n",
    "        if self.remove_external_attention:\n",
    "\n",
    "            return self_attention_output\n",
    "\n",
    "        if self.remove_gate:  \n",
    "\n",
    "            return self_attention_output + external_attention_output\n",
    "        # Concatenate self-attention and external attention outputs\n",
    "        concatenated_output = torch.cat((self_attention_output, external_attention_output), dim=-1)\n",
    "        # Compute gate values\n",
    "        gate_values = self.gate_layer(concatenated_output)\n",
    "        # Calculate gated embedding\n",
    "        gated_embedding = gate_values * self_attention_output + (1 - gate_values) * external_attention_output\n",
    "\n",
    "        return gated_embedding\n",
    "    \n",
    "    \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        ffn_hidden_dim, \n",
    "        ffn_dropout, \n",
    "        layer_depth, \n",
    "        weight_initializer_std=0.02, \n",
    "        bias_initializer_value=0,\n",
    "    ):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_embedding, ffn_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ffn_dropout),\n",
    "            nn.Linear(ffn_hidden_dim, d_embedding),\n",
    "            nn.Dropout(ffn_dropout)\n",
    "        )\n",
    "\n",
    "        self.layer_depth = layer_depth\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feedforward(x)\n",
    "    \n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for i, module in enumerate(self.feedforward):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "                \n",
    "                # Rescale the output matrices of the last linear projection\n",
    "                if i == len(self.feedforward) - 2:\n",
    "                    scale_factor = 1 / (2 * self.layer_depth) ** 0.5\n",
    "                    module.weight.data *= scale_factor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        n_heads, \n",
    "        ffn_hidden_dim, \n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        layer_depth,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.residual_norm_self_attention = ResidualNorm(d_embedding)\n",
    "        self.external_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            kdim=external_dim, \n",
    "            vdim=external_dim, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.residual_norm_external_attention = ResidualNorm(d_embedding)\n",
    "        self.gated_attention_fusion = GatedAttentionFusion(\n",
    "            d_embedding, \n",
    "            gate_dropout,\n",
    "            weight_initializer_std,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention, \n",
    "            remove_gate,\n",
    "        )\n",
    "        self.residual_norm_fusion = ResidualNorm(d_embedding)\n",
    "        self.feed_forward = FeedForwardNetwork(\n",
    "            d_embedding, \n",
    "            ffn_hidden_dim, \n",
    "            ffn_dropout, \n",
    "            layer_depth, \n",
    "            weight_initializer_std, \n",
    "            linear_bias_initializer_value\n",
    "        )\n",
    "        self.residual_norm_ffn = ResidualNorm(d_embedding)\n",
    "        # Initialize self-attention\n",
    "        self._initialize_attention_weights(self.self_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "        # Initialize external-attention\n",
    "        self._initialize_attention_weights(self.external_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "\n",
    "    def _initialize_attention_weights(\n",
    "        self, \n",
    "        attention_module, \n",
    "        weight_initializer_std, \n",
    "        linear_bias_initializer_value, \n",
    "        layer_depth\n",
    "    ):\n",
    "        if attention_module._qkv_same_embed_dim:\n",
    "            nn.init.normal_(attention_module.in_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "        else:\n",
    "            nn.init.normal_(attention_module.q_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.k_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.v_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "\n",
    "        if attention_module.in_proj_bias is not None:\n",
    "            nn.init.constant_(attention_module.in_proj_bias, linear_bias_initializer_value)\n",
    "            nn.init.constant_(attention_module.out_proj.bias, linear_bias_initializer_value)\n",
    "        \n",
    "        if attention_module.bias_k is not None:\n",
    "            nn.init.constant_(attention_module.bias_k, linear_bias_initializer_value)\n",
    "        if attention_module.bias_v is not None:\n",
    "            nn.init.constant_(attention_module.bias_v, linear_bias_initializer_value)\n",
    "        \n",
    "        # Transformer layer rescaling for output weights\n",
    "        scale_factor = 1 / (2 * layer_depth) ** 0.5\n",
    "        nn.init.normal_(attention_module.out_proj.weight, mean=0.0, std=weight_initializer_std * scale_factor)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        surface_embeddings, \n",
    "        external_features,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Self-Attention\n",
    "        self_attention_output, self_attention_weights = self.self_attention(surface_embeddings, surface_embeddings, surface_embeddings)\n",
    "        self_attention_output = self.residual_norm_self_attention(surface_embeddings, self_attention_output)\n",
    "        # External Attention\n",
    "        external_attention_output, external_attention_weights = self.external_attention(surface_embeddings, external_features, external_features) \n",
    "        external_attention_output = self.residual_norm_external_attention(surface_embeddings, external_attention_output)\n",
    "        # Gated Attention Fusion\n",
    "        gated_embedding = self.gated_attention_fusion(self_attention_output, external_attention_output)\n",
    "        gated_embedding = self.residual_norm_fusion(surface_embeddings, gated_embedding)\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.feed_forward(gated_embedding)\n",
    "        # Final Residual Connection and Layer Normalization\n",
    "        surface_embeddings = self.residual_norm_ffn(gated_embedding, ffn_output)\n",
    "\n",
    "        if output_attention_map:\n",
    "            # Remove the batch dimension for attention weights\n",
    "            return surface_embeddings, self_attention_weights.squeeze(0), external_attention_weights.squeeze(0)\n",
    "        \n",
    "        return surface_embeddings, None, None\n",
    "\n",
    "class SurfaceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(SurfaceEncoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            Encoder(\n",
    "                d_embedding, \n",
    "                n_heads, \n",
    "                ffn_hidden_dim, \n",
    "                attention_dropout, \n",
    "                gate_dropout,\n",
    "                ffn_dropout,\n",
    "                external_dim,\n",
    "                (i + 1),\n",
    "                weight_initializer_std,\n",
    "                linear_bias_initializer_value,\n",
    "                gate_bias_initializer_value,\n",
    "                remove_external_attention,\n",
    "                remove_gate\n",
    "            )\n",
    "            for i in range(num_encoder_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        embedded_sequences_batch, \n",
    "        external_features_batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        batch_size = len(embedded_sequences_batch)\n",
    "        encoded_sequences_batch = []\n",
    "        self_attention_maps = []\n",
    "        external_attention_maps = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            surface_embeddings = embedded_sequences_batch[i].unsqueeze(1) \n",
    "            external_features = external_features_batch[i].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            for j, encoder in enumerate(self.encoders):\n",
    "                if j == len(self.encoders) - 1 and output_attention_map:\n",
    "                    surface_embeddings, self_attention_map, external_attention_map = encoder(surface_embeddings, external_features, output_attention_map)\n",
    "                    \n",
    "                else:\n",
    "                    surface_embeddings, _, _ = encoder(surface_embeddings, external_features)\n",
    "                \n",
    "            encoded_sequences_batch.append(surface_embeddings.squeeze(1))\n",
    "            if output_attention_map:\n",
    "                self_attention_maps.append(self_attention_map)\n",
    "                external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return encoded_sequences_batch, self_attention_maps, external_attention_maps\n",
    "        \n",
    "        return encoded_sequences_batch, None, None    \n",
    "\n",
    "# Example of initializing and using these modules\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "external_dim = 5\n",
    "\n",
    "surface_encoder = SurfaceEncoder(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim, \n",
    "    attention_dropout, \n",
    "    gate_dropout, \n",
    "    ffn_dropout, \n",
    "    external_dim, \n",
    ")\n",
    "\n",
    "# Assume embedded_sequences_batch is the output of the SurfaceEmbedding module and\n",
    "# external_features is the formatted external market features batch\n",
    "# encoded_sequences_batch, self_attention_map_batch, external_attention_map_batch = surface_encoder(embedded_sequences_batch, external_features_batch)\n",
    "# encoded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IvySPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0189, 0.0146, 0.0169,  ..., 0.0089, 0.0056, 0.0169],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0162, 0.0175, 0.0127,  ..., 0.0128, 0.0146, 0.0121],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0174, 0.0173, 0.0179,  ..., 0.0112, 0.0033, 0.0030],\n",
       "        grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class IvySPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(IvySPT, self).__init__()\n",
    "        self.surface_embedding = SurfaceEmbedding(\n",
    "            d_embedding, \n",
    "            remove_kernel, \n",
    "            remove_positional_embedding\n",
    "        )\n",
    "        self.surface_encoder = SurfaceEncoder(\n",
    "            d_embedding, \n",
    "            num_encoder_blocks,\n",
    "            n_heads, \n",
    "            ffn_hidden_dim,\n",
    "            attention_dropout, \n",
    "            gate_dropout,\n",
    "            ffn_dropout,\n",
    "            external_dim,\n",
    "            weight_initializer_std,\n",
    "            linear_bias_initializer_value,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention,\n",
    "            remove_gate\n",
    "        )\n",
    "        self.final_layer = nn.Linear(d_embedding, 1)\n",
    "        nn.init.normal_(self.final_layer.weight, mean=0.0, std=weight_initializer_std * (1 / (2 * (num_encoder_blocks + 1)) ** 0.5))\n",
    "        nn.init.constant_(self.final_layer.bias, linear_bias_initializer_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Obtain the embedded sequences and external features from the SurfaceEmbedding module\n",
    "        embedded_sequences_batch, external_features_batch = self.surface_embedding(batch)\n",
    "\n",
    "        # Encode the sequences using the SurfaceEncoder module\n",
    "        encoded_sequences_batch, self_attention_maps, external_attention_maps = self.surface_encoder(\n",
    "            embedded_sequences_batch, \n",
    "            external_features_batch, \n",
    "            output_attention_map\n",
    "        )\n",
    "\n",
    "        # List to hold the implied volatility estimates for each query point in the batch\n",
    "        iv_estimates_batch = []\n",
    "\n",
    "        query_self_attention_maps = []\n",
    "        query_external_attention_maps = []\n",
    "\n",
    "        for i in range(len(encoded_sequences_batch)):\n",
    "            # Extract the encoded sequence\n",
    "            encoded_sequence = encoded_sequences_batch[i]\n",
    "\n",
    "            # Determine the number of query points for this sequence\n",
    "            num_query_points = len(batch['Query Points']['Log Moneyness'][i])\n",
    "\n",
    "            # Extract the encoded query points (last num_query_points elements in the sequence)\n",
    "            encoded_query_points = encoded_sequence[-num_query_points:]\n",
    "\n",
    "            # Estimate the implied volatility for each query point using the fully connected layer\n",
    "            iv_estimates = self.final_layer(encoded_query_points).squeeze(-1)\n",
    "\n",
    "            # Append the estimates to the batch list\n",
    "            iv_estimates_batch.append(iv_estimates)\n",
    "\n",
    "            if output_attention_map:\n",
    "                # Extract the attention maps for the query points\n",
    "                self_attention_map = self_attention_maps[i][-num_query_points:]\n",
    "                external_attention_map = external_attention_maps[i][-num_query_points:]\n",
    "\n",
    "                query_self_attention_maps.append(self_attention_map)\n",
    "                query_external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return iv_estimates_batch, self_attention_maps, external_attention_maps\n",
    "        \n",
    "        return iv_estimates_batch, None, None\n",
    "\n",
    "# Example of initializing and using this module\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "external_dim = 5\n",
    "\n",
    "ivy_spt = IvySPT(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim,\n",
    "    attention_dropout, \n",
    "    gate_dropout,\n",
    "    ffn_dropout,\n",
    "    external_dim\n",
    ")\n",
    "\n",
    "# Pass the batch through the IvySPT model to get implied volatility estimates\n",
    "iv_estimates_batch, self_attention_maps, external_attention_maps = ivy_spt(batch, output_attention_map=False)\n",
    "gc.collect()\n",
    "iv_estimates_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.2806, 0.3295, 0.2561,  ..., 0.2955, 0.3111, 0.2893]),\n",
       " tensor([0.2795, 0.2733, 0.3101,  ..., 0.3701, 0.2938, 0.2950]),\n",
       " tensor([0.2561, 0.2259, 0.2187,  ..., 0.2626, 0.2929, 0.2982])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['Query Points']['Implied Volatility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1875, grad_fn=<AddBackward0>),\n",
       " tensor(0.0908, grad_fn=<MeanBackward0>),\n",
       " tensor(7.0262e-11, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0967, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SurfaceArbitrageFreeLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mse_coeff=1,\n",
    "        calendar_coeff=1,\n",
    "        butterfly_coeff=1\n",
    "    ):\n",
    "        super(SurfaceArbitrageFreeLoss, self).__init__()\n",
    "        self.mse_coeff = mse_coeff\n",
    "        self.calendar_coeff = calendar_coeff\n",
    "        self.butterfly_coeff = butterfly_coeff\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        iv_estimates_batch, \n",
    "        batch,\n",
    "        testing_mode=False\n",
    "    ):\n",
    "        mse_losses = []\n",
    "        calendar_arbitrage_losses = []\n",
    "        butterfly_arbitrage_losses = []\n",
    "        loss_records = []\n",
    "\n",
    "        for iv_estimates, target_volatility, time_to_maturity, log_moneyness in zip(\n",
    "            iv_estimates_batch, \n",
    "            batch['Query Points']['Implied Volatility'], \n",
    "            batch['Query Points']['Time to Maturity'], \n",
    "            batch['Query Points']['Log Moneyness']\n",
    "        ):\n",
    "            # Calculate mean squared error between model estimates and target volatilities\n",
    "            mse_loss = F.mse_loss(iv_estimates, target_volatility, reduction='none')\n",
    "            mse_losses.append(mse_loss)\n",
    "\n",
    "            # Calculate the total implied variance\n",
    "            total_implied_variance = time_to_maturity * iv_estimates.pow(2)\n",
    "\n",
    "            # Compute gradients needed for arbitrage conditions\n",
    "            w_t = torch.autograd.grad(total_implied_variance.sum(), time_to_maturity, create_graph=True)[0] \n",
    "            w_x = torch.autograd.grad(total_implied_variance.sum(), log_moneyness, create_graph=True)[0]\n",
    "            w_xx = torch.autograd.grad(w_x.sum(), log_moneyness, create_graph=True)[0]\n",
    "\n",
    "            # Calculate Calendar Arbitrage Loss\n",
    "            calendar_arbitrage_loss = torch.clamp(-w_t, min=0) ** 2\n",
    "            calendar_arbitrage_losses.append(calendar_arbitrage_loss)\n",
    "\n",
    "            # Calculate Butterfly Arbitrage Loss\n",
    "            w = total_implied_variance\n",
    "            g = (1 - log_moneyness * w_x / (2 * w)) ** 2 - w_x / 4 * (1 / w + 1 / 4) + w_xx / 2\n",
    "            butterfly_arbitrage_loss = torch.clamp(-g, min=0) ** 2\n",
    "            butterfly_arbitrage_losses.append(butterfly_arbitrage_loss)\n",
    "\n",
    "            if testing_mode:\n",
    "                record = {\n",
    "                    'MSE Loss': mse_loss.mean().item(),\n",
    "                    'Calendar Arbitrage Loss': calendar_arbitrage_loss.mean().item(),\n",
    "                    'Butterfly Arbitrage Loss': butterfly_arbitrage_loss.mean().item()\n",
    "                }\n",
    "                loss_records.append(record)\n",
    "\n",
    "        # Combine all losses\n",
    "        mse_loss = torch.cat(mse_losses).mean()\n",
    "        calendar_arbitrage_loss = torch.cat(calendar_arbitrage_losses).mean()\n",
    "        butterfly_arbitrage_loss = torch.cat(butterfly_arbitrage_losses).mean()\n",
    "\n",
    "        if testing_mode:\n",
    "            loss_df = pd.DataFrame(loss_records)\n",
    "            loss_df['Datetime'] = batch['Datetime']\n",
    "            loss_df['Mask Proportion'] = batch['Mask Proportion']\n",
    "            loss_df.set_index(['Datetime', 'Mask Proportion'], inplace=True)\n",
    "\n",
    "            return loss_df, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss\n",
    "\n",
    "        total_loss = self.mse_coeff * mse_loss + self.calendar_coeff * calendar_arbitrage_loss + \\\n",
    "            self.butterfly_coeff * butterfly_arbitrage_loss\n",
    "\n",
    "        return total_loss, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss\n",
    "\n",
    "surface_arbitrage_free_loss = SurfaceArbitrageFreeLoss()  \n",
    "total_loss, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss = surface_arbitrage_free_loss(iv_estimates_batch, batch)\n",
    "total_loss, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del total_loss, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
