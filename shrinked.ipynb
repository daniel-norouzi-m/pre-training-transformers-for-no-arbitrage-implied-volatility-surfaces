{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Number of Query Points' : 1,\n",
    "        'Batch Size' : 5\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 8,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 16,\n",
    "        'Attention Dropout' : 0.1,\n",
    "        'Gate Dropout' : 0.1,\n",
    "        'FFN Dropout' : 0.1,\n",
    "        'Number of Blocks' : 2,\n",
    "        'External Feature Dimension' : 5,\n",
    "    },\n",
    "    'No-Arbitrage' : {\n",
    "        'Butterfly' : 1,\n",
    "        'Calendar' : 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Log Moneyness</th>\n",
       "      <th>Time to Maturity</th>\n",
       "      <th>Implied Volatility</th>\n",
       "      <th>Market Return</th>\n",
       "      <th>Market Volatility</th>\n",
       "      <th>Treasury Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.157580</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.157580</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.145163</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.145163</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.132898</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-06-28</th>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.401026</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.407931</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.407931</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.414785</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.414785</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574326 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Log Moneyness  Time to Maturity  Implied Volatility  \\\n",
       "Datetime   Symbol                                                        \n",
       "2013-01-02 AAPL        -0.157580          0.007937              0.3726   \n",
       "           AAPL        -0.157580          0.007937              0.6095   \n",
       "           AAPL        -0.145163          0.007937              0.3726   \n",
       "           AAPL        -0.145163          0.007937              0.6095   \n",
       "           AAPL        -0.132898          0.007937              0.3726   \n",
       "...                          ...               ...                 ...   \n",
       "2013-06-28 GOOGL        0.401026          2.253968              0.2430   \n",
       "           GOOGL        0.407931          2.253968              0.2383   \n",
       "           GOOGL        0.407931          2.253968              0.2426   \n",
       "           GOOGL        0.414785          2.253968              0.2402   \n",
       "           GOOGL        0.414785          2.253968              0.2433   \n",
       "\n",
       "                   Market Return  Market Volatility  Treasury Rate  \n",
       "Datetime   Symbol                                                   \n",
       "2013-01-02 AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "...                          ...                ...            ...  \n",
       "2013-06-28 GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "\n",
       "[574326 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_googl_data = pd.read_csv('volatility_surface_AAPL_GOOGL_2013_01_2013_06.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "aapl_googl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yfinance as yf\n",
    "# # Load the data\n",
    "# aapl_googl_data = pd.read_csv('volatility_surface_AAPL_GOOGL_2013_01_2013_06.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "# # Fetch historical close and adjusted close prices for AAPL and GOOGL\n",
    "# aapl = yf.download('AAPL', start='2013-01-01', end='2013-06-30')\n",
    "# googl = yf.download('GOOG', start='2013-01-01', end='2013-06-30')\n",
    "# # Create a dictionary to hold close and adjusted close prices for easy access\n",
    "# prices = {\n",
    "#     'AAPL': {'Close': aapl['Close'], 'Adj Close': aapl['Adj Close']},\n",
    "#     'GOOGL': {'Close': googl['Close'], 'Adj Close': googl['Adj Close']}\n",
    "# }\n",
    "# # Define a function to calculate the modified log moneyness\n",
    "# def modified_log_moneyness(row):\n",
    "#     symbol = row.name[1]\n",
    "#     date = row.name[0]\n",
    "#     close_price = prices[symbol]['Close'][date]\n",
    "#     adj_close_price = prices[symbol]['Adj Close'][date]\n",
    "#     log_moneyness = row['Log Moneyness'] + np.log(close_price / adj_close_price)\n",
    "    \n",
    "#     treasury_rate = row['Treasury Rate']\n",
    "#     time_to_maturity = row['Time to Maturity']\n",
    "#     exponential_treasury_rate = np.log(1 + treasury_rate)\n",
    "#     discount_factor = np.exp(-exponential_treasury_rate * time_to_maturity)\n",
    "    \n",
    "#     return log_moneyness * discount_factor\n",
    "# # Apply the function to each row\n",
    "# aapl_googl_data['Log Moneyness'] = aapl_googl_data.apply(modified_log_moneyness, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': Timestamp('2013-01-02 00:00:00'),\n",
       " 'Symbol': 'AAPL',\n",
       " 'Market Features': {'Market Return': 0.0250861159586972,\n",
       "  'Market Volatility': 14.68000030517578,\n",
       "  'Treasury Rate': 0.0549999997019767},\n",
       " 'Surface': {'Log Moneyness': array([-0.1575799 , -0.1575799 , -0.14516266, ...,  0.68106222,\n",
       "          0.68923077,  0.68923077]),\n",
       "  'Time to Maturity': array([0.00793651, 0.00793651, 0.00793651, ..., 2.95634921, 2.95634921,\n",
       "         2.95634921]),\n",
       "  'Implied Volatility': array([0.3726, 0.6095, 0.3726, ..., 0.3387, 0.3342, 0.3389])}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def implied_volatility_surfaces(options_market_data):\n",
    "    # Group the data by Datetime and Symbol\n",
    "    grouped_data = options_market_data.groupby(level=['Datetime', 'Symbol'])\n",
    "\n",
    "    surfaces = []\n",
    "    for (date, symbol), surface in grouped_data:\n",
    "        surface_dict = {\n",
    "            'Datetime': date,\n",
    "            'Symbol': symbol,\n",
    "            'Market Features': {\n",
    "                'Market Return': surface['Market Return'].values[0],\n",
    "                'Market Volatility': surface['Market Volatility'].values[0],\n",
    "                'Treasury Rate': surface['Treasury Rate'].values[0],\n",
    "            },\n",
    "            'Surface': {\n",
    "                'Log Moneyness': surface['Log Moneyness'].values,\n",
    "                'Time to Maturity': surface['Time to Maturity'].values,\n",
    "                'Implied Volatility': surface['Implied Volatility'].values,\n",
    "            }\n",
    "        }\n",
    "        surfaces.append(surface_dict)\n",
    "\n",
    "    return surfaces\n",
    "\n",
    "surfaces = implied_volatility_surfaces(aapl_googl_data)\n",
    "surfaces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00'),\n",
       "  Timestamp('2013-03-07 00:00:00'),\n",
       "  Timestamp('2013-02-06 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL', 'AAPL', 'GOOGL'],\n",
       " 'Mask Proportion': [0.7, 0.7, 0.3, 0.3, 0.1],\n",
       " 'Market Features': {'Market Return': tensor([-0.0003, -0.0019, -0.0007,  0.0018,  0.0005]),\n",
       "  'Market Volatility': tensor([15.4400, 13.5700, 13.0200, 13.0600, 13.4100]),\n",
       "  'Treasury Rate': tensor([0.0400, 0.0600, 0.0350, 0.0900, 0.0650]),\n",
       "  'IV Mean': tensor([0.3143, 0.3343, 0.2734, 0.3650, 0.2504]),\n",
       "  'IV Std.': tensor([0.0559, 0.0721, 0.0508, 0.1272, 0.0580])},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-0.1384, -0.1086, -0.0107,  0.0156,  0.0285,  0.0538,  0.0662,  0.0662,\n",
       "            0.0785,  0.0906,  0.1026,  0.1261,  0.1491,  0.1604,  0.1935,  0.2043,\n",
       "            0.2256,  0.2464,  0.2566,  0.2668,  0.2768,  0.2768,  0.2868,  0.3160,\n",
       "            0.3351,  0.3351,  0.3444,  0.3537, -0.6381, -0.6381, -0.5894, -0.5430,\n",
       "           -0.5430, -0.4986, -0.4771, -0.4561, -0.3956, -0.3762, -0.3572, -0.3386,\n",
       "           -0.2846, -0.2672, -0.2502, -0.2334, -0.2169, -0.1690, -0.1535, -0.1535,\n",
       "           -0.1085, -0.0795, -0.0514, -0.0241, -0.0241, -0.0107,  0.0285,  0.0412,\n",
       "            0.0537,  0.0537,  0.0661,  0.0905,  0.1024,  0.1143,  0.1375,  0.1823,\n",
       "            0.1932,  0.2040,  0.2147,  0.2357,  0.2664,  0.2765,  0.2962,  0.3060,\n",
       "            0.3156,  0.3346,  0.3440,  0.3625,  0.3806,  0.4160,  0.4246,  0.4332,\n",
       "            0.4750,  0.5150,  0.5306,  0.5306,  0.5384,  0.5384,  0.5460,  0.5686,\n",
       "            0.5686,  0.5834,  0.6194,  0.6194,  0.6542,  0.6610,  0.6678,  0.6745,\n",
       "            0.6745,  0.6812,  0.6944,  0.7139,  0.7203,  0.7393,  0.7456,  0.7456,\n",
       "            0.7883,  0.8002,  0.8061,  0.8120,  0.8236,  0.8293,  0.8350,  0.8407,\n",
       "            0.8407,  0.8520,  0.8576,  0.8632,  0.8687,  0.8797,  0.8851,  0.8905,\n",
       "            0.8959,  0.9066,  0.9171,  0.9224,  0.9328, -0.1688, -0.1084, -0.0938,\n",
       "           -0.0795, -0.0240,  0.0025,  0.0156,  0.0285,  0.0537,  0.0661,  0.0783,\n",
       "            0.0904,  0.1142,  0.1258,  0.1373,  0.1487,  0.1600,  0.1712,  0.2038,\n",
       "            0.2145,  0.2561,  0.3153,  0.3249,  0.3437,  0.3530,  0.3713,  0.3713,\n",
       "            0.3803,  0.3892,  0.3981,  0.4156, -0.0513, -0.0240, -0.0106,  0.0284,\n",
       "            0.0536,  0.0536,  0.0660,  0.1372,  0.1372,  0.1486,  0.1598,  0.1710,\n",
       "            0.2248,  0.2353,  0.2558,  0.2659,  0.2759,  0.2858,  0.2858,  0.2956,\n",
       "            0.3340,  0.3526,  0.3618,  0.3709,  0.3709,  0.3799,  0.3977,  0.4064,\n",
       "           -0.4751, -0.4541, -0.4336, -0.4136, -0.3557, -0.3009, -0.2834, -0.2834,\n",
       "           -0.2661, -0.1998, -0.1998, -0.1682, -0.1377, -0.1227, -0.1227, -0.1080,\n",
       "           -0.0935, -0.0792, -0.0512, -0.0375, -0.0240, -0.0240,  0.0155,  0.0284,\n",
       "            0.0284,  0.0410,  0.0535,  0.0901,  0.0901,  0.1482,  0.1595,  0.2753,\n",
       "            0.2852,  0.3425,  0.3790,  0.4142,  0.4481,  0.4565,  0.4890,  0.5128,\n",
       "            0.5283,  0.5360,  0.5360,  0.5587,  0.5735,  0.5954,  0.5954,  0.6238,\n",
       "            0.6307,  0.6377,  0.6377,  0.6445,  0.6514,  0.6581,  0.6649,  0.6782,\n",
       "            0.6848,  0.6914,  0.6914,  0.7172,  0.7235,  0.7299,  0.7486,  0.7486,\n",
       "            0.7547,  0.7608,  0.7669,  0.7789,  0.7789,  0.7908,  0.7967,  0.8084,\n",
       "            0.8142,  0.8200,  0.8200,  0.8539,  0.8649,  0.8704,  0.8813,  0.8866,\n",
       "            0.8920,  0.8973,  0.9026,  0.9026,  0.9079,  0.9131,  0.9184,  0.9338,\n",
       "            0.9491,  0.9541,  0.9591,  0.9740,  0.9838,  0.9838, -0.6326, -0.5843,\n",
       "           -0.5843, -0.5610, -0.5383, -0.4943, -0.4522, -0.4118, -0.3922, -0.3356,\n",
       "           -0.3175, -0.2996, -0.0931, -0.0788, -0.0510, -0.0373,  0.0155,  0.0282,\n",
       "            0.0408,  0.0533,  0.0656,  0.0656,  0.0777,  0.0897,  0.1363,  0.1363,\n",
       "            0.1476,  0.1588,  0.1698,  0.1808,  0.2023,  0.2439,  0.2741,  0.2839,\n",
       "            0.2839,  0.3033,  0.3317,  0.3410,  0.3862,  0.4037,  0.4209,  0.4545,\n",
       "            0.4627,  0.4708,  0.5028,  0.5106,  0.5337, -0.5353, -0.5132, -0.4916,\n",
       "           -0.4497, -0.4294, -0.4095, -0.2980, -0.2806, -0.2301, -0.2138, -0.1821,\n",
       "           -0.1666, -0.1666, -0.1069, -0.0926, -0.0784, -0.0507, -0.0371, -0.0237,\n",
       "            0.0652,  0.0773,  0.1127,  0.1242,  0.1468,  0.1468,  0.1579,  0.1798,\n",
       "            0.1905,  0.2012,  0.2117,  0.2221,  0.2324,  0.2324,  0.2426,  0.2527,\n",
       "            0.2921,  0.3299,  0.3392,  0.3483,  0.3574,  0.3753,  0.3841,  0.4355,\n",
       "            0.4355,  0.4438,  0.4520,  0.4602,  0.5078,  0.5155,  0.5308,  0.5532,\n",
       "            0.5532,  0.5606,  0.5606, -0.4478, -0.3693, -0.3144, -0.2967, -0.2967,\n",
       "           -0.2456, -0.2291, -0.2129, -0.1970, -0.1813, -0.1659, -0.1659, -0.1507,\n",
       "           -0.1210, -0.1065, -0.0781, -0.0781, -0.0505, -0.0370, -0.0370, -0.0236,\n",
       "           -0.0236,  0.0025,  0.0025,  0.0280,  0.0770,  0.0888,  0.1006,  0.1122,\n",
       "            0.1236,  0.1236,  0.1350,  0.1461,  0.2003,  0.2314,  0.2516,  0.2714,\n",
       "            0.2812,  0.2908,  0.3004,  0.3098,  0.3192,  0.3377,  0.3648,  0.3737,\n",
       "            0.3998,  0.4418,  0.4500,  0.4662,  0.4742,  0.4742,  0.4822,  0.4900,\n",
       "            0.4978,  0.5133,  0.5209,  0.5285,  0.5434,  0.5434,  0.5508,  0.5654,\n",
       "            0.5727,  0.5799,  0.5870,  0.5870,  0.6011,  0.6081,  0.6150,  0.6219,\n",
       "            0.6355,  0.6422,  0.6422,  0.6555,  0.6817,  0.6881,  0.6945,  0.6945,\n",
       "            0.7071,  0.7071,  0.7134,  0.7196,  0.7380,  0.7441,  0.7501,  0.7621,\n",
       "            0.7680,  0.7680,  0.7797,  0.8141,  0.8141,  0.8197,  0.8253, -0.5937,\n",
       "           -0.5477, -0.5038, -0.4826, -0.4618, -0.4415, -0.4020, -0.3829, -0.3457,\n",
       "           -0.3277, -0.3099, -0.3099, -0.2754, -0.2586, -0.2259, -0.2259, -0.1788,\n",
       "           -0.1635, -0.1486, -0.1338, -0.1193, -0.0909, -0.0770, -0.0633, -0.0498,\n",
       "           -0.0364, -0.0364, -0.0103,  0.0151,  0.0640,  0.0992,  0.1219,  0.1441,\n",
       "            0.1658,  0.2382,  0.2579,  0.2676,  0.3055,  0.3239,  0.3329,  0.3597,\n",
       "            0.3684,  0.3942,  0.4026,  0.4110,  0.4193,  0.4676,  0.4754,  0.4985,\n",
       "            0.5061,  0.5210,  0.5284,  0.5503,  0.5575,  0.5646,  0.5717,  0.5717,\n",
       "            0.5787,  0.5857,  0.5857,  0.6131,  0.6265,  0.6332,  0.6463,  0.6593,\n",
       "            0.7336,  0.7396,  0.7455,  0.7514,  0.7572,  0.7630,  0.7630,  0.7915,\n",
       "            0.7971,  0.8027,  0.8137,  0.8408,  0.8514,  0.8566,  0.8566,  0.8671,\n",
       "            0.8671,  0.8723,  0.8774,  0.8825,  0.9028,  0.9127,  0.9177,  0.9177,\n",
       "            0.9468,  0.9563,  0.9657,  0.9751, -0.5854, -0.4553, -0.3590, -0.3409,\n",
       "           -0.3409, -0.3231, -0.2884, -0.2884, -0.2227, -0.2070, -0.2070, -0.1762,\n",
       "           -0.1612, -0.1612, -0.1465, -0.1176, -0.1176, -0.1035, -0.0759, -0.0491,\n",
       "           -0.0491, -0.0102,  0.0149,  0.0272,  0.0748,  0.0864,  0.0978,  0.1202,\n",
       "            0.1312,  0.1421,  0.1528,  0.1635,  0.1635,  0.1740,  0.1947,  0.2049,\n",
       "            0.2150,  0.2348,  0.2733,  0.2920,  0.3103,  0.3283,  0.3371,  0.3459,\n",
       "            0.3546,  0.3632,  0.3718,  0.3802,  0.3886,  0.4052,  0.4052,  0.4215,\n",
       "            0.4215,  0.4295,  0.4454,  0.4532,  0.4839,  0.4915,  0.5064,  0.5137,\n",
       "            0.5210,  0.5210,  0.5283,  0.5426, -0.6067, -0.6067, -0.5390, -0.5176,\n",
       "           -0.4171, -0.3799, -0.3799, -0.2764, -0.1835, -0.0727, -0.0470, -0.0344,\n",
       "           -0.0098, -0.0098,  0.0023,  0.0377,  0.0605,  0.0937,  0.1152,  0.1567,\n",
       "            0.1668,  0.1964,  0.2060,  0.2250,  0.2344,  0.2619,  0.2709,  0.2798,\n",
       "            0.2798,  0.3060,  0.3146,  0.3398,  0.3725,  0.4782,  0.4923,  0.5063,\n",
       "            0.5200,  0.5335,  0.5469,  0.5600,  0.6107,  0.6230,  0.6230,  0.7155,\n",
       "            0.7479,  0.7689,  0.7792,  0.7894,  0.8291,  0.8388,  0.8483,  0.8483,\n",
       "            0.8578,  0.8671,  0.8856,  0.8947,  0.9037,  0.9301,  0.9301]),\n",
       "   tensor([-0.0499,  0.0163,  0.0416,  0.0663,  0.0903,  0.1368,  0.1593,  0.1703,\n",
       "            0.1703,  0.2238,  0.2444,  0.2545,  0.2645,  0.3133,  0.3228,  0.3321,\n",
       "            0.3414,  0.3506,  0.3506,  0.3688,  0.3778,  0.3866,  0.3954,  0.4042,\n",
       "            0.4214,  0.4383,  0.4467,  0.4794,  0.4954,  0.0163,  0.0290,  0.0290,\n",
       "            0.0415,  0.0662,  0.1020,  0.1137,  0.1700,  0.1917,  0.2024,  0.2024,\n",
       "            0.2129,  0.2234,  0.2541,  0.2641,  0.3032,  0.3128,  0.3501,  0.3682,\n",
       "            0.3771,  0.3860,  0.4460,  0.4625, -0.3874, -0.3499, -0.3137, -0.3137,\n",
       "           -0.2787, -0.2617, -0.2123, -0.1964, -0.1501, -0.1351, -0.0915, -0.0634,\n",
       "           -0.0362, -0.0228,  0.0034,  0.0162,  0.0162,  0.0415,  0.0415,  0.0538,\n",
       "            0.0781,  0.1018,  0.1018,  0.1135,  0.1250,  0.1363,  0.1476,  0.2126,\n",
       "            0.2230,  0.2333,  0.2435,  0.2735,  0.2833,  0.3122,  0.3122,  0.3216,\n",
       "            0.3494,  0.3675,  0.4284,  0.4284,  0.4368,  0.4778,  0.4778,  0.4937,\n",
       "            0.5094,  0.5094,  0.5171,  0.5400,  0.5550,  0.5771,  0.5915,  0.5986,\n",
       "            0.6127,  0.6267,  0.6336,  0.6336,  0.6539,  0.6673,  0.6870,  0.7000,\n",
       "            0.7064,  0.7064,  0.7190,  0.7316,  0.7378,  0.7622,  0.7742,  0.7860,\n",
       "            0.7919,  0.7919,  0.7978,  0.8036,  0.8093,  0.8151,  0.8208,  0.8321,\n",
       "            0.8377,  0.8433,  0.8598,  0.8815,  0.8815,  0.9027,  0.9079,  0.9234,\n",
       "            0.9234,  0.9387,  0.9734,  0.0414,  0.0899,  0.0899,  0.1133,  0.1585,\n",
       "            0.1804,  0.2123,  0.2227,  0.2227,  0.2432,  0.2829,  0.3212,  0.3305,\n",
       "            0.3398,  0.3670,  0.4108,  0.4108, -0.0096,  0.0162,  0.0162,  0.0658,\n",
       "            0.0898,  0.1131,  0.1359,  0.1582,  0.2326,  0.2428,  0.2428,  0.2529,\n",
       "            0.2628,  0.2922,  0.3207,  0.3207,  0.3392,  0.3664,  0.3841,  0.3929,\n",
       "            0.4016,  0.4102, -0.4041, -0.3295, -0.2600, -0.2600, -0.2434, -0.2110,\n",
       "           -0.2110, -0.1951, -0.0909, -0.0769, -0.0630, -0.0630, -0.0227, -0.0227,\n",
       "           -0.0096, -0.0096,  0.0034,  0.0161,  0.0535,  0.0776,  0.0894,  0.0894,\n",
       "            0.1242,  0.1466,  0.2007,  0.2112,  0.2419,  0.2520,  0.2520,  0.2718,\n",
       "            0.2912,  0.3007,  0.3102,  0.3102,  0.3196,  0.3380,  0.3380,  0.3472,\n",
       "            0.3652,  0.4002,  0.4087,  0.4340,  0.4586,  0.4586,  0.4667,  0.4667,\n",
       "            0.4984,  0.5138,  0.5214,  0.5514,  0.5588,  0.5661,  0.5661,  0.5877,\n",
       "            0.5877,  0.5948,  0.6018,  0.6157,  0.6226,  0.6363,  0.6761,  0.6826,\n",
       "            0.6890,  0.7081,  0.7330,  0.7452,  0.7573,  0.7984,  0.7984,  0.8098,\n",
       "            0.8323,  0.8378,  0.8378,  0.8543,  0.8651,  0.8651, -0.4009, -0.3631,\n",
       "           -0.3268, -0.3268, -0.2579, -0.1331, -0.1186, -0.0902, -0.0625, -0.0490,\n",
       "            0.0160,  0.0285,  0.0285,  0.0409,  0.0530,  0.0530,  0.0651,  0.0887,\n",
       "            0.1003,  0.1118,  0.1232,  0.1344,  0.1344,  0.1455,  0.1564,  0.1673,\n",
       "            0.1886,  0.2198,  0.2696,  0.2696,  0.2792,  0.2888,  0.3077,  0.3170,\n",
       "            0.3353,  0.3444,  0.3533,  0.3622,  0.3710,  0.3884,  0.3969,  0.3969,\n",
       "            0.4054,  0.4305,  0.4387,  0.4709,  0.4866,  0.5097,  0.5097,  0.5172,\n",
       "            0.5172,  0.5248,  0.5396,  0.5470,  0.5543,  0.6039,  0.6176,  0.6176,\n",
       "            0.6244,  0.6244,  0.6311,  0.6378,  0.6771,  0.6835,  0.6898,  0.6898,\n",
       "            0.7086,  0.7271,  0.7332,  0.7689,  0.7747,  0.7805,  0.7862,  0.7976,\n",
       "            0.8033,  0.8089,  0.8089,  0.8145,  0.8311,  0.8311,  0.8366,  0.8420,\n",
       "            0.8528,  0.8581,  0.8581,  0.8634,  0.8687,  0.8740,  0.8740,  0.8844,\n",
       "            0.8999,  0.9251,  0.9399,  0.9497,  0.9689,  0.9784,  0.9784,  0.9878,\n",
       "            0.9878, -0.3983, -0.3983, -0.3794, -0.3608, -0.3247, -0.3071, -0.2899,\n",
       "           -0.2729, -0.2079, -0.2079, -0.1618, -0.1469, -0.1323, -0.1178, -0.1036,\n",
       "           -0.0621, -0.0487, -0.0354, -0.0223,  0.0159,  0.0765,  0.1111,  0.1111,\n",
       "            0.1335,  0.1335,  0.1445,  0.1554,  0.1554,  0.1662,  0.1662,  0.1768,\n",
       "            0.1874,  0.1978,  0.2284,  0.2581,  0.2869,  0.2964,  0.3241,  0.3332,\n",
       "            0.3686,  0.3859,  0.3944,  0.4195,  0.4277,  0.4359,  0.4599,  0.4678,\n",
       "            0.4678,  0.4757,  0.4911,  0.4988,  0.5064,  0.5064,  0.5361,  0.5361,\n",
       "            0.5650,  0.5650,  0.5721,  0.6136,  0.6271,  0.6337,  0.6403,  0.6403,\n",
       "           -0.3951, -0.3763, -0.3221, -0.2875, -0.2542, -0.2379, -0.2219, -0.2062,\n",
       "           -0.1755, -0.1312, -0.1169, -0.0751, -0.0616, -0.0221, -0.0221, -0.0093,\n",
       "           -0.0093,  0.0033,  0.0281,  0.0403,  0.0989,  0.0989,  0.1214,  0.1542,\n",
       "            0.1754,  0.1859,  0.1962,  0.2064,  0.2560,  0.2657,  0.2752,  0.2940,\n",
       "            0.3032,  0.3215,  0.3482,  0.3570,  0.3656,  0.3742,  0.3828,  0.3912,\n",
       "            0.3996,  0.3996,  0.4079,  0.4161,  0.4243,  0.4324,  0.4404,  0.4404,\n",
       "            0.4718,  0.4795,  0.5098,  0.5098,  0.5172,  0.5245,  0.5391,  0.5534,\n",
       "            0.5605,  0.5883,  0.6019,  0.6154,  0.6220,  0.6286,  0.6352,  0.6352,\n",
       "            0.6417,  0.6417,  0.6610,  0.6673,  0.6799,  0.6984,  0.7106,  0.7166,\n",
       "            0.7226,  0.7226,  0.7462,  0.7520,  0.7577,  0.7635,  0.7749,  0.7805,\n",
       "            0.7805,  0.7861,  0.7917,  0.7972,  0.8027,  0.8137,  0.8191,  0.8191,\n",
       "            0.8245,  0.8404,  0.8457,  0.8613,  0.8767,  0.8818,  0.8869, -0.3925,\n",
       "           -0.3556, -0.3376, -0.2857, -0.2690, -0.2690, -0.2525, -0.2364, -0.2364,\n",
       "           -0.2205, -0.1895, -0.1744, -0.1595, -0.0747, -0.0349,  0.0033,  0.0157,\n",
       "            0.0400,  0.0754,  0.0983,  0.1424,  0.1532,  0.1638,  0.1949,  0.2350,\n",
       "            0.2350,  0.2447,  0.2544,  0.2921,  0.3104,  0.3283,  0.3283,  0.3460,\n",
       "            0.3547,  0.3718,  0.3718,  0.3970,  0.3970,  0.4215,  0.4296,  0.4688,\n",
       "            0.4764,  0.4840,  0.4916,  0.4991,  0.5065,  0.5211,  0.5284,  0.5356,\n",
       "            0.5356,  0.5427,  0.5498,  0.5498,  0.5639,  0.5708,  0.5845,  0.5980,\n",
       "            0.6114,  0.6180,  0.6246,  0.6503,  0.6567,  0.6630,  0.6755,  0.6816,\n",
       "            0.6878,  0.6878,  0.6999,  0.7060,  0.7179,  0.7355,  0.7471,  0.7642,\n",
       "            0.7975,  0.8403,  0.8455,  0.8660,  0.8711,  0.8761,  0.9107,  0.9155,\n",
       "            0.9204,  0.9299,  0.9299,  0.9394,  0.9394,  0.9488, -0.3661, -0.3482,\n",
       "           -0.3306, -0.3306, -0.3133, -0.2964, -0.2797, -0.2634, -0.2315, -0.2159,\n",
       "           -0.2159, -0.2006, -0.1708, -0.1137, -0.0865, -0.0731, -0.0731, -0.0091,\n",
       "            0.0032,  0.0154,  0.0392,  0.0624,  0.0738,  0.0738,  0.1288,  0.1604,\n",
       "            0.1707,  0.2204,  0.2301,  0.2396,  0.2491,  0.2491,  0.2585,  0.2677,\n",
       "            0.2677,  0.2769,  0.2950,  0.3128,  0.3128,  0.3215,  0.3388,  0.3724,\n",
       "            0.3887,  0.3968,  0.4048,  0.4128,  0.4206,  0.4284,  0.4439,  0.4515,\n",
       "            0.4590,  0.4665,  0.4740,  0.4959,  0.5174,  0.5244,  0.5521,  0.5790,\n",
       "            0.5856,  0.5922,  0.6051,  0.6115,  0.6179,  0.6243,  0.6492,  0.6614,\n",
       "            0.6735,  0.6794,  0.7145,  0.7145,  0.7259,  0.7316,  0.7428,  0.7483,\n",
       "            0.7538,  0.7593, -0.5335, -0.4917, -0.4714, -0.4516, -0.4132, -0.3946,\n",
       "           -0.3409, -0.2902, -0.1964, -0.1817, -0.1672, -0.1529, -0.1389, -0.1114,\n",
       "           -0.1114, -0.0847, -0.0587, -0.0211,  0.0150,  0.0268,  0.0384,  0.0611,\n",
       "            0.0723,  0.0723,  0.0833,  0.0942,  0.1050,  0.1366,  0.1469,  0.1570,\n",
       "            0.1869,  0.1967,  0.2347,  0.2711,  0.2976,  0.3148,  0.3317,  0.3400,\n",
       "            0.3400,  0.3565,  0.3727,  0.3806,  0.3885,  0.3964,  0.4042,  0.4119,\n",
       "            0.4195,  0.4195,  0.4421,  0.4495,  0.4568,  0.4568,  0.4641,  0.4641,\n",
       "            0.4713,  0.4785,  0.4927,  0.5066,  0.5135,  0.5272,  0.5604,  0.5734,\n",
       "            0.5798,  0.5925,  0.5988,  0.6174,  0.6296,  0.6536,  0.6594,  0.6711,\n",
       "            0.6769,  0.6883,  0.6940,  0.6996,  0.7108,  0.7163,  0.7381,  0.7435,\n",
       "            0.7541,  0.7699,  0.7751,  0.7751,  0.7854,  0.7854,  0.8106,  0.8106,\n",
       "            0.8205,  0.8400,  0.8448,  0.8496,  0.8544,  0.8591,  0.8732,  0.8778,\n",
       "            0.9007,  0.9186,  0.9274,  0.9274, -0.4334, -0.3973, -0.3973, -0.3628,\n",
       "           -0.3628, -0.1806, -0.0778, -0.0778,  0.0138,  0.0138,  0.0562,  0.0562,\n",
       "            0.1160,  0.1350,  0.1536,  0.1718,  0.1897,  0.2071,  0.2071,  0.2410,\n",
       "            0.2410,  0.2894,  0.3049,  0.3352,  0.3499,  0.3499,  0.3644,  0.3786,\n",
       "            0.3926,  0.4064,  0.4199,  0.4593,  0.4970,  0.5092,  0.5330,  0.5562,\n",
       "            0.5562,  0.5788,  0.5899,  0.6222,  0.6431,  0.6736,  0.6933,  0.7030,\n",
       "            0.7220,  0.7313,  0.7766,  0.7854,  0.8027,  0.8196,  0.8362]),\n",
       "   tensor([-0.3727, -0.3727, -0.3648,  ...,  0.3724,  0.3791,  0.3791]),\n",
       "   tensor([-0.0258, -0.0120,  0.0016,  ...,  0.8200,  0.8276,  0.8276]),\n",
       "   tensor([-0.2319, -0.2156, -0.2156,  ...,  0.2196,  0.2995,  0.2995])],\n",
       "  'Time to Maturity': [tensor([0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
       "           0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "           0.0714, 0.0714, 0.0714, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.1587, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698, 0.2698,\n",
       "           0.2698, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087, 0.4087,\n",
       "           0.4087, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198, 0.5198,\n",
       "           0.5198, 0.5198, 0.5198, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 0.8810, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 1.2421, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254]),\n",
       "   tensor([0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159,\n",
       "           0.0159, 0.0159, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437,\n",
       "           0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437,\n",
       "           0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0437, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754, 0.0754,\n",
       "           0.0754, 0.0754, 0.0754, 0.0754, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992, 0.0992,\n",
       "           0.0992, 0.0992, 0.0992, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270,\n",
       "           0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270,\n",
       "           0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1270, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865, 0.1865,\n",
       "           0.1865, 0.1865, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254, 0.3254,\n",
       "           0.3254, 0.3254, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365, 0.4365,\n",
       "           0.4365, 0.4365, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754,\n",
       "           0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.5754, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.6865, 0.6865, 0.6865, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476,\n",
       "           1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532]),\n",
       "   tensor([0.0159, 0.0159, 0.0159,  ..., 2.4087, 2.4087, 2.4087]),\n",
       "   tensor([0.0040, 0.0040, 0.0040,  ..., 2.7024, 2.7024, 2.7024]),\n",
       "   tensor([0.0079, 0.0079, 0.0079,  ..., 2.8175, 2.8175, 2.8175])],\n",
       "  'Implied Volatility': [tensor([0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.4187, 0.3716,\n",
       "           0.3716, 0.3617, 0.3363, 0.3212, 0.3147, 0.3133, 0.3133, 0.3716, 0.3133,\n",
       "           0.3716, 0.3133, 0.3716, 0.3133, 0.3133, 0.3133, 0.3716, 0.3133, 0.3716,\n",
       "           0.3716, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473,\n",
       "           0.3473, 0.3258, 0.3258, 0.3258, 0.3258, 0.3473, 0.3473, 0.3258, 0.3258,\n",
       "           0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3473, 0.3473,\n",
       "           0.3258, 0.3258, 0.3180, 0.3207, 0.3086, 0.2900, 0.2851, 0.2781, 0.2698,\n",
       "           0.2607, 0.2637, 0.2668, 0.2688, 0.2859, 0.3138, 0.2561, 0.2561, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295,\n",
       "           0.3295, 0.3295, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295,\n",
       "           0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.3295, 0.2561, 0.3295, 0.2561,\n",
       "           0.2561, 0.3295, 0.2561, 0.3295, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.3295, 0.3381,\n",
       "           0.3381, 0.3381, 0.3381, 0.3265, 0.3632, 0.3381, 0.3216, 0.3165, 0.3037,\n",
       "           0.3004, 0.2912, 0.2812, 0.2803, 0.2746, 0.2723, 0.2720, 0.2671, 0.2695,\n",
       "           0.2617, 0.2847, 0.2702, 0.2702, 0.3356, 0.2702, 0.3356, 0.2702, 0.3356,\n",
       "           0.2702, 0.2702, 0.3356, 0.3564, 0.3187, 0.3187, 0.3002, 0.2968, 0.2920,\n",
       "           0.2829, 0.2660, 0.2642, 0.2607, 0.2606, 0.2599, 0.2623, 0.2655, 0.2648,\n",
       "           0.2754, 0.2797, 0.2871, 0.2661, 0.2920, 0.2661, 0.3215, 0.2661, 0.3215,\n",
       "           0.2661, 0.3215, 0.3215, 0.2661, 0.3369, 0.3510, 0.3510, 0.3369, 0.3369,\n",
       "           0.3369, 0.3510, 0.3369, 0.3369, 0.3510, 0.3369, 0.3510, 0.3510, 0.3510,\n",
       "           0.3369, 0.3369, 0.3369, 0.3369, 0.3295, 0.3397, 0.3226, 0.3172, 0.3031,\n",
       "           0.2975, 0.2933, 0.2938, 0.2866, 0.2749, 0.2728, 0.2622, 0.2596, 0.2658,\n",
       "           0.2692, 0.2912, 0.3146, 0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.3302, 0.3302, 0.3302,\n",
       "           0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302,\n",
       "           0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.2695, 0.3302, 0.3302, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.3302, 0.3302, 0.2695, 0.3695,\n",
       "           0.5483, 0.3695, 0.3695, 0.5483, 0.5483, 0.5483, 0.5483, 0.5483, 0.3695,\n",
       "           0.5483, 0.3695, 0.3469, 0.4183, 0.3310, 0.3267, 0.3105, 0.3345, 0.3049,\n",
       "           0.3031, 0.3168, 0.3001, 0.2975, 0.3078, 0.2949, 0.2905, 0.2926, 0.2908,\n",
       "           0.2887, 0.2866, 0.2852, 0.2911, 0.2942, 0.2847, 0.2983, 0.2876, 0.2911,\n",
       "           0.3135, 0.3053, 0.3091, 0.3461, 0.3318, 0.3461, 0.3386, 0.3461, 0.3415,\n",
       "           0.3461, 0.3459, 0.6848, 0.3459, 0.6848, 0.6642, 0.3459, 0.5333, 0.5164,\n",
       "           0.4740, 0.4519, 0.4324, 0.4189, 0.3381, 0.3202, 0.3164, 0.3511, 0.3059,\n",
       "           0.3018, 0.2989, 0.2883, 0.2808, 0.2774, 0.2753, 0.2727, 0.2728, 0.2728,\n",
       "           0.2691, 0.2710, 0.2698, 0.2669, 0.2671, 0.2664, 0.2710, 0.2712, 0.2666,\n",
       "           0.2672, 0.2711, 0.2715, 0.2728, 0.2745, 0.2770, 0.2782, 0.2864, 0.3104,\n",
       "           0.3104, 0.2921, 0.3104, 0.3076, 0.3133, 0.3104, 0.3256, 0.3104, 0.3273,\n",
       "           0.3104, 0.3472, 0.3472, 0.3472, 0.3822, 0.3472, 0.3472, 0.3822, 0.3422,\n",
       "           0.3822, 0.3320, 0.3775, 0.3280, 0.3672, 0.3508, 0.3103, 0.3265, 0.3036,\n",
       "           0.2970, 0.3053, 0.2938, 0.3040, 0.2912, 0.2958, 0.2864, 0.2824, 0.2764,\n",
       "           0.2747, 0.2725, 0.2711, 0.2698, 0.2707, 0.2691, 0.2686, 0.2639, 0.2656,\n",
       "           0.2663, 0.2630, 0.2670, 0.2628, 0.2658, 0.2692, 0.2642, 0.2661, 0.2684,\n",
       "           0.2775, 0.2869, 0.2849, 0.2849, 0.2853, 0.2869, 0.2849, 0.2849, 0.2919,\n",
       "           0.2849, 0.3020, 0.3043, 0.2849, 0.3108, 0.2849, 0.3125, 0.2849, 0.3207,\n",
       "           0.2849, 0.3229, 0.2849, 0.3229, 0.3229, 0.3124, 0.2849, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.6576, 0.6189, 0.5936, 0.3469, 0.3469, 0.5412,\n",
       "           0.3469, 0.3469, 0.3469, 0.4520, 0.4388, 0.3434, 0.4219, 0.3313, 0.3866,\n",
       "           0.3244, 0.3141, 0.3109, 0.3080, 0.3051, 0.3263, 0.3153, 0.2955, 0.2940,\n",
       "           0.3014, 0.2983, 0.2901, 0.2870, 0.2879, 0.2810, 0.2771, 0.2740, 0.2726,\n",
       "           0.2743, 0.2690, 0.2725, 0.2678, 0.2671, 0.2746, 0.2683, 0.2779, 0.2691,\n",
       "           0.2702, 0.2804, 0.2854, 0.2824, 0.2965, 0.2942, 0.2804, 0.3037, 0.2857,\n",
       "           0.3085, 0.3089, 0.2902, 0.2918, 0.2932, 0.3089, 0.3089, 0.2964, 0.3089,\n",
       "           0.3042, 0.3057, 0.3089, 0.3089, 0.3116, 0.3116, 0.3089, 0.3116, 0.3089,\n",
       "           0.3116, 0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3116, 0.3116,\n",
       "           0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116,\n",
       "           0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3439, 0.3439, 0.4487,\n",
       "           0.4355, 0.3331, 0.4234, 0.4020, 0.3242, 0.3154, 0.3551, 0.3130, 0.3100,\n",
       "           0.3391, 0.3078, 0.3058, 0.3209, 0.3031, 0.3157, 0.2985, 0.3037, 0.2965,\n",
       "           0.2930, 0.2948, 0.2915, 0.2874, 0.2867, 0.2854, 0.2849, 0.2848, 0.2818,\n",
       "           0.2839, 0.2803, 0.2834, 0.2811, 0.2824, 0.2822, 0.2819, 0.2779, 0.2814,\n",
       "           0.2811, 0.2763, 0.2819, 0.2824, 0.2767, 0.2829, 0.2817, 0.2824, 0.2765,\n",
       "           0.2772, 0.2783, 0.2858, 0.2779, 0.2890, 0.2871, 0.2882, 0.2902, 0.2938,\n",
       "           0.2975, 0.3004, 0.2849, 0.2853, 0.2980, 0.3009, 0.2999, 0.5636, 0.3514,\n",
       "           0.3463, 0.5072, 0.3388, 0.4173, 0.3351, 0.3262, 0.3204, 0.3153, 0.3135,\n",
       "           0.3134, 0.3101, 0.3126, 0.3115, 0.3105, 0.3050, 0.3026, 0.3019, 0.3061,\n",
       "           0.3052, 0.2983, 0.3050, 0.2977, 0.3053, 0.3043, 0.2967, 0.2968, 0.3043,\n",
       "           0.3043, 0.3040, 0.2955, 0.2960, 0.3071, 0.3091, 0.3101, 0.2967, 0.3110,\n",
       "           0.2989, 0.3118, 0.3167, 0.3020, 0.3136, 0.3194, 0.3105, 0.3119, 0.3129,\n",
       "           0.3137, 0.3182, 0.3192, 0.3202, 0.3194, 0.3194, 0.3232, 0.3194, 0.3194,\n",
       "           0.3284, 0.3320, 0.3194]),\n",
       "   tensor([0.4484, 0.5840, 0.5429, 0.4101, 0.4310, 0.3847, 0.3700, 0.3681, 0.3691,\n",
       "           0.4633, 0.4731, 0.4101, 0.4101, 0.4101, 0.4101, 0.4101, 0.4731, 0.4101,\n",
       "           0.4731, 0.4731, 0.4731, 0.4101, 0.4731, 0.4731, 0.4731, 0.4101, 0.4101,\n",
       "           0.4101, 0.4731, 0.6166, 0.5844, 0.3907, 0.5625, 0.3622, 0.3420, 0.3351,\n",
       "           0.3262, 0.3439, 0.3255, 0.3600, 0.3293, 0.3802, 0.3609, 0.3686, 0.4195,\n",
       "           0.4312, 0.3968, 0.4799, 0.3968, 0.4799, 0.4799, 0.3968, 0.7247, 0.7247,\n",
       "           0.7247, 0.3801, 0.7247, 0.7247, 0.3801, 0.3801, 0.7247, 0.7247, 0.7104,\n",
       "           0.6531, 0.5995, 0.3801, 0.3683, 0.4839, 0.3595, 0.4370, 0.3410, 0.3375,\n",
       "           0.3253, 0.3287, 0.3174, 0.3125, 0.3124, 0.3101, 0.3071, 0.2957, 0.2975,\n",
       "           0.3348, 0.3369, 0.3255, 0.3313, 0.3556, 0.4041, 0.4041, 0.4041, 0.4277,\n",
       "           0.4977, 0.4041, 0.5077, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041,\n",
       "           0.4041, 0.4041, 0.5174, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041,\n",
       "           0.5174, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041,\n",
       "           0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.5174, 0.5174, 0.4041, 0.4041,\n",
       "           0.5174, 0.5174, 0.4041, 0.5174, 0.4041, 0.5174, 0.4041, 0.5174, 0.5174,\n",
       "           0.5174, 0.4041, 0.5174, 0.4041, 0.3262, 0.3205, 0.3120, 0.3063, 0.3012,\n",
       "           0.2893, 0.2909, 0.2918, 0.3028, 0.3062, 0.3124, 0.3597, 0.3706, 0.3556,\n",
       "           0.3763, 0.4201, 0.3597, 0.4300, 0.4024, 0.3242, 0.3140, 0.3244, 0.3084,\n",
       "           0.3010, 0.2962, 0.2856, 0.2924, 0.3028, 0.2940, 0.2967, 0.3057, 0.3184,\n",
       "           0.2893, 0.3283, 0.3463, 0.2893, 0.2893, 0.2893, 0.2893, 0.5372, 0.3481,\n",
       "           0.3962, 0.3481, 0.3481, 0.3962, 0.3481, 0.3962, 0.3347, 0.3962, 0.3962,\n",
       "           0.3493, 0.3835, 0.3255, 0.3572, 0.3246, 0.3167, 0.3150, 0.3045, 0.2954,\n",
       "           0.2926, 0.2979, 0.2877, 0.2843, 0.2904, 0.2905, 0.2975, 0.2869, 0.2964,\n",
       "           0.3042, 0.2950, 0.2979, 0.2983, 0.2973, 0.3000, 0.3090, 0.3143, 0.3151,\n",
       "           0.3202, 0.3151, 0.3427, 0.3151, 0.3578, 0.3151, 0.3578, 0.3151, 0.3578,\n",
       "           0.3578, 0.3151, 0.3578, 0.3578, 0.3578, 0.3151, 0.3578, 0.3151, 0.3578,\n",
       "           0.3151, 0.3578, 0.3578, 0.3151, 0.3578, 0.3151, 0.3578, 0.3151, 0.3578,\n",
       "           0.3578, 0.3151, 0.3578, 0.3151, 0.3151, 0.3151, 0.3578, 0.3151, 0.3151,\n",
       "           0.3578, 0.3151, 0.3632, 0.3355, 0.3632, 0.3355, 0.3355, 0.3355, 0.3570,\n",
       "           0.3231, 0.3372, 0.3129, 0.3020, 0.2973, 0.2963, 0.2949, 0.2864, 0.2920,\n",
       "           0.2847, 0.2818, 0.2809, 0.2853, 0.2829, 0.2790, 0.2829, 0.2775, 0.2771,\n",
       "           0.2809, 0.2744, 0.2753, 0.2774, 0.2859, 0.2783, 0.2859, 0.2807, 0.2829,\n",
       "           0.2856, 0.2894, 0.2945, 0.2933, 0.2970, 0.3034, 0.3025, 0.3017, 0.3017,\n",
       "           0.3165, 0.3017, 0.3017, 0.3413, 0.3519, 0.3017, 0.3550, 0.3017, 0.3017,\n",
       "           0.3017, 0.3017, 0.3017, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3017,\n",
       "           0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3017, 0.3017, 0.3784, 0.3017,\n",
       "           0.3017, 0.3017, 0.3017, 0.3784, 0.3784, 0.3784, 0.3017, 0.3017, 0.3784,\n",
       "           0.3017, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3784, 0.3784, 0.3784,\n",
       "           0.3017, 0.3784, 0.3017, 0.3784, 0.3784, 0.3784, 0.3017, 0.3784, 0.3017,\n",
       "           0.3784, 0.3017, 0.4610, 0.3484, 0.4610, 0.3484, 0.3484, 0.3484, 0.3484,\n",
       "           0.3484, 0.4487, 0.3484, 0.3464, 0.3953, 0.3835, 0.3753, 0.3358, 0.3477,\n",
       "           0.3237, 0.3221, 0.3331, 0.3128, 0.3037, 0.2988, 0.3029, 0.2974, 0.3021,\n",
       "           0.3011, 0.2953, 0.3013, 0.2928, 0.3012, 0.2923, 0.3008, 0.3009, 0.3017,\n",
       "           0.2903, 0.2903, 0.3054, 0.2918, 0.3098, 0.3156, 0.3001, 0.3271, 0.3284,\n",
       "           0.3091, 0.3284, 0.3185, 0.3212, 0.3284, 0.3284, 0.3284, 0.3284, 0.3323,\n",
       "           0.3284, 0.3443, 0.3284, 0.3563, 0.3284, 0.3589, 0.3284, 0.3678, 0.3678,\n",
       "           0.3678, 0.3284, 0.3532, 0.4089, 0.3532, 0.3532, 0.3356, 0.4089, 0.3407,\n",
       "           0.3976, 0.3716, 0.3439, 0.3410, 0.3278, 0.3096, 0.3063, 0.3047, 0.3044,\n",
       "           0.3021, 0.3030, 0.2967, 0.2939, 0.2848, 0.2881, 0.2814, 0.2781, 0.2856,\n",
       "           0.2858, 0.2834, 0.2764, 0.2755, 0.2761, 0.2859, 0.2926, 0.2862, 0.2790,\n",
       "           0.2924, 0.2829, 0.3071, 0.2859, 0.2990, 0.3016, 0.2913, 0.3016, 0.3016,\n",
       "           0.3016, 0.2940, 0.3016, 0.2969, 0.3016, 0.3047, 0.3016, 0.3147, 0.3016,\n",
       "           0.3016, 0.3181, 0.3016, 0.3277, 0.3315, 0.3411, 0.3016, 0.3016, 0.3514,\n",
       "           0.3543, 0.3532, 0.3016, 0.3557, 0.3016, 0.3634, 0.3016, 0.3701, 0.3701,\n",
       "           0.3701, 0.3016, 0.3701, 0.3016, 0.3016, 0.3016, 0.3701, 0.3016, 0.3016,\n",
       "           0.3701, 0.3016, 0.3701, 0.3701, 0.3016, 0.3016, 0.3701, 0.3701, 0.3016,\n",
       "           0.3016, 0.3701, 0.3701, 0.3701, 0.3701, 0.3016, 0.3701, 0.3370, 0.3370,\n",
       "           0.3370, 0.3370, 0.3792, 0.3370, 0.3370, 0.3792, 0.3396, 0.3317, 0.3646,\n",
       "           0.3534, 0.3232, 0.3050, 0.2994, 0.2949, 0.2921, 0.2858, 0.2819, 0.2792,\n",
       "           0.2747, 0.2743, 0.2737, 0.2718, 0.2713, 0.2806, 0.2783, 0.2782, 0.2793,\n",
       "           0.2810, 0.2717, 0.2814, 0.2726, 0.2844, 0.2753, 0.2864, 0.2785, 0.2922,\n",
       "           0.2819, 0.2975, 0.2878, 0.3048, 0.2926, 0.3048, 0.3048, 0.2963, 0.3048,\n",
       "           0.3048, 0.3048, 0.3048, 0.3048, 0.3084, 0.3048, 0.3122, 0.3151, 0.3048,\n",
       "           0.3219, 0.3253, 0.3255, 0.3270, 0.3270, 0.3048, 0.3270, 0.3270, 0.3048,\n",
       "           0.3270, 0.3048, 0.3048, 0.3270, 0.3048, 0.3048, 0.3048, 0.3270, 0.3270,\n",
       "           0.3048, 0.3270, 0.3048, 0.3270, 0.3048, 0.3048, 0.3270, 0.3048, 0.3270,\n",
       "           0.3048, 0.3270, 0.3048, 0.3048, 0.4674, 0.4503, 0.4401, 0.3400, 0.4230,\n",
       "           0.3331, 0.3296, 0.3264, 0.3173, 0.3554, 0.3187, 0.3088, 0.3114, 0.3012,\n",
       "           0.2979, 0.3003, 0.2969, 0.2887, 0.2859, 0.2873, 0.2803, 0.2800, 0.2784,\n",
       "           0.2837, 0.2785, 0.2796, 0.2788, 0.2733, 0.2733, 0.2678, 0.2664, 0.2746,\n",
       "           0.2738, 0.2636, 0.2758, 0.2744, 0.2748, 0.2650, 0.2768, 0.2648, 0.2624,\n",
       "           0.2654, 0.2733, 0.2647, 0.2786, 0.2652, 0.2654, 0.2824, 0.2809, 0.2663,\n",
       "           0.2717, 0.2693, 0.2693, 0.2736, 0.2725, 0.2738, 0.2768, 0.2768, 0.2938,\n",
       "           0.2938, 0.2938, 0.2938, 0.2768, 0.2938, 0.2938, 0.2938, 0.2768, 0.2768,\n",
       "           0.2768, 0.2938, 0.2768, 0.2768, 0.2938, 0.2768, 0.2938, 0.2768, 0.3342,\n",
       "           0.3372, 0.3342, 0.3372, 0.3342, 0.3372, 0.3372, 0.3372, 0.3371, 0.3124,\n",
       "           0.3026, 0.3212, 0.3165, 0.3111, 0.3025, 0.3012, 0.2988, 0.2934, 0.2914,\n",
       "           0.2895, 0.2888, 0.2870, 0.2785, 0.2880, 0.2794, 0.2798, 0.2784, 0.2748,\n",
       "           0.2753, 0.2819, 0.2802, 0.2722, 0.2708, 0.2657, 0.2768, 0.2651, 0.2774,\n",
       "           0.2668, 0.2774, 0.2671, 0.2758, 0.2659, 0.2792, 0.2772, 0.2642, 0.2788,\n",
       "           0.2676, 0.2832, 0.2826, 0.2645, 0.2700, 0.2786, 0.2695, 0.2829, 0.2879,\n",
       "           0.2695, 0.2834, 0.2856, 0.2675, 0.2675, 0.2675, 0.2675, 0.2950, 0.2950,\n",
       "           0.2675, 0.2950, 0.2950, 0.2950, 0.2675, 0.2675, 0.2950, 0.2675, 0.2950,\n",
       "           0.2675, 0.2675, 0.2675, 0.2950, 0.2950, 0.2675, 0.2675, 0.2675, 0.2950,\n",
       "           0.2675, 0.2950, 0.2675, 0.2950, 0.2675, 0.2675, 0.2950, 0.2675, 0.2675,\n",
       "           0.2675, 0.2675, 0.2950, 0.2950, 0.2675, 0.2675, 0.2950, 0.4397, 0.4170,\n",
       "           0.3202, 0.3932, 0.3227, 0.3240, 0.3066, 0.3105, 0.2992, 0.3058, 0.2980,\n",
       "           0.3065, 0.3008, 0.3022, 0.2925, 0.3030, 0.2924, 0.2913, 0.3004, 0.2901,\n",
       "           0.2999, 0.2988, 0.2979, 0.2880, 0.2891, 0.3005, 0.2996, 0.2913, 0.2899,\n",
       "           0.2996, 0.3002, 0.3008, 0.3030, 0.2911, 0.2888, 0.2875, 0.3041, 0.3029,\n",
       "           0.3021, 0.3065, 0.2897, 0.2883, 0.3090, 0.2939, 0.2950, 0.3084, 0.2916,\n",
       "           0.3084, 0.2916, 0.2916, 0.3084]),\n",
       "   tensor([0.3347, 0.3682, 0.3347,  ..., 0.2350, 0.2302, 0.2355]),\n",
       "   tensor([0.7914, 0.5199, 0.5199,  ..., 0.3147, 0.2958, 0.3147]),\n",
       "   tensor([0.4321, 0.2589, 0.4321,  ..., 0.2346, 0.2261, 0.2306])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([0.4582], requires_grad=True),\n",
       "   tensor([0.2204], requires_grad=True),\n",
       "   tensor([-0.8295], requires_grad=True),\n",
       "   tensor([-0.3801], requires_grad=True),\n",
       "   tensor([-0.5229], requires_grad=True)],\n",
       "  'Time to Maturity': [tensor([0.5198], requires_grad=True),\n",
       "   tensor([1.0476], requires_grad=True),\n",
       "   tensor([0.9643], requires_grad=True),\n",
       "   tensor([0.8968], requires_grad=True),\n",
       "   tensor([0.4008], requires_grad=True)],\n",
       "  'Implied Volatility': [tensor([0.2849]),\n",
       "   tensor([0.2683]),\n",
       "   tensor([0.2642]),\n",
       "   tensor([0.3741]),\n",
       "   tensor([0.4566])]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "class IVSurfaceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data, \n",
    "        mask_proportions, \n",
    "        random_state=0,\n",
    "        n_query_points=None\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.mask_proportions = mask_proportions\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        self.n_query_points = n_query_points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        surface_data = self.data[idx]\n",
    "        \n",
    "        # Extract the surface coordinates and volatilities\n",
    "        points_coordinates = np.stack([\n",
    "            surface_data['Surface']['Log Moneyness'], \n",
    "            surface_data['Surface']['Time to Maturity']\n",
    "        ], axis=1)\n",
    "        points_volatilities = surface_data['Surface']['Implied Volatility']\n",
    "\n",
    "        # Select a random mask proportion\n",
    "        proportion = self.rng.choice(self.mask_proportions)\n",
    "\n",
    "        # Perform clustering\n",
    "        n_clusters = int(np.ceil(1 / proportion))\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('kmeans', KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init='auto'))\n",
    "        ])\n",
    "        labels = pipeline.fit_predict(points_coordinates)\n",
    "        masked_indices = np.array([], dtype=int)\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_indices = np.where(labels == cluster)[0]\n",
    "            num_to_mask = int(np.ceil(len(cluster_indices) * proportion))\n",
    "            masked_indices = np.append(masked_indices, [self.rng.choice(cluster_indices, size=num_to_mask, replace=False)])\n",
    "        \n",
    "        unmasked_indices = np.setdiff1d(range(len(labels)), masked_indices)\n",
    "\n",
    "        # Calculate IV mean and std for unmasked points\n",
    "        iv_mean = np.mean(points_volatilities[unmasked_indices])\n",
    "        iv_std = np.std(points_volatilities[unmasked_indices])\n",
    "\n",
    "        # Define query indices based on n_query_points\n",
    "        if self.n_query_points is None:\n",
    "            query_indices = masked_indices\n",
    "        else:\n",
    "            query_indices = self.rng.choice(masked_indices, size=self.n_query_points, replace=False)\n",
    "            \n",
    "        data_item = {\n",
    "            'Datetime': surface_data['Datetime'],\n",
    "            'Symbol': surface_data['Symbol'],\n",
    "            'Mask Proportion': proportion,\n",
    "            'Market Features': {\n",
    "                'Market Return': torch.tensor(surface_data['Market Features']['Market Return'], dtype=torch.float32),\n",
    "                'Market Volatility': torch.tensor(surface_data['Market Features']['Market Volatility'], dtype=torch.float32),\n",
    "                'Treasury Rate': torch.tensor(surface_data['Market Features']['Treasury Rate'], dtype=torch.float32),\n",
    "                'IV Mean': torch.tensor(iv_mean, dtype=torch.float32),\n",
    "                'IV Std.': torch.tensor(iv_std, dtype=torch.float32),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[unmasked_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[unmasked_indices, 1], dtype=torch.float32),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[unmasked_indices], dtype=torch.float32)\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[query_indices, 0], dtype=torch.float32, requires_grad=True),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[query_indices, 1], dtype=torch.float32, requires_grad=True),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[query_indices], dtype=torch.float32)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return data_item\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batched_data = {\n",
    "            'Datetime': [item['Datetime'] for item in batch],\n",
    "            'Symbol': [item['Symbol'] for item in batch],\n",
    "            'Mask Proportion': [item['Mask Proportion'] for item in batch],\n",
    "            'Market Features': {\n",
    "                'Market Return': default_collate([item['Market Features']['Market Return'] for item in batch]),\n",
    "                'Market Volatility': default_collate([item['Market Features']['Market Volatility'] for item in batch]),\n",
    "                'Treasury Rate': default_collate([item['Market Features']['Treasury Rate'] for item in batch]),\n",
    "                'IV Mean': default_collate([item['Market Features']['IV Mean'] for item in batch]),\n",
    "                'IV Std.': default_collate([item['Market Features']['IV Std.'] for item in batch]),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': [item['Input Surface']['Log Moneyness'] for item in batch],\n",
    "                'Time to Maturity': [item['Input Surface']['Time to Maturity'] for item in batch],\n",
    "                'Implied Volatility': [item['Input Surface']['Implied Volatility'] for item in batch],\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': [item['Query Points']['Log Moneyness'].requires_grad_(True) for item in batch],\n",
    "                'Time to Maturity': [item['Query Points']['Time to Maturity'].requires_grad_(True) for item in batch],\n",
    "                'Implied Volatility': [item['Query Points']['Implied Volatility'] for item in batch],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "# Assuming surfaces is the output from the implied_volatility_surfaces function\n",
    "mask_proportions = HYPERPARAMETERS['Input Preprocessing']['Mask Proportions']  \n",
    "n_query_points = HYPERPARAMETERS['Input Preprocessing']['Number of Query Points']  \n",
    "dataset = IVSurfaceDataset(surfaces, mask_proportions, RANDOM_STATE, n_query_points)\n",
    "data_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=HYPERPARAMETERS['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Fetch one batch from the DataLoader\n",
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00'),\n",
       "  Timestamp('2013-03-07 00:00:00'),\n",
       "  Timestamp('2013-02-06 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL', 'AAPL', 'GOOGL'],\n",
       " 'Mask Proportion': [0.7, 0.7, 0.3, 0.3, 0.1],\n",
       " 'Market Features': {'Market Return': tensor([-0.0703, -0.5136, -0.1766,  0.5667,  0.1937],\n",
       "         grad_fn=<SqueezeBackward1>),\n",
       "  'Market Volatility': tensor([ 1.9452, -0.1453, -0.7602, -0.7155, -0.3242],\n",
       "         grad_fn=<SqueezeBackward1>),\n",
       "  'Treasury Rate': tensor([-0.9045,  0.1005, -1.1558,  1.6081,  0.3518],\n",
       "         grad_fn=<SqueezeBackward1>),\n",
       "  'IV Mean': tensor([ 0.1665,  0.6480, -0.8246,  1.3913, -1.3811],\n",
       "         grad_fn=<SqueezeBackward1>),\n",
       "  'IV Std.': tensor([-0.5974, -0.0247, -0.7790,  1.9236, -0.5225],\n",
       "         grad_fn=<SqueezeBackward1>)},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-5.1374e-01, -4.4264e-01, -2.0928e-01, -1.4660e-01, -1.1587e-01,\n",
       "           -5.5568e-02, -2.5981e-02, -2.5981e-02,  3.2430e-03,  3.2113e-02,\n",
       "            6.0637e-02,  1.1668e-01,  1.7143e-01,  1.9835e-01,  2.7731e-01,\n",
       "            3.0305e-01,  3.5373e-01,  4.0335e-01,  4.2778e-01,  4.5196e-01,\n",
       "            4.7590e-01,  4.7590e-01,  4.9960e-01,  5.6931e-01,  6.1468e-01,\n",
       "            6.1468e-01,  6.3704e-01,  6.5920e-01, -1.7046e+00, -1.7046e+00,\n",
       "           -1.5885e+00, -1.4778e+00, -1.4778e+00, -1.3721e+00, -1.3209e+00,\n",
       "           -1.2709e+00, -1.1266e+00, -1.0805e+00, -1.0351e+00, -9.9068e-01,\n",
       "           -8.6207e-01, -8.2070e-01, -7.8003e-01, -7.4005e-01, -7.0074e-01,\n",
       "           -5.8653e-01, -5.4965e-01, -5.4965e-01, -4.4232e-01, -3.7337e-01,\n",
       "           -3.0636e-01, -2.4118e-01, -2.4118e-01, -2.0925e-01, -1.1595e-01,\n",
       "           -8.5650e-02, -5.5728e-02, -5.5728e-02, -2.6177e-02,  3.1844e-02,\n",
       "            6.0332e-02,  8.8484e-02,  1.4381e-01,  2.5073e-01,  2.7673e-01,\n",
       "            3.0245e-01,  3.2789e-01,  3.7797e-01,  4.5117e-01,  4.7508e-01,\n",
       "            5.2218e-01,  5.4539e-01,  5.6837e-01,  6.1369e-01,  6.3602e-01,\n",
       "            6.8007e-01,  7.2332e-01,  8.0754e-01,  8.2813e-01,  8.4855e-01,\n",
       "            9.4810e-01,  1.0436e+00,  1.0808e+00,  1.0808e+00,  1.0992e+00,\n",
       "            1.0992e+00,  1.1174e+00,  1.1713e+00,  1.1713e+00,  1.2065e+00,\n",
       "            1.2924e+00,  1.2924e+00,  1.3753e+00,  1.3915e+00,  1.4077e+00,\n",
       "            1.4237e+00,  1.4237e+00,  1.4396e+00,  1.4711e+00,  1.5176e+00,\n",
       "            1.5329e+00,  1.5782e+00,  1.5931e+00,  1.5931e+00,  1.6950e+00,\n",
       "            1.7233e+00,  1.7373e+00,  1.7513e+00,  1.7789e+00,  1.7926e+00,\n",
       "            1.8063e+00,  1.8198e+00,  1.8198e+00,  1.8467e+00,  1.8600e+00,\n",
       "            1.8733e+00,  1.8865e+00,  1.9126e+00,  1.9256e+00,  1.9385e+00,\n",
       "            1.9513e+00,  1.9767e+00,  2.0019e+00,  2.0144e+00,  2.0392e+00,\n",
       "           -5.8616e-01, -4.4208e-01, -4.0739e-01, -3.7319e-01, -2.4113e-01,\n",
       "           -1.7775e-01, -1.4668e-01, -1.1602e-01, -5.5847e-02, -2.6325e-02,\n",
       "            2.8358e-03,  3.1643e-02,  8.8230e-02,  1.1603e-01,  1.4350e-01,\n",
       "            1.7066e-01,  1.9752e-01,  2.2407e-01,  3.0200e-01,  3.2741e-01,\n",
       "            4.2645e-01,  5.6767e-01,  5.9041e-01,  6.3526e-01,  6.5736e-01,\n",
       "            7.0097e-01,  7.0097e-01,  7.2248e-01,  7.4379e-01,  7.6492e-01,\n",
       "            8.0661e-01, -3.0611e-01, -2.4106e-01, -2.0920e-01, -1.1609e-01,\n",
       "           -5.5987e-02, -5.5987e-02, -2.6496e-02,  1.4314e-01,  1.4314e-01,\n",
       "            1.7028e-01,  1.9710e-01,  2.2362e-01,  3.5198e-01,  3.7684e-01,\n",
       "            4.2579e-01,  4.4989e-01,  4.7375e-01,  4.9737e-01,  4.9737e-01,\n",
       "            5.2076e-01,  6.1207e-01,  6.5645e-01,  6.7833e-01,  7.0001e-01,\n",
       "            7.0001e-01,  7.2149e-01,  7.6388e-01,  7.8480e-01, -1.3160e+00,\n",
       "           -1.2661e+00, -1.2173e+00, -1.1694e+00, -1.0314e+00, -9.0104e-01,\n",
       "           -8.5912e-01, -8.5912e-01, -8.1793e-01, -6.5998e-01, -6.5998e-01,\n",
       "           -5.8478e-01, -5.1190e-01, -4.7629e-01, -4.7629e-01, -4.4120e-01,\n",
       "           -4.0662e-01, -3.7254e-01, -3.0582e-01, -2.7315e-01, -2.4093e-01,\n",
       "           -2.4093e-01, -1.4681e-01, -1.1625e-01, -1.1625e-01, -8.6076e-02,\n",
       "           -5.6285e-02,  3.0906e-02,  3.0906e-02,  1.6945e-01,  1.9621e-01,\n",
       "            4.7221e-01,  4.9578e-01,  6.3246e-01,  7.1938e-01,  8.0323e-01,\n",
       "            8.8421e-01,  9.0403e-01,  9.8169e-01,  1.0383e+00,  1.0753e+00,\n",
       "            1.0936e+00,  1.0936e+00,  1.1476e+00,  1.1830e+00,  1.2350e+00,\n",
       "            1.2350e+00,  1.3027e+00,  1.3193e+00,  1.3358e+00,  1.3358e+00,\n",
       "            1.3522e+00,  1.3685e+00,  1.3847e+00,  1.4007e+00,  1.4325e+00,\n",
       "            1.4483e+00,  1.4639e+00,  1.4639e+00,  1.5254e+00,  1.5405e+00,\n",
       "            1.5556e+00,  1.6001e+00,  1.6001e+00,  1.6148e+00,  1.6294e+00,\n",
       "            1.6439e+00,  1.6726e+00,  1.6726e+00,  1.7009e+00,  1.7150e+00,\n",
       "            1.7429e+00,  1.7567e+00,  1.7704e+00,  1.7704e+00,  1.8511e+00,\n",
       "            1.8775e+00,  1.8905e+00,  1.9164e+00,  1.9292e+00,  1.9420e+00,\n",
       "            1.9547e+00,  1.9673e+00,  1.9673e+00,  1.9799e+00,  1.9924e+00,\n",
       "            2.0048e+00,  2.0417e+00,  2.0781e+00,  2.0901e+00,  2.1020e+00,\n",
       "            2.1374e+00,  2.1608e+00,  2.1608e+00, -1.6914e+00, -1.5763e+00,\n",
       "           -1.5763e+00, -1.5208e+00, -1.4666e+00, -1.3618e+00, -1.2614e+00,\n",
       "           -1.1652e+00, -1.1185e+00, -9.8368e-01, -9.4041e-01, -8.9792e-01,\n",
       "           -4.0565e-01, -3.7172e-01, -3.0529e-01, -2.7277e-01, -1.4697e-01,\n",
       "           -1.1654e-01, -8.6501e-02, -5.6839e-02, -2.7545e-02, -2.7545e-02,\n",
       "            1.3889e-03,  2.9972e-02,  1.4096e-01,  1.4096e-01,  1.6791e-01,\n",
       "            1.9456e-01,  2.2091e-01,  2.4696e-01,  2.9823e-01,  3.9753e-01,\n",
       "            4.6936e-01,  4.9282e-01,  4.9282e-01,  5.3906e-01,  6.0676e-01,\n",
       "            6.2891e-01,  7.3660e-01,  7.7834e-01,  8.1935e-01,  8.9930e-01,\n",
       "            9.1886e-01,  9.3827e-01,  1.0143e+00,  1.0330e+00,  1.0880e+00,\n",
       "           -1.4596e+00, -1.4069e+00, -1.3554e+00, -1.2556e+00, -1.2072e+00,\n",
       "           -1.1598e+00, -8.9404e-01, -8.5253e-01, -7.3223e-01, -6.9347e-01,\n",
       "           -6.1780e-01, -5.8087e-01, -5.8087e-01, -4.3869e-01, -4.0445e-01,\n",
       "           -3.7070e-01, -3.0463e-01, -2.7228e-01, -2.4037e-01, -2.8394e-02,\n",
       "            3.8274e-04,  8.4654e-02,  1.1209e-01,  1.6600e-01,  1.6600e-01,\n",
       "            1.9250e-01,  2.4462e-01,  2.7026e-01,  2.9561e-01,  3.2069e-01,\n",
       "            3.4551e-01,  3.7007e-01,  3.7007e-01,  3.9437e-01,  4.1843e-01,\n",
       "            5.1225e-01,  6.0247e-01,  6.2449e-01,  6.4631e-01,  6.6792e-01,\n",
       "            7.1057e-01,  7.3160e-01,  8.5399e-01,  8.5399e-01,  8.7379e-01,\n",
       "            8.9341e-01,  9.1287e-01,  1.0264e+00,  1.0448e+00,  1.0811e+00,\n",
       "            1.1346e+00,  1.1346e+00,  1.1522e+00,  1.1522e+00, -1.2509e+00,\n",
       "           -1.0640e+00, -9.3303e-01, -8.9095e-01, -8.9095e-01, -7.6909e-01,\n",
       "           -7.2985e-01, -6.9125e-01, -6.5328e-01, -6.1592e-01, -5.7914e-01,\n",
       "           -5.7914e-01, -5.4294e-01, -4.7217e-01, -4.3758e-01, -3.6989e-01,\n",
       "           -3.6989e-01, -3.0411e-01, -2.7190e-01, -2.7190e-01, -2.4013e-01,\n",
       "           -2.4013e-01, -1.7785e-01, -1.7785e-01, -1.1720e-01, -4.1825e-04,\n",
       "            2.7886e-02,  5.5852e-02,  8.3487e-02,  1.1080e-01,  1.1080e-01,\n",
       "            1.3779e-01,  1.6448e-01,  2.9353e-01,  3.6766e-01,  4.1581e-01,\n",
       "            4.6299e-01,  4.8622e-01,  5.0923e-01,  5.3201e-01,  5.5457e-01,\n",
       "            5.7692e-01,  6.2098e-01,  6.8555e-01,  7.0668e-01,  7.6895e-01,\n",
       "            8.6919e-01,  8.8873e-01,  9.2732e-01,  9.4639e-01,  9.4639e-01,\n",
       "            9.6529e-01,  9.8405e-01,  1.0027e+00,  1.0394e+00,  1.0576e+00,\n",
       "            1.0756e+00,  1.1113e+00,  1.1113e+00,  1.1289e+00,  1.1638e+00,\n",
       "            1.1810e+00,  1.1981e+00,  1.2151e+00,  1.2151e+00,  1.2487e+00,\n",
       "            1.2653e+00,  1.2818e+00,  1.2982e+00,  1.3306e+00,  1.3467e+00,\n",
       "            1.3467e+00,  1.3784e+00,  1.4407e+00,  1.4560e+00,  1.4712e+00,\n",
       "            1.4712e+00,  1.5014e+00,  1.5014e+00,  1.5163e+00,  1.5311e+00,\n",
       "            1.5750e+00,  1.5895e+00,  1.6039e+00,  1.6324e+00,  1.6465e+00,\n",
       "            1.6465e+00,  1.6744e+00,  1.7564e+00,  1.7564e+00,  1.7698e+00,\n",
       "            1.7831e+00, -1.5988e+00, -1.4892e+00, -1.3845e+00, -1.3339e+00,\n",
       "           -1.2844e+00, -1.2359e+00, -1.1419e+00, -1.0963e+00, -1.0078e+00,\n",
       "           -9.6474e-01, -9.2249e-01, -9.2249e-01, -8.4026e-01, -8.0022e-01,\n",
       "           -7.2217e-01, -7.2217e-01, -6.0984e-01, -5.7358e-01, -5.3789e-01,\n",
       "           -5.0274e-01, -4.6812e-01, -4.0040e-01, -3.6727e-01, -3.3462e-01,\n",
       "           -3.0241e-01, -2.7066e-01, -2.7066e-01, -2.0843e-01, -1.4784e-01,\n",
       "           -3.1247e-02,  5.2482e-02,  1.0666e-01,  1.5958e-01,  2.1132e-01,\n",
       "            3.8376e-01,  4.3075e-01,  4.5389e-01,  5.4419e-01,  5.8804e-01,\n",
       "            6.0966e-01,  6.7332e-01,  6.9416e-01,  7.5555e-01,  7.7566e-01,\n",
       "            7.9559e-01,  8.1536e-01,  9.3049e-01,  9.4913e-01,  1.0042e+00,\n",
       "            1.0222e+00,  1.0579e+00,  1.0756e+00,  1.1277e+00,  1.1448e+00,\n",
       "            1.1618e+00,  1.1787e+00,  1.1787e+00,  1.1954e+00,  1.2120e+00,\n",
       "            1.2120e+00,  1.2774e+00,  1.3093e+00,  1.3252e+00,  1.3565e+00,\n",
       "            1.3874e+00,  1.5646e+00,  1.5787e+00,  1.5928e+00,  1.6068e+00,\n",
       "            1.6207e+00,  1.6345e+00,  1.6345e+00,  1.7025e+00,  1.7158e+00,\n",
       "            1.7291e+00,  1.7554e+00,  1.8199e+00,  1.8452e+00,  1.8577e+00,\n",
       "            1.8577e+00,  1.8826e+00,  1.8826e+00,  1.8950e+00,  1.9072e+00,\n",
       "            1.9195e+00,  1.9677e+00,  1.9914e+00,  2.0032e+00,  2.0032e+00,\n",
       "            2.0726e+00,  2.0953e+00,  2.1178e+00,  2.1400e+00, -1.5789e+00,\n",
       "           -1.2689e+00, -1.0394e+00, -9.9618e-01, -9.9618e-01, -9.5375e-01,\n",
       "           -8.7120e-01, -8.7120e-01, -7.1460e-01, -6.7708e-01, -6.7708e-01,\n",
       "           -6.0385e-01, -5.6810e-01, -5.6810e-01, -5.3291e-01, -4.6412e-01,\n",
       "           -4.6412e-01, -4.3049e-01, -3.6469e-01, -3.0075e-01, -3.0075e-01,\n",
       "           -2.0809e-01, -1.4835e-01, -1.1906e-01, -5.5405e-03,  2.1974e-02,\n",
       "            4.9158e-02,  1.0257e-01,  1.2881e-01,  1.5475e-01,  1.8040e-01,\n",
       "            2.0576e-01,  2.0576e-01,  2.3085e-01,  2.8019e-01,  3.0447e-01,\n",
       "            3.2849e-01,  3.7578e-01,  4.6751e-01,  5.1202e-01,  5.5567e-01,\n",
       "            5.9850e-01,  6.1962e-01,  6.4054e-01,  6.6127e-01,  6.8181e-01,\n",
       "            7.0217e-01,  7.2234e-01,  7.4234e-01,  7.8182e-01,  7.8182e-01,\n",
       "            8.2062e-01,  8.2062e-01,  8.3978e-01,  8.7761e-01,  8.9629e-01,\n",
       "            9.6952e-01,  9.8746e-01,  1.0229e+00,  1.0405e+00,  1.0579e+00,\n",
       "            1.0579e+00,  1.0751e+00,  1.1093e+00, -1.6297e+00, -1.6297e+00,\n",
       "           -1.4685e+00, -1.4173e+00, -1.1780e+00, -1.0892e+00, -1.0892e+00,\n",
       "           -8.4261e-01, -6.2118e-01, -3.5717e-01, -2.9588e-01, -2.6588e-01,\n",
       "           -2.0708e-01, -2.0708e-01, -1.7826e-01, -9.4039e-02, -3.9650e-02,\n",
       "            3.9467e-02,  9.0657e-02,  1.8956e-01,  2.1360e-01,  2.8416e-01,\n",
       "            3.0718e-01,  3.5250e-01,  3.7482e-01,  4.4041e-01,  4.6185e-01,\n",
       "            4.8307e-01,  4.8307e-01,  5.4553e-01,  5.6596e-01,  6.2611e-01,\n",
       "            7.0381e-01,  9.5580e-01,  9.8953e-01,  1.0227e+00,  1.0555e+00,\n",
       "            1.0877e+00,  1.1194e+00,  1.1508e+00,  1.2716e+00,  1.3008e+00,\n",
       "            1.3008e+00,  1.5213e+00,  1.5986e+00,  1.6486e+00,  1.6732e+00,\n",
       "            1.6975e+00,  1.7921e+00,  1.8151e+00,  1.8379e+00,  1.8379e+00,\n",
       "            1.8604e+00,  1.8827e+00,  1.9267e+00,  1.9483e+00,  1.9698e+00,\n",
       "            2.0328e+00,  2.0328e+00], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-3.0271e-01, -1.4497e-01, -8.4684e-02, -2.5891e-02,  3.1485e-02,\n",
       "            1.4225e-01,  1.9576e-01,  2.2207e-01,  2.2207e-01,  3.4942e-01,\n",
       "            3.9852e-01,  4.2269e-01,  4.4662e-01,  5.6279e-01,  5.8536e-01,\n",
       "            6.0772e-01,  6.2987e-01,  6.5181e-01,  6.5181e-01,  6.9510e-01,\n",
       "            7.1646e-01,  7.3762e-01,  7.5860e-01,  7.7940e-01,  8.2045e-01,\n",
       "            8.6081e-01,  8.8073e-01,  9.5880e-01,  9.9690e-01, -1.4503e-01,\n",
       "           -1.1475e-01, -1.1475e-01, -8.4845e-02, -2.6146e-02,  5.9269e-02,\n",
       "            8.7073e-02,  2.2141e-01,  2.7309e-01,  2.9851e-01,  2.9851e-01,\n",
       "            3.2367e-01,  3.4856e-01,  4.2171e-01,  4.4560e-01,  5.3884e-01,\n",
       "            5.6158e-01,  6.5046e-01,  6.9368e-01,  7.1500e-01,  7.3613e-01,\n",
       "            8.7901e-01,  9.1830e-01, -1.1072e+00, -1.0176e+00, -9.3133e-01,\n",
       "           -9.3133e-01, -8.4807e-01, -8.0751e-01, -6.8983e-01, -6.5186e-01,\n",
       "           -5.4148e-01, -5.0580e-01, -4.0187e-01, -3.3502e-01, -2.7001e-01,\n",
       "           -2.3816e-01, -1.7572e-01, -1.4510e-01, -1.4510e-01, -8.5028e-02,\n",
       "           -8.5028e-02, -5.5552e-02,  2.3232e-03,  5.8820e-02,  5.8820e-02,\n",
       "            8.6572e-02,  1.1400e-01,  1.4112e-01,  1.6793e-01,  3.2273e-01,\n",
       "            3.4758e-01,  3.7217e-01,  3.9650e-01,  4.6805e-01,  4.9143e-01,\n",
       "            5.6021e-01,  5.6021e-01,  5.8270e-01,  6.4892e-01,  6.9206e-01,\n",
       "            8.3716e-01,  8.3716e-01,  8.5719e-01,  9.5485e-01,  9.5485e-01,\n",
       "            9.9281e-01,  1.0302e+00,  1.0302e+00,  1.0486e+00,  1.1032e+00,\n",
       "            1.1389e+00,  1.1914e+00,  1.2258e+00,  1.2428e+00,  1.2765e+00,\n",
       "            1.3096e+00,  1.3261e+00,  1.3261e+00,  1.3747e+00,  1.4065e+00,\n",
       "            1.4535e+00,  1.4843e+00,  1.4996e+00,  1.4996e+00,  1.5298e+00,\n",
       "            1.5596e+00,  1.5744e+00,  1.6327e+00,  1.6612e+00,  1.6895e+00,\n",
       "            1.7035e+00,  1.7035e+00,  1.7174e+00,  1.7312e+00,  1.7450e+00,\n",
       "            1.7587e+00,  1.7723e+00,  1.7992e+00,  1.8126e+00,  1.8259e+00,\n",
       "            1.8653e+00,  1.9169e+00,  1.9169e+00,  1.9674e+00,  1.9799e+00,\n",
       "            2.0168e+00,  2.0168e+00,  2.0532e+00,  2.1361e+00, -8.5164e-02,\n",
       "            3.0442e-02,  3.0442e-02,  8.6197e-02,  1.9392e-01,  2.4600e-01,\n",
       "            3.2203e-01,  3.4684e-01,  3.4684e-01,  3.9570e-01,  4.9049e-01,\n",
       "            5.8163e-01,  6.0388e-01,  6.2592e-01,  6.9085e-01,  7.9524e-01,\n",
       "            7.9524e-01, -2.0667e-01, -1.4522e-01, -1.4522e-01, -2.6910e-02,\n",
       "            3.0096e-02,  8.5760e-02,  1.4015e-01,  1.9331e-01,  3.7050e-01,\n",
       "            3.9476e-01,  3.9476e-01,  4.1878e-01,  4.4255e-01,  5.1248e-01,\n",
       "            5.8040e-01,  5.8040e-01,  6.2461e-01,  6.8943e-01,  7.3168e-01,\n",
       "            7.5252e-01,  7.7318e-01,  7.9366e-01, -1.1470e+00, -9.6899e-01,\n",
       "           -8.0348e-01, -8.0348e-01, -7.6386e-01, -6.8656e-01, -6.8656e-01,\n",
       "           -6.4884e-01, -4.0046e-01, -3.6702e-01, -3.3405e-01, -3.3405e-01,\n",
       "           -2.3781e-01, -2.3781e-01, -2.0659e-01, -2.0659e-01, -1.7577e-01,\n",
       "           -1.4535e-01, -5.6380e-02,  1.1219e-03,  2.9355e-02,  2.9355e-02,\n",
       "            1.1208e-01,  1.6566e-01,  2.9452e-01,  3.1946e-01,  3.9276e-01,\n",
       "            4.1669e-01,  4.1669e-01,  4.6384e-01,  5.1007e-01,  5.3285e-01,\n",
       "            5.5540e-01,  5.5540e-01,  5.7775e-01,  6.2182e-01,  6.2182e-01,\n",
       "            6.4354e-01,  6.8641e-01,  7.6987e-01,  7.9028e-01,  8.5047e-01,\n",
       "            9.0916e-01,  9.0916e-01,  9.2841e-01,  9.2841e-01,  1.0039e+00,\n",
       "            1.0407e+00,  1.0589e+00,  1.1303e+00,  1.1479e+00,  1.1653e+00,\n",
       "            1.1653e+00,  1.2167e+00,  1.2167e+00,  1.2336e+00,  1.2504e+00,\n",
       "            1.2836e+00,  1.3000e+00,  1.3325e+00,  1.4275e+00,  1.4429e+00,\n",
       "            1.4583e+00,  1.5038e+00,  1.5631e+00,  1.5922e+00,  1.6209e+00,\n",
       "            1.7189e+00,  1.7189e+00,  1.7461e+00,  1.7997e+00,  1.8129e+00,\n",
       "            1.8129e+00,  1.8521e+00,  1.8779e+00,  1.8779e+00, -1.1392e+00,\n",
       "           -1.0493e+00, -9.6266e-01, -9.6266e-01, -7.9849e-01, -5.0114e-01,\n",
       "           -4.6650e-01, -3.9871e-01, -3.3284e-01, -3.0058e-01, -1.4566e-01,\n",
       "           -1.1587e-01, -1.1587e-01, -8.6456e-02, -5.7407e-02, -5.7407e-02,\n",
       "           -2.8714e-02,  2.7637e-02,  5.5311e-02,  8.2662e-02,  1.0970e-01,\n",
       "            1.3642e-01,  1.3642e-01,  1.6284e-01,  1.8897e-01,  2.1481e-01,\n",
       "            2.6565e-01,  3.3989e-01,  4.5862e-01,  4.5862e-01,  4.8166e-01,\n",
       "            5.0447e-01,  5.4945e-01,  5.7161e-01,  6.1532e-01,  6.3687e-01,\n",
       "            6.5823e-01,  6.7939e-01,  7.0037e-01,  7.4176e-01,  7.6218e-01,\n",
       "            7.6218e-01,  7.8243e-01,  8.4213e-01,  8.6170e-01,  9.3838e-01,\n",
       "            9.7580e-01,  1.0308e+00,  1.0308e+00,  1.0489e+00,  1.0489e+00,\n",
       "            1.0668e+00,  1.1022e+00,  1.1197e+00,  1.1371e+00,  1.2553e+00,\n",
       "            1.2881e+00,  1.2881e+00,  1.3042e+00,  1.3042e+00,  1.3203e+00,\n",
       "            1.3363e+00,  1.4298e+00,  1.4451e+00,  1.4602e+00,  1.4602e+00,\n",
       "            1.5050e+00,  1.5490e+00,  1.5635e+00,  1.6485e+00,  1.6624e+00,\n",
       "            1.6762e+00,  1.6899e+00,  1.7171e+00,  1.7306e+00,  1.7440e+00,\n",
       "            1.7440e+00,  1.7573e+00,  1.7968e+00,  1.7968e+00,  1.8099e+00,\n",
       "            1.8228e+00,  1.8485e+00,  1.8613e+00,  1.8613e+00,  1.8739e+00,\n",
       "            1.8865e+00,  1.8991e+00,  1.8991e+00,  1.9240e+00,  1.9608e+00,\n",
       "            2.0209e+00,  2.0562e+00,  2.0795e+00,  2.1254e+00,  2.1480e+00,\n",
       "            2.1480e+00,  2.1703e+00,  2.1703e+00, -1.1331e+00, -1.1331e+00,\n",
       "           -1.0879e+00, -1.0437e+00, -9.5763e-01, -9.1577e-01, -8.7464e-01,\n",
       "           -8.3424e-01, -6.7929e-01, -6.7929e-01, -5.6950e-01, -5.3403e-01,\n",
       "           -4.9909e-01, -4.6467e-01, -4.3076e-01, -3.3187e-01, -2.9983e-01,\n",
       "           -2.6822e-01, -2.3703e-01, -1.4591e-01, -1.5526e-03,  8.0942e-02,\n",
       "            8.0942e-02,  1.3435e-01,  1.3435e-01,  1.6061e-01,  1.8657e-01,\n",
       "            1.8657e-01,  2.1224e-01,  2.1224e-01,  2.3763e-01,  2.6275e-01,\n",
       "            2.8760e-01,  3.6059e-01,  4.3136e-01,  5.0003e-01,  5.2248e-01,\n",
       "            5.8855e-01,  6.1016e-01,  6.9466e-01,  7.3578e-01,  7.5607e-01,\n",
       "            8.1591e-01,  8.3551e-01,  8.5495e-01,  9.1232e-01,  9.3114e-01,\n",
       "            9.3114e-01,  9.4980e-01,  9.8668e-01,  1.0049e+00,  1.0230e+00,\n",
       "            1.0230e+00,  1.0939e+00,  1.0939e+00,  1.1628e+00,  1.1628e+00,\n",
       "            1.1797e+00,  1.2786e+00,  1.3106e+00,  1.3265e+00,  1.3422e+00,\n",
       "            1.3422e+00, -1.1254e+00, -1.0807e+00, -9.5139e-01, -8.6908e-01,\n",
       "           -7.8960e-01, -7.5086e-01, -7.1277e-01, -6.7530e-01, -6.0213e-01,\n",
       "           -4.9655e-01, -4.6241e-01, -3.6291e-01, -3.3068e-01, -2.3660e-01,\n",
       "           -2.3660e-01, -2.0608e-01, -2.0608e-01, -1.7595e-01, -1.1685e-01,\n",
       "           -8.7864e-02,  5.1853e-02,  5.1853e-02,  1.0545e-01,  1.8358e-01,\n",
       "            2.3424e-01,  2.5915e-01,  2.8380e-01,  3.0819e-01,  4.2640e-01,\n",
       "            4.4933e-01,  4.7204e-01,  5.1679e-01,  5.3884e-01,  5.8233e-01,\n",
       "            6.4605e-01,  6.6691e-01,  6.8758e-01,  7.0806e-01,  7.2837e-01,\n",
       "            7.4850e-01,  7.6845e-01,  7.6845e-01,  7.8823e-01,  8.0785e-01,\n",
       "            8.2730e-01,  8.4658e-01,  8.6571e-01,  8.6571e-01,  9.4066e-01,\n",
       "            9.5903e-01,  1.0311e+00,  1.0311e+00,  1.0487e+00,  1.0662e+00,\n",
       "            1.1009e+00,  1.1350e+00,  1.1519e+00,  1.2182e+00,  1.2507e+00,\n",
       "            1.2827e+00,  1.2986e+00,  1.3143e+00,  1.3299e+00,  1.3299e+00,\n",
       "            1.3454e+00,  1.3454e+00,  1.3914e+00,  1.4065e+00,  1.4364e+00,\n",
       "            1.4806e+00,  1.5096e+00,  1.5239e+00,  1.5382e+00,  1.5382e+00,\n",
       "            1.5944e+00,  1.6083e+00,  1.6220e+00,  1.6357e+00,  1.6628e+00,\n",
       "            1.6763e+00,  1.6763e+00,  1.6896e+00,  1.7029e+00,  1.7161e+00,\n",
       "            1.7292e+00,  1.7553e+00,  1.7682e+00,  1.7682e+00,  1.7810e+00,\n",
       "            1.8191e+00,  1.8317e+00,  1.8690e+00,  1.9056e+00,  1.9177e+00,\n",
       "            1.9298e+00, -1.1193e+00, -1.0313e+00, -9.8846e-01, -8.6465e-01,\n",
       "           -8.2483e-01, -8.2483e-01, -7.8569e-01, -7.4721e-01, -7.4721e-01,\n",
       "           -7.0936e-01, -6.3549e-01, -5.9943e-01, -5.6393e-01, -3.6176e-01,\n",
       "           -2.6700e-01, -1.7600e-01, -1.4646e-01, -8.8484e-02, -4.1885e-03,\n",
       "            5.0332e-02,  1.5563e-01,  1.8121e-01,  2.0651e-01,  2.8078e-01,\n",
       "            3.7620e-01,  3.7620e-01,  3.9945e-01,  4.2246e-01,  5.1227e-01,\n",
       "            5.5588e-01,  5.9868e-01,  5.9868e-01,  6.4070e-01,  6.6142e-01,\n",
       "            7.0231e-01,  7.0231e-01,  7.6231e-01,  7.6231e-01,  8.2077e-01,\n",
       "            8.3993e-01,  9.3340e-01,  9.5165e-01,  9.6975e-01,  9.8771e-01,\n",
       "            1.0055e+00,  1.0232e+00,  1.0582e+00,  1.0755e+00,  1.0926e+00,\n",
       "            1.0926e+00,  1.1096e+00,  1.1265e+00,  1.1265e+00,  1.1600e+00,\n",
       "            1.1765e+00,  1.2092e+00,  1.2414e+00,  1.2733e+00,  1.2890e+00,\n",
       "            1.3046e+00,  1.3661e+00,  1.3812e+00,  1.3962e+00,  1.4260e+00,\n",
       "            1.4407e+00,  1.4553e+00,  1.4553e+00,  1.4843e+00,  1.4987e+00,\n",
       "            1.5271e+00,  1.5691e+00,  1.5967e+00,  1.6375e+00,  1.7169e+00,\n",
       "            1.8187e+00,  1.8311e+00,  1.8801e+00,  1.8921e+00,  1.9042e+00,\n",
       "            1.9866e+00,  1.9981e+00,  2.0096e+00,  2.0324e+00,  2.0324e+00,\n",
       "            2.0549e+00,  2.0549e+00,  2.0773e+00, -1.0563e+00, -1.0136e+00,\n",
       "           -9.7170e-01, -9.7170e-01, -9.3056e-01, -8.9016e-01, -8.5048e-01,\n",
       "           -8.1148e-01, -7.3547e-01, -6.9841e-01, -6.9841e-01, -6.6196e-01,\n",
       "           -5.9077e-01, -4.5485e-01, -3.8986e-01, -3.5805e-01, -3.5805e-01,\n",
       "           -2.0547e-01, -1.7616e-01, -1.4723e-01, -9.0469e-02, -3.5106e-02,\n",
       "           -7.9288e-03, -7.9288e-03,  1.2322e-01,  1.9838e-01,  2.2289e-01,\n",
       "            3.4154e-01,  3.6454e-01,  3.8730e-01,  4.0984e-01,  4.0984e-01,\n",
       "            4.3215e-01,  4.5424e-01,  4.5424e-01,  4.7611e-01,  5.1923e-01,\n",
       "            5.6153e-01,  5.6153e-01,  5.8239e-01,  6.2353e-01,  7.0361e-01,\n",
       "            7.4261e-01,  7.6185e-01,  7.8093e-01,  7.9985e-01,  8.1862e-01,\n",
       "            8.3722e-01,  8.7398e-01,  8.9213e-01,  9.1014e-01,  9.2801e-01,\n",
       "            9.4573e-01,  9.9808e-01,  1.0492e+00,  1.0660e+00,  1.1320e+00,\n",
       "            1.1960e+00,  1.2118e+00,  1.2274e+00,  1.2583e+00,  1.2736e+00,\n",
       "            1.2888e+00,  1.3039e+00,  1.3633e+00,  1.3924e+00,  1.4212e+00,\n",
       "            1.4354e+00,  1.5190e+00,  1.5190e+00,  1.5462e+00,  1.5596e+00,\n",
       "            1.5863e+00,  1.5995e+00,  1.6127e+00,  1.6258e+00, -1.4554e+00,\n",
       "           -1.3556e+00, -1.3073e+00, -1.2601e+00, -1.1686e+00, -1.1243e+00,\n",
       "           -9.9633e-01, -8.7545e-01, -6.5200e-01, -6.1687e-01, -5.8230e-01,\n",
       "           -5.4826e-01, -5.1474e-01, -4.4921e-01, -4.4921e-01, -3.8557e-01,\n",
       "           -3.2372e-01, -2.3410e-01, -1.4799e-01, -1.2003e-01, -9.2412e-02,\n",
       "           -3.8202e-02, -1.1591e-02, -1.1591e-02,  1.4701e-02,  4.0683e-02,\n",
       "            6.6360e-02,  1.4164e-01,  1.6617e-01,  1.9043e-01,  2.6163e-01,\n",
       "            2.8487e-01,  3.7541e-01,  4.6237e-01,  5.2540e-01,  5.6644e-01,\n",
       "            6.0672e-01,  6.2659e-01,  6.2659e-01,  6.6579e-01,  7.0431e-01,\n",
       "            7.2332e-01,  7.4216e-01,  7.6085e-01,  7.7937e-01,  7.9774e-01,\n",
       "            8.1596e-01,  8.1596e-01,  8.6973e-01,  8.8736e-01,  9.0486e-01,\n",
       "            9.0486e-01,  9.2221e-01,  9.2221e-01,  9.3943e-01,  9.5652e-01,\n",
       "            9.9029e-01,  1.0236e+00,  1.0400e+00,  1.0725e+00,  1.1518e+00,\n",
       "            1.1827e+00,  1.1980e+00,  1.2283e+00,  1.2433e+00,  1.2876e+00,\n",
       "            1.3167e+00,  1.3737e+00,  1.3878e+00,  1.4156e+00,  1.4293e+00,\n",
       "            1.4566e+00,  1.4701e+00,  1.4835e+00,  1.5101e+00,  1.5233e+00,\n",
       "            1.5753e+00,  1.5881e+00,  1.6135e+00,  1.6510e+00,  1.6634e+00,\n",
       "            1.6634e+00,  1.6879e+00,  1.6879e+00,  1.7481e+00,  1.7481e+00,\n",
       "            1.7717e+00,  1.8181e+00,  1.8296e+00,  1.8410e+00,  1.8523e+00,\n",
       "            1.8636e+00,  1.8971e+00,  1.9082e+00,  1.9627e+00,  2.0053e+00,\n",
       "            2.0263e+00,  2.0263e+00, -1.2166e+00, -1.1307e+00, -1.1307e+00,\n",
       "           -1.0483e+00, -1.0483e+00, -6.1421e-01, -3.6928e-01, -3.6928e-01,\n",
       "           -1.5089e-01, -1.5089e-01, -4.9957e-02, -4.9957e-02,  9.2562e-02,\n",
       "            1.3792e-01,  1.8227e-01,  2.2568e-01,  2.6817e-01,  3.0978e-01,\n",
       "            3.0978e-01,  3.9052e-01,  3.9052e-01,  5.0587e-01,  5.4291e-01,\n",
       "            6.1499e-01,  6.5009e-01,  6.5009e-01,  6.8459e-01,  7.1851e-01,\n",
       "            7.5187e-01,  7.8469e-01,  8.1698e-01,  9.1086e-01,  1.0006e+00,\n",
       "            1.0296e+00,  1.0865e+00,  1.1418e+00,  1.1418e+00,  1.1956e+00,\n",
       "            1.2220e+00,  1.2991e+00,  1.3489e+00,  1.4215e+00,  1.4684e+00,\n",
       "            1.4915e+00,  1.5368e+00,  1.5591e+00,  1.6671e+00,  1.6880e+00,\n",
       "            1.7292e+00,  1.7695e+00,  1.8091e+00],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-1.0721, -1.0721, -1.0531,  ...,  0.7036,  0.7197,  0.7197],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.2453, -0.2124, -0.1800,  ...,  1.7705,  1.7886,  1.7886],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.7365, -0.6978, -0.6978,  ...,  0.3396,  0.5298,  0.5298],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056,\n",
       "           -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056,\n",
       "           -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056,\n",
       "           -0.9056, -0.9056, -0.9056, -0.9056, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8597,\n",
       "           -0.8597, -0.8597, -0.8597, -0.8597, -0.8597, -0.8253, -0.8253, -0.8253,\n",
       "           -0.8253, -0.8253, -0.8253, -0.8253, -0.8253, -0.8253, -0.8253, -0.8253,\n",
       "           -0.8253, -0.8253, -0.8253, -0.8253, -0.8253, -0.8253, -0.8253, -0.8253,\n",
       "           -0.8253, -0.8253, -0.8253, -0.8253, -0.8253, -0.8253, -0.8253, -0.8253,\n",
       "           -0.8253, -0.8253, -0.8253, -0.8253, -0.7851, -0.7851, -0.7851, -0.7851,\n",
       "           -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851,\n",
       "           -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851,\n",
       "           -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990,\n",
       "           -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.6990, -0.5384, -0.5384,\n",
       "           -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384,\n",
       "           -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384,\n",
       "           -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384,\n",
       "           -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384,\n",
       "           -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.5384,\n",
       "           -0.5384, -0.5384, -0.5384, -0.5384, -0.5384, -0.3376, -0.3376, -0.3376,\n",
       "           -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376,\n",
       "           -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376,\n",
       "           -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376,\n",
       "           -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376,\n",
       "           -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376,\n",
       "           -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376, -0.3376,\n",
       "           -0.3376, -0.3376, -0.3376, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
       "           -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,  0.3451,\n",
       "            0.3451,  0.3451,  0.3451,  0.3451,  0.8671,  0.8671,  0.8671,  0.8671,\n",
       "            0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,\n",
       "            0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,\n",
       "            0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,\n",
       "            0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,\n",
       "            0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,\n",
       "            0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,\n",
       "            0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,  0.8671,\n",
       "            0.8671,  0.8671,  0.8671,  0.8671,  2.4333,  2.4333,  2.4333,  2.4333,\n",
       "            2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,\n",
       "            2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,\n",
       "            2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,\n",
       "            2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,\n",
       "            2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,\n",
       "            2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,\n",
       "            2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333,  2.4333],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056,\n",
       "           -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056,\n",
       "           -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.9056,\n",
       "           -0.9056, -0.9056, -0.9056, -0.9056, -0.9056, -0.8654, -0.8654, -0.8654,\n",
       "           -0.8654, -0.8654, -0.8654, -0.8654, -0.8654, -0.8654, -0.8654, -0.8654,\n",
       "           -0.8654, -0.8654, -0.8654, -0.8654, -0.8654, -0.8654, -0.8654, -0.8654,\n",
       "           -0.8654, -0.8654, -0.8654, -0.8654, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195, -0.8195,\n",
       "           -0.8195, -0.8195, -0.8195, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851,\n",
       "           -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851, -0.7851,\n",
       "           -0.7851, -0.7851, -0.7851, -0.7851, -0.7449, -0.7449, -0.7449, -0.7449,\n",
       "           -0.7449, -0.7449, -0.7449, -0.7449, -0.7449, -0.7449, -0.7449, -0.7449,\n",
       "           -0.7449, -0.7449, -0.7449, -0.7449, -0.7449, -0.7449, -0.7449, -0.7449,\n",
       "           -0.7449, -0.7449, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589,\n",
       "           -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589,\n",
       "           -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589,\n",
       "           -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589,\n",
       "           -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589,\n",
       "           -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589,\n",
       "           -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589,\n",
       "           -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589,\n",
       "           -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589,\n",
       "           -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.6589, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581, -0.4581,\n",
       "           -0.4581, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975,\n",
       "           -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975,\n",
       "           -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975,\n",
       "           -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975,\n",
       "           -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975,\n",
       "           -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975,\n",
       "           -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975,\n",
       "           -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975, -0.2975,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,\n",
       "           -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967, -0.0967,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,\n",
       "            0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.0640,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,  0.5860,\n",
       "            0.5860,  0.5860,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,  1.1081,\n",
       "            1.1081,  1.1081,  1.1081,  1.1081,  3.1963,  3.1963,  3.1963,  3.1963,\n",
       "            3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,\n",
       "            3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,\n",
       "            3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,\n",
       "            3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,\n",
       "            3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,\n",
       "            3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963,  3.1963],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9056, -0.9056, -0.9056,  ...,  2.5538,  2.5538,  2.5538],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9228, -0.9228, -0.9228,  ...,  2.9783,  2.9783,  2.9783],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9170, -0.9170, -0.9170,  ...,  3.1447,  3.1447,  3.1447],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.3716, 0.4187, 0.3716,\n",
       "           0.3716, 0.3617, 0.3363, 0.3212, 0.3147, 0.3133, 0.3133, 0.3716, 0.3133,\n",
       "           0.3716, 0.3133, 0.3716, 0.3133, 0.3133, 0.3133, 0.3716, 0.3133, 0.3716,\n",
       "           0.3716, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473,\n",
       "           0.3473, 0.3258, 0.3258, 0.3258, 0.3258, 0.3473, 0.3473, 0.3258, 0.3258,\n",
       "           0.3258, 0.3258, 0.3473, 0.3258, 0.3258, 0.3473, 0.3258, 0.3473, 0.3473,\n",
       "           0.3258, 0.3258, 0.3180, 0.3207, 0.3086, 0.2900, 0.2851, 0.2781, 0.2698,\n",
       "           0.2607, 0.2637, 0.2668, 0.2688, 0.2859, 0.3138, 0.2561, 0.2561, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295,\n",
       "           0.3295, 0.3295, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.2561, 0.3295,\n",
       "           0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.3295, 0.2561, 0.3295, 0.2561,\n",
       "           0.2561, 0.3295, 0.2561, 0.3295, 0.2561, 0.2561, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.3295, 0.2561, 0.3295, 0.3295, 0.3295, 0.2561,\n",
       "           0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.3295, 0.3295, 0.3295, 0.3381,\n",
       "           0.3381, 0.3381, 0.3381, 0.3265, 0.3632, 0.3381, 0.3216, 0.3165, 0.3037,\n",
       "           0.3004, 0.2912, 0.2812, 0.2803, 0.2746, 0.2723, 0.2720, 0.2671, 0.2695,\n",
       "           0.2617, 0.2847, 0.2702, 0.2702, 0.3356, 0.2702, 0.3356, 0.2702, 0.3356,\n",
       "           0.2702, 0.2702, 0.3356, 0.3564, 0.3187, 0.3187, 0.3002, 0.2968, 0.2920,\n",
       "           0.2829, 0.2660, 0.2642, 0.2607, 0.2606, 0.2599, 0.2623, 0.2655, 0.2648,\n",
       "           0.2754, 0.2797, 0.2871, 0.2661, 0.2920, 0.2661, 0.3215, 0.2661, 0.3215,\n",
       "           0.2661, 0.3215, 0.3215, 0.2661, 0.3369, 0.3510, 0.3510, 0.3369, 0.3369,\n",
       "           0.3369, 0.3510, 0.3369, 0.3369, 0.3510, 0.3369, 0.3510, 0.3510, 0.3510,\n",
       "           0.3369, 0.3369, 0.3369, 0.3369, 0.3295, 0.3397, 0.3226, 0.3172, 0.3031,\n",
       "           0.2975, 0.2933, 0.2938, 0.2866, 0.2749, 0.2728, 0.2622, 0.2596, 0.2658,\n",
       "           0.2692, 0.2912, 0.3146, 0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.3302, 0.3302, 0.3302,\n",
       "           0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302, 0.2695,\n",
       "           0.2695, 0.2695, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695, 0.3302, 0.3302,\n",
       "           0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.2695, 0.3302, 0.3302, 0.3302, 0.2695, 0.3302, 0.2695, 0.2695, 0.2695,\n",
       "           0.3302, 0.2695, 0.2695, 0.3302, 0.3302, 0.3302, 0.3302, 0.2695, 0.3695,\n",
       "           0.5483, 0.3695, 0.3695, 0.5483, 0.5483, 0.5483, 0.5483, 0.5483, 0.3695,\n",
       "           0.5483, 0.3695, 0.3469, 0.4183, 0.3310, 0.3267, 0.3105, 0.3345, 0.3049,\n",
       "           0.3031, 0.3168, 0.3001, 0.2975, 0.3078, 0.2949, 0.2905, 0.2926, 0.2908,\n",
       "           0.2887, 0.2866, 0.2852, 0.2911, 0.2942, 0.2847, 0.2983, 0.2876, 0.2911,\n",
       "           0.3135, 0.3053, 0.3091, 0.3461, 0.3318, 0.3461, 0.3386, 0.3461, 0.3415,\n",
       "           0.3461, 0.3459, 0.6848, 0.3459, 0.6848, 0.6642, 0.3459, 0.5333, 0.5164,\n",
       "           0.4740, 0.4519, 0.4324, 0.4189, 0.3381, 0.3202, 0.3164, 0.3511, 0.3059,\n",
       "           0.3018, 0.2989, 0.2883, 0.2808, 0.2774, 0.2753, 0.2727, 0.2728, 0.2728,\n",
       "           0.2691, 0.2710, 0.2698, 0.2669, 0.2671, 0.2664, 0.2710, 0.2712, 0.2666,\n",
       "           0.2672, 0.2711, 0.2715, 0.2728, 0.2745, 0.2770, 0.2782, 0.2864, 0.3104,\n",
       "           0.3104, 0.2921, 0.3104, 0.3076, 0.3133, 0.3104, 0.3256, 0.3104, 0.3273,\n",
       "           0.3104, 0.3472, 0.3472, 0.3472, 0.3822, 0.3472, 0.3472, 0.3822, 0.3422,\n",
       "           0.3822, 0.3320, 0.3775, 0.3280, 0.3672, 0.3508, 0.3103, 0.3265, 0.3036,\n",
       "           0.2970, 0.3053, 0.2938, 0.3040, 0.2912, 0.2958, 0.2864, 0.2824, 0.2764,\n",
       "           0.2747, 0.2725, 0.2711, 0.2698, 0.2707, 0.2691, 0.2686, 0.2639, 0.2656,\n",
       "           0.2663, 0.2630, 0.2670, 0.2628, 0.2658, 0.2692, 0.2642, 0.2661, 0.2684,\n",
       "           0.2775, 0.2869, 0.2849, 0.2849, 0.2853, 0.2869, 0.2849, 0.2849, 0.2919,\n",
       "           0.2849, 0.3020, 0.3043, 0.2849, 0.3108, 0.2849, 0.3125, 0.2849, 0.3207,\n",
       "           0.2849, 0.3229, 0.2849, 0.3229, 0.3229, 0.3124, 0.2849, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124,\n",
       "           0.2849, 0.2849, 0.3124, 0.3124, 0.2849, 0.3124, 0.2849, 0.3124, 0.3124,\n",
       "           0.2849, 0.2849, 0.2849, 0.6576, 0.6189, 0.5936, 0.3469, 0.3469, 0.5412,\n",
       "           0.3469, 0.3469, 0.3469, 0.4520, 0.4388, 0.3434, 0.4219, 0.3313, 0.3866,\n",
       "           0.3244, 0.3141, 0.3109, 0.3080, 0.3051, 0.3263, 0.3153, 0.2955, 0.2940,\n",
       "           0.3014, 0.2983, 0.2901, 0.2870, 0.2879, 0.2810, 0.2771, 0.2740, 0.2726,\n",
       "           0.2743, 0.2690, 0.2725, 0.2678, 0.2671, 0.2746, 0.2683, 0.2779, 0.2691,\n",
       "           0.2702, 0.2804, 0.2854, 0.2824, 0.2965, 0.2942, 0.2804, 0.3037, 0.2857,\n",
       "           0.3085, 0.3089, 0.2902, 0.2918, 0.2932, 0.3089, 0.3089, 0.2964, 0.3089,\n",
       "           0.3042, 0.3057, 0.3089, 0.3089, 0.3116, 0.3116, 0.3089, 0.3116, 0.3089,\n",
       "           0.3116, 0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3116, 0.3116,\n",
       "           0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116, 0.3089, 0.3116,\n",
       "           0.3116, 0.3089, 0.3089, 0.3116, 0.3089, 0.3089, 0.3439, 0.3439, 0.4487,\n",
       "           0.4355, 0.3331, 0.4234, 0.4020, 0.3242, 0.3154, 0.3551, 0.3130, 0.3100,\n",
       "           0.3391, 0.3078, 0.3058, 0.3209, 0.3031, 0.3157, 0.2985, 0.3037, 0.2965,\n",
       "           0.2930, 0.2948, 0.2915, 0.2874, 0.2867, 0.2854, 0.2849, 0.2848, 0.2818,\n",
       "           0.2839, 0.2803, 0.2834, 0.2811, 0.2824, 0.2822, 0.2819, 0.2779, 0.2814,\n",
       "           0.2811, 0.2763, 0.2819, 0.2824, 0.2767, 0.2829, 0.2817, 0.2824, 0.2765,\n",
       "           0.2772, 0.2783, 0.2858, 0.2779, 0.2890, 0.2871, 0.2882, 0.2902, 0.2938,\n",
       "           0.2975, 0.3004, 0.2849, 0.2853, 0.2980, 0.3009, 0.2999, 0.5636, 0.3514,\n",
       "           0.3463, 0.5072, 0.3388, 0.4173, 0.3351, 0.3262, 0.3204, 0.3153, 0.3135,\n",
       "           0.3134, 0.3101, 0.3126, 0.3115, 0.3105, 0.3050, 0.3026, 0.3019, 0.3061,\n",
       "           0.3052, 0.2983, 0.3050, 0.2977, 0.3053, 0.3043, 0.2967, 0.2968, 0.3043,\n",
       "           0.3043, 0.3040, 0.2955, 0.2960, 0.3071, 0.3091, 0.3101, 0.2967, 0.3110,\n",
       "           0.2989, 0.3118, 0.3167, 0.3020, 0.3136, 0.3194, 0.3105, 0.3119, 0.3129,\n",
       "           0.3137, 0.3182, 0.3192, 0.3202, 0.3194, 0.3194, 0.3232, 0.3194, 0.3194,\n",
       "           0.3284, 0.3320, 0.3194]),\n",
       "   tensor([0.4484, 0.5840, 0.5429, 0.4101, 0.4310, 0.3847, 0.3700, 0.3681, 0.3691,\n",
       "           0.4633, 0.4731, 0.4101, 0.4101, 0.4101, 0.4101, 0.4101, 0.4731, 0.4101,\n",
       "           0.4731, 0.4731, 0.4731, 0.4101, 0.4731, 0.4731, 0.4731, 0.4101, 0.4101,\n",
       "           0.4101, 0.4731, 0.6166, 0.5844, 0.3907, 0.5625, 0.3622, 0.3420, 0.3351,\n",
       "           0.3262, 0.3439, 0.3255, 0.3600, 0.3293, 0.3802, 0.3609, 0.3686, 0.4195,\n",
       "           0.4312, 0.3968, 0.4799, 0.3968, 0.4799, 0.4799, 0.3968, 0.7247, 0.7247,\n",
       "           0.7247, 0.3801, 0.7247, 0.7247, 0.3801, 0.3801, 0.7247, 0.7247, 0.7104,\n",
       "           0.6531, 0.5995, 0.3801, 0.3683, 0.4839, 0.3595, 0.4370, 0.3410, 0.3375,\n",
       "           0.3253, 0.3287, 0.3174, 0.3125, 0.3124, 0.3101, 0.3071, 0.2957, 0.2975,\n",
       "           0.3348, 0.3369, 0.3255, 0.3313, 0.3556, 0.4041, 0.4041, 0.4041, 0.4277,\n",
       "           0.4977, 0.4041, 0.5077, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041,\n",
       "           0.4041, 0.4041, 0.5174, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041,\n",
       "           0.5174, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.4041, 0.5174, 0.4041,\n",
       "           0.5174, 0.4041, 0.4041, 0.5174, 0.4041, 0.5174, 0.5174, 0.4041, 0.4041,\n",
       "           0.5174, 0.5174, 0.4041, 0.5174, 0.4041, 0.5174, 0.4041, 0.5174, 0.5174,\n",
       "           0.5174, 0.4041, 0.5174, 0.4041, 0.3262, 0.3205, 0.3120, 0.3063, 0.3012,\n",
       "           0.2893, 0.2909, 0.2918, 0.3028, 0.3062, 0.3124, 0.3597, 0.3706, 0.3556,\n",
       "           0.3763, 0.4201, 0.3597, 0.4300, 0.4024, 0.3242, 0.3140, 0.3244, 0.3084,\n",
       "           0.3010, 0.2962, 0.2856, 0.2924, 0.3028, 0.2940, 0.2967, 0.3057, 0.3184,\n",
       "           0.2893, 0.3283, 0.3463, 0.2893, 0.2893, 0.2893, 0.2893, 0.5372, 0.3481,\n",
       "           0.3962, 0.3481, 0.3481, 0.3962, 0.3481, 0.3962, 0.3347, 0.3962, 0.3962,\n",
       "           0.3493, 0.3835, 0.3255, 0.3572, 0.3246, 0.3167, 0.3150, 0.3045, 0.2954,\n",
       "           0.2926, 0.2979, 0.2877, 0.2843, 0.2904, 0.2905, 0.2975, 0.2869, 0.2964,\n",
       "           0.3042, 0.2950, 0.2979, 0.2983, 0.2973, 0.3000, 0.3090, 0.3143, 0.3151,\n",
       "           0.3202, 0.3151, 0.3427, 0.3151, 0.3578, 0.3151, 0.3578, 0.3151, 0.3578,\n",
       "           0.3578, 0.3151, 0.3578, 0.3578, 0.3578, 0.3151, 0.3578, 0.3151, 0.3578,\n",
       "           0.3151, 0.3578, 0.3578, 0.3151, 0.3578, 0.3151, 0.3578, 0.3151, 0.3578,\n",
       "           0.3578, 0.3151, 0.3578, 0.3151, 0.3151, 0.3151, 0.3578, 0.3151, 0.3151,\n",
       "           0.3578, 0.3151, 0.3632, 0.3355, 0.3632, 0.3355, 0.3355, 0.3355, 0.3570,\n",
       "           0.3231, 0.3372, 0.3129, 0.3020, 0.2973, 0.2963, 0.2949, 0.2864, 0.2920,\n",
       "           0.2847, 0.2818, 0.2809, 0.2853, 0.2829, 0.2790, 0.2829, 0.2775, 0.2771,\n",
       "           0.2809, 0.2744, 0.2753, 0.2774, 0.2859, 0.2783, 0.2859, 0.2807, 0.2829,\n",
       "           0.2856, 0.2894, 0.2945, 0.2933, 0.2970, 0.3034, 0.3025, 0.3017, 0.3017,\n",
       "           0.3165, 0.3017, 0.3017, 0.3413, 0.3519, 0.3017, 0.3550, 0.3017, 0.3017,\n",
       "           0.3017, 0.3017, 0.3017, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3017,\n",
       "           0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3017, 0.3017, 0.3784, 0.3017,\n",
       "           0.3017, 0.3017, 0.3017, 0.3784, 0.3784, 0.3784, 0.3017, 0.3017, 0.3784,\n",
       "           0.3017, 0.3017, 0.3784, 0.3017, 0.3784, 0.3017, 0.3784, 0.3784, 0.3784,\n",
       "           0.3017, 0.3784, 0.3017, 0.3784, 0.3784, 0.3784, 0.3017, 0.3784, 0.3017,\n",
       "           0.3784, 0.3017, 0.4610, 0.3484, 0.4610, 0.3484, 0.3484, 0.3484, 0.3484,\n",
       "           0.3484, 0.4487, 0.3484, 0.3464, 0.3953, 0.3835, 0.3753, 0.3358, 0.3477,\n",
       "           0.3237, 0.3221, 0.3331, 0.3128, 0.3037, 0.2988, 0.3029, 0.2974, 0.3021,\n",
       "           0.3011, 0.2953, 0.3013, 0.2928, 0.3012, 0.2923, 0.3008, 0.3009, 0.3017,\n",
       "           0.2903, 0.2903, 0.3054, 0.2918, 0.3098, 0.3156, 0.3001, 0.3271, 0.3284,\n",
       "           0.3091, 0.3284, 0.3185, 0.3212, 0.3284, 0.3284, 0.3284, 0.3284, 0.3323,\n",
       "           0.3284, 0.3443, 0.3284, 0.3563, 0.3284, 0.3589, 0.3284, 0.3678, 0.3678,\n",
       "           0.3678, 0.3284, 0.3532, 0.4089, 0.3532, 0.3532, 0.3356, 0.4089, 0.3407,\n",
       "           0.3976, 0.3716, 0.3439, 0.3410, 0.3278, 0.3096, 0.3063, 0.3047, 0.3044,\n",
       "           0.3021, 0.3030, 0.2967, 0.2939, 0.2848, 0.2881, 0.2814, 0.2781, 0.2856,\n",
       "           0.2858, 0.2834, 0.2764, 0.2755, 0.2761, 0.2859, 0.2926, 0.2862, 0.2790,\n",
       "           0.2924, 0.2829, 0.3071, 0.2859, 0.2990, 0.3016, 0.2913, 0.3016, 0.3016,\n",
       "           0.3016, 0.2940, 0.3016, 0.2969, 0.3016, 0.3047, 0.3016, 0.3147, 0.3016,\n",
       "           0.3016, 0.3181, 0.3016, 0.3277, 0.3315, 0.3411, 0.3016, 0.3016, 0.3514,\n",
       "           0.3543, 0.3532, 0.3016, 0.3557, 0.3016, 0.3634, 0.3016, 0.3701, 0.3701,\n",
       "           0.3701, 0.3016, 0.3701, 0.3016, 0.3016, 0.3016, 0.3701, 0.3016, 0.3016,\n",
       "           0.3701, 0.3016, 0.3701, 0.3701, 0.3016, 0.3016, 0.3701, 0.3701, 0.3016,\n",
       "           0.3016, 0.3701, 0.3701, 0.3701, 0.3701, 0.3016, 0.3701, 0.3370, 0.3370,\n",
       "           0.3370, 0.3370, 0.3792, 0.3370, 0.3370, 0.3792, 0.3396, 0.3317, 0.3646,\n",
       "           0.3534, 0.3232, 0.3050, 0.2994, 0.2949, 0.2921, 0.2858, 0.2819, 0.2792,\n",
       "           0.2747, 0.2743, 0.2737, 0.2718, 0.2713, 0.2806, 0.2783, 0.2782, 0.2793,\n",
       "           0.2810, 0.2717, 0.2814, 0.2726, 0.2844, 0.2753, 0.2864, 0.2785, 0.2922,\n",
       "           0.2819, 0.2975, 0.2878, 0.3048, 0.2926, 0.3048, 0.3048, 0.2963, 0.3048,\n",
       "           0.3048, 0.3048, 0.3048, 0.3048, 0.3084, 0.3048, 0.3122, 0.3151, 0.3048,\n",
       "           0.3219, 0.3253, 0.3255, 0.3270, 0.3270, 0.3048, 0.3270, 0.3270, 0.3048,\n",
       "           0.3270, 0.3048, 0.3048, 0.3270, 0.3048, 0.3048, 0.3048, 0.3270, 0.3270,\n",
       "           0.3048, 0.3270, 0.3048, 0.3270, 0.3048, 0.3048, 0.3270, 0.3048, 0.3270,\n",
       "           0.3048, 0.3270, 0.3048, 0.3048, 0.4674, 0.4503, 0.4401, 0.3400, 0.4230,\n",
       "           0.3331, 0.3296, 0.3264, 0.3173, 0.3554, 0.3187, 0.3088, 0.3114, 0.3012,\n",
       "           0.2979, 0.3003, 0.2969, 0.2887, 0.2859, 0.2873, 0.2803, 0.2800, 0.2784,\n",
       "           0.2837, 0.2785, 0.2796, 0.2788, 0.2733, 0.2733, 0.2678, 0.2664, 0.2746,\n",
       "           0.2738, 0.2636, 0.2758, 0.2744, 0.2748, 0.2650, 0.2768, 0.2648, 0.2624,\n",
       "           0.2654, 0.2733, 0.2647, 0.2786, 0.2652, 0.2654, 0.2824, 0.2809, 0.2663,\n",
       "           0.2717, 0.2693, 0.2693, 0.2736, 0.2725, 0.2738, 0.2768, 0.2768, 0.2938,\n",
       "           0.2938, 0.2938, 0.2938, 0.2768, 0.2938, 0.2938, 0.2938, 0.2768, 0.2768,\n",
       "           0.2768, 0.2938, 0.2768, 0.2768, 0.2938, 0.2768, 0.2938, 0.2768, 0.3342,\n",
       "           0.3372, 0.3342, 0.3372, 0.3342, 0.3372, 0.3372, 0.3372, 0.3371, 0.3124,\n",
       "           0.3026, 0.3212, 0.3165, 0.3111, 0.3025, 0.3012, 0.2988, 0.2934, 0.2914,\n",
       "           0.2895, 0.2888, 0.2870, 0.2785, 0.2880, 0.2794, 0.2798, 0.2784, 0.2748,\n",
       "           0.2753, 0.2819, 0.2802, 0.2722, 0.2708, 0.2657, 0.2768, 0.2651, 0.2774,\n",
       "           0.2668, 0.2774, 0.2671, 0.2758, 0.2659, 0.2792, 0.2772, 0.2642, 0.2788,\n",
       "           0.2676, 0.2832, 0.2826, 0.2645, 0.2700, 0.2786, 0.2695, 0.2829, 0.2879,\n",
       "           0.2695, 0.2834, 0.2856, 0.2675, 0.2675, 0.2675, 0.2675, 0.2950, 0.2950,\n",
       "           0.2675, 0.2950, 0.2950, 0.2950, 0.2675, 0.2675, 0.2950, 0.2675, 0.2950,\n",
       "           0.2675, 0.2675, 0.2675, 0.2950, 0.2950, 0.2675, 0.2675, 0.2675, 0.2950,\n",
       "           0.2675, 0.2950, 0.2675, 0.2950, 0.2675, 0.2675, 0.2950, 0.2675, 0.2675,\n",
       "           0.2675, 0.2675, 0.2950, 0.2950, 0.2675, 0.2675, 0.2950, 0.4397, 0.4170,\n",
       "           0.3202, 0.3932, 0.3227, 0.3240, 0.3066, 0.3105, 0.2992, 0.3058, 0.2980,\n",
       "           0.3065, 0.3008, 0.3022, 0.2925, 0.3030, 0.2924, 0.2913, 0.3004, 0.2901,\n",
       "           0.2999, 0.2988, 0.2979, 0.2880, 0.2891, 0.3005, 0.2996, 0.2913, 0.2899,\n",
       "           0.2996, 0.3002, 0.3008, 0.3030, 0.2911, 0.2888, 0.2875, 0.3041, 0.3029,\n",
       "           0.3021, 0.3065, 0.2897, 0.2883, 0.3090, 0.2939, 0.2950, 0.3084, 0.2916,\n",
       "           0.3084, 0.2916, 0.2916, 0.3084]),\n",
       "   tensor([0.3347, 0.3682, 0.3347,  ..., 0.2350, 0.2302, 0.2355]),\n",
       "   tensor([0.7914, 0.5199, 0.5199,  ..., 0.3147, 0.2958, 0.3147]),\n",
       "   tensor([0.4321, 0.2589, 0.4321,  ..., 0.2346, 0.2261, 0.2306])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([0.9081], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([0.3415], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-2.1606], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-1.0897], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-1.4299], grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.1770], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([0.5860], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([0.4656], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([0.3680], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.3491], grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.2849]),\n",
       "   tensor([0.2683]),\n",
       "   tensor([0.2642]),\n",
       "   tensor([0.3741]),\n",
       "   tensor([0.4566])]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "class SurfaceBatchNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features=1, \n",
    "        momentum=0.1\n",
    "    ):\n",
    "        super(SurfaceBatchNorm, self).__init__()\n",
    "        self.log_moneyness_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.time_to_maturity_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_return_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_volatility_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.treasury_rate_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.iv_mean_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.iv_std_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Concatenate all tensors from the Input Surface into one tensor for each feature\n",
    "        input_surface_log_moneyness = torch.cat([x for x in batch['Input Surface']['Log Moneyness']])\n",
    "        input_surface_time_to_maturity = torch.cat([x for x in batch['Input Surface']['Time to Maturity']])\n",
    "\n",
    "        # Concatenate Input Surface tensors with Query Points tensors\n",
    "        total_log_moneyness = torch.cat([input_surface_log_moneyness] + [x for x in batch['Query Points']['Log Moneyness']])\n",
    "        total_time_to_maturity = torch.cat([input_surface_time_to_maturity] + [x for x in batch['Query Points']['Time to Maturity']])\n",
    "\n",
    "        # Normalize Log Moneyness and Time to Maturity\n",
    "        norm_log_moneyness = self.log_moneyness_bn(total_log_moneyness.unsqueeze(1)).squeeze(1)\n",
    "        norm_time_to_maturity = self.time_to_maturity_bn(total_time_to_maturity.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Split the normalized results back to corresponding structures\n",
    "        input_surface_sizes = [len(x) for x in batch['Input Surface']['Log Moneyness']]\n",
    "        query_points_sizes = [len(x) for x in batch['Query Points']['Log Moneyness']]\n",
    "        total_input_size = sum(input_surface_sizes)\n",
    "\n",
    "        # Normalizing Market Features\n",
    "        market_features = batch['Market Features']\n",
    "        norm_market_return = self.market_return_bn(market_features['Market Return'].unsqueeze(1)).squeeze(1)\n",
    "        norm_market_volatility = self.market_volatility_bn(market_features['Market Volatility'].unsqueeze(1)).squeeze(1)\n",
    "        norm_treasury_rate = self.treasury_rate_bn(market_features['Treasury Rate'].unsqueeze(1)).squeeze(1)\n",
    "        norm_iv_mean = self.iv_mean_bn(market_features['IV Mean'].unsqueeze(1)).squeeze(1)\n",
    "        norm_iv_std = self.iv_std_bn(market_features['IV Std.'].unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Reconstructing the batch with normalized data\n",
    "        output = {\n",
    "            'Datetime': batch['Datetime'],\n",
    "            'Symbol': batch['Symbol'],\n",
    "            'Mask Proportion': batch['Mask Proportion'],\n",
    "            'Market Features': {\n",
    "                'Market Return': norm_market_return,\n",
    "                'Market Volatility': norm_market_volatility,\n",
    "                'Treasury Rate': norm_treasury_rate,\n",
    "                'IV Mean': norm_iv_mean,\n",
    "                'IV Std.': norm_iv_std\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[:total_input_size], input_surface_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[:total_input_size], input_surface_sizes)),\n",
    "                'Implied Volatility': batch['Input Surface']['Implied Volatility']\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[total_input_size:], query_points_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[total_input_size:], query_points_sizes)),\n",
    "                'Implied Volatility': batch['Query Points']['Implied Volatility']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Ensure requires_grad is True for query point values\n",
    "        # for key in output['Query Points']:\n",
    "        #     if key != 'Implied Volatility':  # We only set requires_grad for Log Moneyness and Time to Maturity\n",
    "        #         for tensor in output['Query Points'][key]:\n",
    "        #             tensor.requires_grad_()\n",
    "\n",
    "        return output\n",
    "\n",
    "# Usage\n",
    "surfacebatchnorm = SurfaceBatchNorm()\n",
    "processed_batch = surfacebatchnorm(batch)\n",
    "processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class EllipticalRBFKernel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        bandwidth, \n",
    "        remove_kernel=False\n",
    "    ):\n",
    "        super(EllipticalRBFKernel, self).__init__()\n",
    "        self.bandwidth = bandwidth\n",
    "        # Initialize the log of the scale vector to zero, which corresponds to scale factors of one\n",
    "        self.log_scale = nn.Parameter(torch.zeros(input_dim))\n",
    "        self.remove_kernel = remove_kernel\n",
    "\n",
    "    def forward(self, distances):\n",
    "        if self.remove_kernel:\n",
    "            # Create a mask for the condition check\n",
    "            all_zeros = torch.all(distances==0.0, dim=-1)\n",
    "            result = torch.where(\n",
    "                all_zeros, \n",
    "                torch.full(distances.shape[:-1], 1.0, device=distances.device),\n",
    "                torch.full(distances.shape[:-1], 1e-10, device=distances.device)\n",
    "            )\n",
    "            return result\n",
    "        # Convert log scale to actual scale values\n",
    "        scale = torch.exp(self.log_scale)\n",
    "        \n",
    "        # Calculate the scaled distances\n",
    "        scaled_distances = (distances ** 2) * scale  # Element-wise multiplication by scale\n",
    "\n",
    "        # Normalize by the trace of the scale matrix\n",
    "        trace_scale_matrix = torch.sum(scale)\n",
    "        normalized_distances = torch.sum(scaled_distances, dim=-1) / trace_scale_matrix\n",
    "\n",
    "        # Compute the RBF kernel output using the normalized distances\n",
    "        kernel_values = torch.exp(-normalized_distances / (2 * self.bandwidth ** 2))\n",
    "\n",
    "        return kernel_values\n",
    "\n",
    "class SurfaceContinuousKernelPositionalEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceContinuousKernelPositionalEmbedding, self).__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "        self.remove_positional_embedding = remove_positional_embedding\n",
    "\n",
    "        # Initialize multiple RBF kernels, each with a different fixed bandwidth\n",
    "        self.kernels = nn.ModuleList()\n",
    "        for i in range(1, d_embedding + 1):\n",
    "            bandwidth_value = torch.erfinv(torch.tensor(i / (d_embedding + 1))) * np.sqrt(2)\n",
    "            self.kernels.append(\n",
    "                EllipticalRBFKernel(\n",
    "                    bandwidth=bandwidth_value, \n",
    "                    input_dim=2, \n",
    "                    remove_kernel=remove_kernel\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.input_surface_layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.query_points_layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "        # Initialize learnable scaling parameter (the base for positional embedding)\n",
    "        self.log_scale = nn.Parameter(torch.log(torch.tensor(10000.0)))\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_surface_batch, \n",
    "        query_points_batch\n",
    "    ):\n",
    "        batch_size = len(input_surface_batch['Log Moneyness'])\n",
    "\n",
    "        input_surface_embeddings = []\n",
    "        query_points_embeddings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Extract the coordinates and implied volatilities for each surface in the batch\n",
    "            surface_coords = torch.stack([\n",
    "                input_surface_batch['Log Moneyness'][i], \n",
    "                input_surface_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "            surface_ivs = input_surface_batch['Implied Volatility'][i]\n",
    "\n",
    "            query_coords = torch.stack([\n",
    "                query_points_batch['Log Moneyness'][i], \n",
    "                query_points_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "\n",
    "            all_coords = torch.cat((surface_coords, query_coords), dim=0)\n",
    "\n",
    "            # Compute the pairwise differences between all points and the input surface points\n",
    "            point_differences = all_coords.unsqueeze(1) - surface_coords.unsqueeze(0)  # (n+m, n, 2)\n",
    "\n",
    "            # Initialize the output embeddings for the current surface with d_embedding channels\n",
    "            all_embedded = torch.zeros((all_coords.shape[0], self.d_embedding), dtype=torch.float32, device=surface_coords.device)\n",
    "\n",
    "            for kernel_idx, kernel in enumerate(self.kernels):\n",
    "                # Apply the RBF kernel to each distance vector \n",
    "                kernel_outputs = kernel(point_differences)\n",
    "\n",
    "                # Compute the weighted sum of IVs based on the kernel outputs\n",
    "                weighted_sum = (kernel_outputs * surface_ivs.unsqueeze(0)).sum(dim=1)\n",
    "                normalization_factor = kernel_outputs.sum(dim=1)\n",
    "\n",
    "                all_embedded[:, kernel_idx] = weighted_sum / normalization_factor    \n",
    "\n",
    "            # Split the embeddings into input surface and query points embeddings\n",
    "            input_surface_embedded = all_embedded[:surface_coords.shape[0], :]\n",
    "            query_points_embedded = all_embedded[surface_coords.shape[0]:, :]\n",
    "\n",
    "            # Normalize the embedded surfaces\n",
    "            input_surface_embedded = self.input_surface_layer_norm(input_surface_embedded)\n",
    "            query_points_embedded = self.query_points_layer_norm(query_points_embedded)\n",
    "\n",
    "            # Positional embedding for input surface points\n",
    "            input_surface_pe = self._compute_positional_embedding(surface_coords)\n",
    "\n",
    "            # Positional embedding for query points\n",
    "            query_points_pe = self._compute_positional_embedding(query_coords)\n",
    "\n",
    "            # Add positional embeddings with a factor of sqrt(2)\n",
    "            input_surface_final = input_surface_embedded + input_surface_pe * np.sqrt(2)\n",
    "            query_points_final = query_points_embedded + query_points_pe * np.sqrt(2)\n",
    "\n",
    "            # Append the encoded surface for this input surface to the batch list\n",
    "            input_surface_embeddings.append(input_surface_final)\n",
    "            query_points_embeddings.append(query_points_final)\n",
    "\n",
    "        # Keep all encoded surfaces as lists to handle variable lengths\n",
    "        return {\n",
    "            'Input Surface': input_surface_embeddings,\n",
    "            'Query Points': query_points_embeddings\n",
    "        }\n",
    "\n",
    "    def _compute_positional_embedding(\n",
    "        self, \n",
    "        coords, \n",
    "    ):\n",
    "        positional_embedding = torch.zeros(coords.size(0), self.d_embedding, device=coords.device)\n",
    "\n",
    "        if not self.remove_positional_embedding:\n",
    "            for i in range(self.d_embedding // 4):\n",
    "                div_factor = torch.exp(self.log_scale) ** (4 * i / self.d_embedding)\n",
    "                positional_embedding[:, 4 * i] = torch.sin(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 1] = torch.cos(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 2] = torch.sin(coords[:, 1] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 3] = torch.cos(coords[:, 1] / div_factor)\n",
    "\n",
    "        return positional_embedding\n",
    "\n",
    "# Example of initializing and using this module\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "\n",
    "# continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "# kernel_positional_embedded_batch = continuous_kernel_positional_embedding(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "# kernel_positional_embedded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SurfaceEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        momentum=0.1,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceEmbedding, self).__init__()\n",
    "        self.batch_norm = SurfaceBatchNorm(num_features=1, momentum=momentum)\n",
    "        self.kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding, remove_kernel, remove_positional_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.mask_token = nn.Parameter(torch.randn(d_embedding))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Apply batch normalization\n",
    "        norm_batch = self.batch_norm(batch)\n",
    "\n",
    "        # Extract market features from processed batch and create external_features_batch tensor\n",
    "        market_features = norm_batch['Market Features']\n",
    "        external_features_batch = torch.stack([\n",
    "            market_features['Market Return'],\n",
    "            market_features['Market Volatility'],\n",
    "            market_features['Treasury Rate'],\n",
    "            market_features['IV Mean'],\n",
    "            market_features['IV Std.']\n",
    "        ], dim=-1)  # (batch, features)\n",
    "\n",
    "        # Compute kernel and positional embeddings\n",
    "        embeddings = self.kernel_positional_embedding(norm_batch['Input Surface'], norm_batch['Query Points'])\n",
    "\n",
    "        input_surface_embeddings = embeddings['Input Surface']\n",
    "        query_points_embeddings = embeddings['Query Points']\n",
    "\n",
    "        embedded_sequences = []\n",
    "\n",
    "        for input_surface_embedding, query_points_embedding in zip(input_surface_embeddings, query_points_embeddings):\n",
    "            # Add mask token to the query point embeddings\n",
    "            masked_query_points_embedding = query_points_embedding + self.mask_token\n",
    "\n",
    "            # Combine input surface embeddings and masked query points embeddings\n",
    "            combined_sequence = torch.cat((input_surface_embedding, masked_query_points_embedding), dim=0)\n",
    "\n",
    "            # Apply layer normalization\n",
    "            combined_sequence = self.layer_norm(combined_sequence)\n",
    "\n",
    "            embedded_sequences.append(combined_sequence)\n",
    "\n",
    "        return embedded_sequences, external_features_batch\n",
    "\n",
    "\n",
    "# # Example of initializing and using this module\n",
    "# d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "# surface_embedding = SurfaceEmbedding(d_embedding=d_embedding)\n",
    "# embedded_sequences_batch, external_features_batch = surface_embedding(batch)\n",
    "# embedded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualNorm(nn.Module):\n",
    "    def __init__(self, d_embedding):\n",
    "        super(ResidualNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x, \n",
    "        sublayer_output\n",
    "    ):\n",
    "        return self.norm(x + sublayer_output)\n",
    "    \n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        gate_dropout,\n",
    "        weight_initializer_std=0.02,\n",
    "        bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(GatedAttentionFusion, self).__init__()\n",
    "        self.gate_layer = nn.Sequential(\n",
    "            nn.Linear(d_embedding * 2, d_embedding),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(gate_dropout)\n",
    "        )\n",
    "        self.remove_external_attention = remove_external_attention\n",
    "        self.remove_gate = remove_gate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for module in self.gate_layer:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        self_attention_output, \n",
    "        external_attention_output\n",
    "    ):\n",
    "        if self.remove_external_attention:\n",
    "\n",
    "            return self_attention_output\n",
    "\n",
    "        if self.remove_gate:  \n",
    "\n",
    "            return self_attention_output + external_attention_output\n",
    "        # Concatenate self-attention and external attention outputs\n",
    "        concatenated_output = torch.cat((self_attention_output, external_attention_output), dim=-1)\n",
    "        # Compute gate values\n",
    "        gate_values = self.gate_layer(concatenated_output)\n",
    "        # Calculate gated embedding\n",
    "        gated_embedding = gate_values * self_attention_output + (1 - gate_values) * external_attention_output\n",
    "\n",
    "        return gated_embedding\n",
    "    \n",
    "    \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        ffn_hidden_dim, \n",
    "        ffn_dropout, \n",
    "        layer_depth, \n",
    "        weight_initializer_std=0.02, \n",
    "        bias_initializer_value=0,\n",
    "    ):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_embedding, ffn_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ffn_dropout),\n",
    "            nn.Linear(ffn_hidden_dim, d_embedding),\n",
    "            nn.Dropout(ffn_dropout)\n",
    "        )\n",
    "\n",
    "        self.layer_depth = layer_depth\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feedforward(x)\n",
    "    \n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for i, module in enumerate(self.feedforward):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "                \n",
    "                # Rescale the output matrices of the last linear projection\n",
    "                if i == len(self.feedforward) - 2:\n",
    "                    scale_factor = 1 / (2 * self.layer_depth) ** 0.5\n",
    "                    module.weight.data *= scale_factor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        n_heads, \n",
    "        ffn_hidden_dim, \n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        layer_depth,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.residual_norm_self_attention = ResidualNorm(d_embedding)\n",
    "        self.external_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            kdim=external_dim, \n",
    "            vdim=external_dim, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.residual_norm_external_attention = ResidualNorm(d_embedding)\n",
    "        self.gated_attention_fusion = GatedAttentionFusion(\n",
    "            d_embedding, \n",
    "            gate_dropout,\n",
    "            weight_initializer_std,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention, \n",
    "            remove_gate,\n",
    "        )\n",
    "        self.residual_norm_fusion = ResidualNorm(d_embedding)\n",
    "        self.feed_forward = FeedForwardNetwork(\n",
    "            d_embedding, \n",
    "            ffn_hidden_dim, \n",
    "            ffn_dropout, \n",
    "            layer_depth, \n",
    "            weight_initializer_std, \n",
    "            linear_bias_initializer_value\n",
    "        )\n",
    "        self.residual_norm_ffn = ResidualNorm(d_embedding)\n",
    "        # Initialize self-attention\n",
    "        self._initialize_attention_weights(self.self_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "        # Initialize external-attention\n",
    "        self._initialize_attention_weights(self.external_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "\n",
    "    def _initialize_attention_weights(\n",
    "        self, \n",
    "        attention_module, \n",
    "        weight_initializer_std, \n",
    "        linear_bias_initializer_value, \n",
    "        layer_depth\n",
    "    ):\n",
    "        if attention_module._qkv_same_embed_dim:\n",
    "            nn.init.normal_(attention_module.in_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "        else:\n",
    "            nn.init.normal_(attention_module.q_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.k_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.v_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "\n",
    "        if attention_module.in_proj_bias is not None:\n",
    "            nn.init.constant_(attention_module.in_proj_bias, linear_bias_initializer_value)\n",
    "            nn.init.constant_(attention_module.out_proj.bias, linear_bias_initializer_value)\n",
    "        \n",
    "        if attention_module.bias_k is not None:\n",
    "            nn.init.constant_(attention_module.bias_k, linear_bias_initializer_value)\n",
    "        if attention_module.bias_v is not None:\n",
    "            nn.init.constant_(attention_module.bias_v, linear_bias_initializer_value)\n",
    "        \n",
    "        # Transformer layer rescaling for output weights\n",
    "        scale_factor = 1 / (2 * layer_depth) ** 0.5\n",
    "        nn.init.normal_(attention_module.out_proj.weight, mean=0.0, std=weight_initializer_std * scale_factor)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        surface_embeddings, \n",
    "        external_features,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Self-Attention\n",
    "        self_attention_output, self_attention_weights = self.self_attention(surface_embeddings, surface_embeddings, surface_embeddings)\n",
    "        self_attention_output = self.residual_norm_self_attention(surface_embeddings, self_attention_output)\n",
    "        # External Attention\n",
    "        external_attention_output, external_attention_weights = self.external_attention(surface_embeddings, external_features, external_features) \n",
    "        external_attention_output = self.residual_norm_external_attention(surface_embeddings, external_attention_output)\n",
    "        # Gated Attention Fusion\n",
    "        gated_embedding = self.gated_attention_fusion(self_attention_output, external_attention_output)\n",
    "        gated_embedding = self.residual_norm_fusion(surface_embeddings, gated_embedding)\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.feed_forward(gated_embedding)\n",
    "        # Final Residual Connection and Layer Normalization\n",
    "        surface_embeddings = self.residual_norm_ffn(gated_embedding, ffn_output)\n",
    "\n",
    "        if output_attention_map:\n",
    "            # Remove the batch dimension for attention weights\n",
    "            return surface_embeddings, self_attention_weights.squeeze(0), external_attention_weights.squeeze(0)\n",
    "        \n",
    "        return surface_embeddings, None, None\n",
    "\n",
    "class SurfaceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(SurfaceEncoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            Encoder(\n",
    "                d_embedding, \n",
    "                n_heads, \n",
    "                ffn_hidden_dim, \n",
    "                attention_dropout, \n",
    "                gate_dropout,\n",
    "                ffn_dropout,\n",
    "                external_dim,\n",
    "                (i + 1),\n",
    "                weight_initializer_std,\n",
    "                linear_bias_initializer_value,\n",
    "                gate_bias_initializer_value,\n",
    "                remove_external_attention,\n",
    "                remove_gate\n",
    "            )\n",
    "            for i in range(num_encoder_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        embedded_sequences_batch, \n",
    "        external_features_batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        batch_size = len(embedded_sequences_batch)\n",
    "        encoded_sequences_batch = []\n",
    "        self_attention_maps = []\n",
    "        external_attention_maps = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            surface_embeddings = embedded_sequences_batch[i].unsqueeze(1) \n",
    "            external_features = external_features_batch[i].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            for j, encoder in enumerate(self.encoders):\n",
    "                if j == len(self.encoders) - 1 and output_attention_map:\n",
    "                    surface_embeddings, self_attention_map, external_attention_map = encoder(surface_embeddings, external_features, output_attention_map)\n",
    "                    \n",
    "                else:\n",
    "                    surface_embeddings, _, _ = encoder(surface_embeddings, external_features)\n",
    "                \n",
    "            encoded_sequences_batch.append(surface_embeddings.squeeze(1))\n",
    "            if output_attention_map:\n",
    "                self_attention_maps.append(self_attention_map)\n",
    "                external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return encoded_sequences_batch, self_attention_maps, external_attention_maps\n",
    "        \n",
    "        return encoded_sequences_batch, None, None    \n",
    "\n",
    "# Example of initializing and using these modules\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "external_dim = 5\n",
    "\n",
    "surface_encoder = SurfaceEncoder(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim, \n",
    "    attention_dropout, \n",
    "    gate_dropout, \n",
    "    ffn_dropout, \n",
    "    external_dim, \n",
    ")\n",
    "\n",
    "# Assume embedded_sequences_batch is the output of the SurfaceEmbedding module and\n",
    "# external_features is the formatted external market features batch\n",
    "# encoded_sequences_batch, self_attention_map_batch, external_attention_map_batch = surface_encoder(embedded_sequences_batch, external_features_batch)\n",
    "# encoded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IvySPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0163], grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0171], grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0093], grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0083], grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0112], grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class IvySPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(IvySPT, self).__init__()\n",
    "        self.surface_embedding = SurfaceEmbedding(\n",
    "            d_embedding, \n",
    "            remove_kernel, \n",
    "            remove_positional_embedding\n",
    "        )\n",
    "        self.surface_encoder = SurfaceEncoder(\n",
    "            d_embedding, \n",
    "            num_encoder_blocks,\n",
    "            n_heads, \n",
    "            ffn_hidden_dim,\n",
    "            attention_dropout, \n",
    "            gate_dropout,\n",
    "            ffn_dropout,\n",
    "            external_dim,\n",
    "            weight_initializer_std,\n",
    "            linear_bias_initializer_value,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention,\n",
    "            remove_gate\n",
    "        )\n",
    "        self.final_layer = nn.Linear(d_embedding, 1)\n",
    "        nn.init.normal_(self.final_layer.weight, mean=0.0, std=weight_initializer_std * (1 / (2 * (num_encoder_blocks + 1)) ** 0.5))\n",
    "        nn.init.constant_(self.final_layer.bias, linear_bias_initializer_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Obtain the embedded sequences and external features from the SurfaceEmbedding module\n",
    "        embedded_sequences_batch, external_features_batch = self.surface_embedding(batch)\n",
    "\n",
    "        # Encode the sequences using the SurfaceEncoder module\n",
    "        encoded_sequences_batch, self_attention_maps, external_attention_maps = self.surface_encoder(\n",
    "            embedded_sequences_batch, \n",
    "            external_features_batch, \n",
    "            output_attention_map\n",
    "        )\n",
    "\n",
    "        # List to hold the implied volatility estimates for each query point in the batch\n",
    "        iv_estimates_batch = []\n",
    "\n",
    "        query_self_attention_maps = []\n",
    "        query_external_attention_maps = []\n",
    "\n",
    "        for i in range(len(encoded_sequences_batch)):\n",
    "            # Extract the encoded sequence\n",
    "            encoded_sequence = encoded_sequences_batch[i]\n",
    "\n",
    "            # Determine the number of query points for this sequence\n",
    "            num_query_points = len(batch['Query Points']['Log Moneyness'][i])\n",
    "\n",
    "            # Extract the encoded query points (last num_query_points elements in the sequence)\n",
    "            encoded_query_points = encoded_sequence[-num_query_points:]\n",
    "\n",
    "            # Estimate the implied volatility for each query point using the fully connected layer\n",
    "            iv_estimates = self.final_layer(encoded_query_points).squeeze(-1)\n",
    "\n",
    "            # Append the estimates to the batch list\n",
    "            iv_estimates_batch.append(iv_estimates)\n",
    "\n",
    "            if output_attention_map:\n",
    "                # Extract the attention maps for the query points\n",
    "                self_attention_map = self_attention_maps[i][-num_query_points:]\n",
    "                external_attention_map = external_attention_maps[i][-num_query_points:]\n",
    "\n",
    "                query_self_attention_maps.append(self_attention_map)\n",
    "                query_external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return iv_estimates_batch, self_attention_maps, external_attention_maps\n",
    "        \n",
    "        return iv_estimates_batch, None, None\n",
    "\n",
    "# Example of initializing and using this module\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "external_dim = 5\n",
    "\n",
    "ivy_spt = IvySPT(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim,\n",
    "    attention_dropout, \n",
    "    gate_dropout,\n",
    "    ffn_dropout,\n",
    "    external_dim\n",
    ")\n",
    "\n",
    "# Pass the batch through the IvySPT model to get implied volatility estimates\n",
    "iv_estimates_batch, self_attention_maps, external_attention_maps = ivy_spt(batch, output_attention_map=False)\n",
    "gc.collect()\n",
    "iv_estimates_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.2849]),\n",
       " tensor([0.2683]),\n",
       " tensor([0.2642]),\n",
       " tensor([0.3741]),\n",
       " tensor([0.4566])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['Query Points']['Implied Volatility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10647381860761795, 0.10647381842136383, 1.862541232355852e-10, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SurfaceArbitrageFreeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SurfaceArbitrageFreeLoss, self).__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        iv_estimates_batch, \n",
    "        batch,\n",
    "        testing_mode=False\n",
    "    ):\n",
    "        mse_loss_sum = 0.0\n",
    "        calendar_arbitrage_loss_sum = 0.0\n",
    "        butterfly_arbitrage_loss_sum = 0.0\n",
    "        total_elements = 0\n",
    "        loss_records = []\n",
    "\n",
    "        for iv_estimates, target_volatility, time_to_maturity, log_moneyness in zip(\n",
    "            iv_estimates_batch, \n",
    "            batch['Query Points']['Implied Volatility'], \n",
    "            batch['Query Points']['Time to Maturity'], \n",
    "            batch['Query Points']['Log Moneyness']\n",
    "        ):\n",
    "            sequence_length = iv_estimates.size(0)\n",
    "            total_elements += sequence_length\n",
    "\n",
    "            # Calculate mean squared error between model estimates and target volatilities\n",
    "            mse_loss = F.mse_loss(iv_estimates, target_volatility, reduction='sum')\n",
    "            mse_loss_sum += mse_loss.item()\n",
    "\n",
    "            # Calculate the total implied variance\n",
    "            total_implied_variance = time_to_maturity * iv_estimates.pow(2)\n",
    "\n",
    "            unit_vectors = torch.eye(sequence_length)\n",
    "\n",
    "            # Compute gradients needed for arbitrage conditions\n",
    "            w_t = torch.stack([\n",
    "                torch.autograd.grad(\n",
    "                    outputs=total_implied_variance, \n",
    "                    inputs=time_to_maturity,\n",
    "                    grad_outputs=vec, \n",
    "                    create_graph=True   \n",
    "                )[0]\n",
    "                for vec in unit_vectors\n",
    "            ]).diag()\n",
    "\n",
    "            w_x = torch.stack([\n",
    "                torch.autograd.grad(\n",
    "                    outputs=total_implied_variance, \n",
    "                    inputs=log_moneyness,\n",
    "                    grad_outputs=vec, \n",
    "                    create_graph=True   \n",
    "                )[0]\n",
    "                for vec in unit_vectors\n",
    "            ]).diag()\n",
    "\n",
    "            w_xx = torch.stack([\n",
    "                torch.autograd.grad(\n",
    "                    outputs=w_x, \n",
    "                    inputs=log_moneyness, \n",
    "                    grad_outputs=vec,\n",
    "                    create_graph=True   \n",
    "                )[0]\n",
    "                for vec in unit_vectors\n",
    "            ]).diag()\n",
    "\n",
    "            # Calculate Calendar Arbitrage Loss\n",
    "            calendar_arbitrage_loss = torch.clamp(-w_t, min=0) ** 2\n",
    "            calendar_arbitrage_loss_sum += calendar_arbitrage_loss.sum().item()\n",
    "\n",
    "            # Calculate Butterfly Arbitrage Loss\n",
    "            w = total_implied_variance\n",
    "            g = (1 - log_moneyness * w_x / (2 * w)) ** 2 - w_x / 4 * (1 / w + 1 / 4) + w_xx / 2\n",
    "            butterfly_arbitrage_loss = torch.clamp(-g, min=0) ** 2\n",
    "            butterfly_arbitrage_loss_sum += butterfly_arbitrage_loss.sum().item()\n",
    "\n",
    "            if testing_mode:\n",
    "                record = {\n",
    "                    'MSE Loss': mse_loss.mean().item(),\n",
    "                    'Calendar Arbitrage Loss': calendar_arbitrage_loss.mean().item(),\n",
    "                    'Butterfly Arbitrage Loss': butterfly_arbitrage_loss.mean().item()\n",
    "                }\n",
    "                loss_records.append(record)\n",
    "\n",
    "        # Calculate mean losses\n",
    "        mse_loss = mse_loss_sum / total_elements\n",
    "        calendar_arbitrage_loss = calendar_arbitrage_loss_sum / total_elements\n",
    "        butterfly_arbitrage_loss = butterfly_arbitrage_loss_sum / total_elements\n",
    "\n",
    "        if testing_mode:\n",
    "            loss_records = pd.DataFrame(loss_records)\n",
    "            loss_records['Datetime'] = batch['Datetime']\n",
    "            loss_records['Mask Proportion'] = batch['Mask Proportion']\n",
    "            loss_records.set_index(['Datetime', 'Mask Proportion'], inplace=True)\n",
    "\n",
    "            return loss_records, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss\n",
    "\n",
    "        return None, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss\n",
    "\n",
    "surface_arbitrage_free_loss = SurfaceArbitrageFreeLoss()  \n",
    "loss_records, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss = surface_arbitrage_free_loss(iv_estimates_batch, batch)\n",
    "loss_records, mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AdaptiveLossCoefficients(nn.Module):\n",
    "    def __init__(self, initial_losses, alpha=1.0, lr=0.001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_losses (list of float): Initial losses for each task, used to initialize the loss weights.\n",
    "            alpha (float): Hyperparameter for controlling the restoring force of the GradNorm algorithm.\n",
    "            lr (float): Learning rate for the optimizer of the loss weights.\n",
    "        \"\"\"\n",
    "        super(AdaptiveLossCoefficients, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.initial_losses = torch.tensor(initial_losses, dtype=torch.float32)\n",
    "        self.loss_weights = nn.Parameter(torch.ones_like(self.initial_losses))\n",
    "        self.optimizer = torch.optim.Adam([self.loss_weights], lr=lr)\n",
    "\n",
    "    def forward(self, current_losses, layer_params):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            current_losses (torch.Tensor): Current losses for each task.\n",
    "            layer_params (iterable): Parameters of the specific layer where GradNorm is applied.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated loss weights.\n",
    "        \"\"\"\n",
    "        # Normalize the initial losses\n",
    "        if not hasattr(self, 'initial_loss_ratios'):\n",
    "            self.initial_loss_ratios = self.initial_losses / self.initial_losses.mean()\n",
    "\n",
    "        # Calculate the weighted loss\n",
    "        weighted_loss = torch.dot(self.loss_weights, current_losses)\n",
    "\n",
    "        # Backward pass for the weighted loss\n",
    "        weighted_loss.backward(retain_graph=True)\n",
    "\n",
    "        # Compute the gradient norms for each task\n",
    "        grad_norms = []\n",
    "        for i, loss in enumerate(current_losses):\n",
    "            grad_norm = torch.autograd.grad(loss, layer_params, retain_graph=True, create_graph=True)[0].norm()\n",
    "            grad_norms.append(grad_norm)\n",
    "        grad_norms = torch.stack(grad_norms)\n",
    "\n",
    "        # Compute the average gradient norm\n",
    "        avg_grad_norm = grad_norms.mean()\n",
    "\n",
    "        # Calculate the relative inverse training rate\n",
    "        current_loss_ratios = current_losses / self.initial_losses\n",
    "        relative_training_rate = current_loss_ratios / current_loss_ratios.mean()\n",
    "\n",
    "        # Compute the GradNorm loss\n",
    "        grad_norm_loss = torch.sum(torch.abs(grad_norms - (avg_grad_norm * (relative_training_rate ** self.alpha))))\n",
    "\n",
    "        # Optimize the loss weights\n",
    "        self.optimizer.zero_grad()\n",
    "        grad_norm_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Normalize the loss weights to maintain their sum\n",
    "        with torch.no_grad():\n",
    "            self.loss_weights.data = self.loss_weights.data / self.loss_weights.sum().detach()\n",
    "\n",
    "        return self.loss_weights.detach()\n",
    "\n",
    "# Example usage during training\n",
    "# adaptive_loss_module = AdaptiveLossCoefficients(initial_losses=[1.0, 1.0, 1.0], alpha=1.0, lr=0.001)\n",
    "# loss_weights = adaptive_loss_module(current_losses, layer_params)\n",
    "# https://github.com/LucasBoTang/GradNorm/blob/main/gradnorm.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
