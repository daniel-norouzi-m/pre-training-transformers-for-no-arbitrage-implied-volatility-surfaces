{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Batch Size' : 4\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 8,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 16,\n",
    "        'Attention Dropout' : 0.1,\n",
    "        'Gate Dropout' : 0.1,\n",
    "        'FFN Dropout' : 0.1,\n",
    "        'Number of Blocks' : 2,\n",
    "        'External Feature Dimension' : 3,\n",
    "    },\n",
    "    'No-Arbitrage' : {\n",
    "        'Butterfly' : 1,\n",
    "        'Calendar' : 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Log Moneyness</th>\n",
       "      <th>Time to Maturity</th>\n",
       "      <th>Implied Volatility</th>\n",
       "      <th>Market Return</th>\n",
       "      <th>Market Volatility</th>\n",
       "      <th>Treasury Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.316688</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.316688</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.304266</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.304266</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.291996</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-06-28</th>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.427518</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.434898</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.434898</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.442224</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.442224</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574326 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Log Moneyness  Time to Maturity  Implied Volatility  \\\n",
       "Datetime   Symbol                                                        \n",
       "2013-01-02 AAPL        -0.316688          0.007937              0.3726   \n",
       "           AAPL        -0.316688          0.007937              0.6095   \n",
       "           AAPL        -0.304266          0.007937              0.3726   \n",
       "           AAPL        -0.304266          0.007937              0.6095   \n",
       "           AAPL        -0.291996          0.007937              0.3726   \n",
       "...                          ...               ...                 ...   \n",
       "2013-06-28 GOOGL        0.427518          2.253968              0.2430   \n",
       "           GOOGL        0.434898          2.253968              0.2383   \n",
       "           GOOGL        0.434898          2.253968              0.2426   \n",
       "           GOOGL        0.442224          2.253968              0.2402   \n",
       "           GOOGL        0.442224          2.253968              0.2433   \n",
       "\n",
       "                   Market Return  Market Volatility  Treasury Rate  \n",
       "Datetime   Symbol                                                   \n",
       "2013-01-02 AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "...                          ...                ...            ...  \n",
       "2013-06-28 GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "\n",
       "[574326 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_googl_data = pd.read_csv('volatility_surface_AAPL_GOOGL_2013_01_2013_06.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "aapl_googl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': Timestamp('2013-01-02 00:00:00'),\n",
       " 'Symbol': 'AAPL',\n",
       " 'Market Features': {'Market Return': 0.0250861159586972,\n",
       "  'Market Volatility': 14.68000030517578,\n",
       "  'Treasury Rate': 0.0549999997019767},\n",
       " 'Surface': {'Log Moneyness': array([-0.31668849, -0.31668849, -0.30426597, ...,  0.63882295,\n",
       "          0.6483924 ,  0.6483924 ]),\n",
       "  'Time to Maturity': array([0.00793651, 0.00793651, 0.00793651, ..., 2.95634921, 2.95634921,\n",
       "         2.95634921]),\n",
       "  'Implied Volatility': array([0.3726, 0.6095, 0.3726, ..., 0.3387, 0.3342, 0.3389])}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def implied_volatility_surfaces(options_market_data):\n",
    "    # Group the data by Datetime and Symbol\n",
    "    grouped_data = options_market_data.groupby(level=['Datetime', 'Symbol'])\n",
    "\n",
    "    surfaces = []\n",
    "    for (date, symbol), surface in grouped_data:\n",
    "        surface_dict = {\n",
    "            'Datetime': date,\n",
    "            'Symbol': symbol,\n",
    "            'Market Features': {\n",
    "                'Market Return': surface['Market Return'].values[0],\n",
    "                'Market Volatility': surface['Market Volatility'].values[0],\n",
    "                'Treasury Rate': surface['Treasury Rate'].values[0],\n",
    "            },\n",
    "            'Surface': {\n",
    "                'Log Moneyness': surface['Log Moneyness'].values,\n",
    "                'Time to Maturity': surface['Time to Maturity'].values,\n",
    "                'Implied Volatility': surface['Implied Volatility'].values,\n",
    "            }\n",
    "        }\n",
    "        surfaces.append(surface_dict)\n",
    "\n",
    "    return surfaces\n",
    "\n",
    "surfaces = implied_volatility_surfaces(aapl_googl_data)\n",
    "surfaces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00'),\n",
       "  Timestamp('2013-03-07 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL', 'AAPL'],\n",
       " 'Market Features': {'Market Return': tensor([-0.0003, -0.0019, -0.0007,  0.0018]),\n",
       "  'Market Volatility': tensor([15.4400, 13.5700, 13.0200, 13.0600]),\n",
       "  'Treasury Rate': tensor([0.0400, 0.0600, 0.0350, 0.0900])},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-0.3159, -0.2852, -0.2553,  ...,  0.8627,  0.8723,  0.8723]),\n",
       "   tensor([-0.2509, -0.2509, -0.2368,  ...,  0.8381,  0.8477,  0.8477]),\n",
       "   tensor([-0.3741, -0.3661, -0.3582,  ...,  0.3886,  0.4034,  0.4107]),\n",
       "   tensor([-0.1790, -0.1790, -0.1652,  ...,  0.8818,  0.8914,  0.8914])],\n",
       "  'Time to Maturity': [tensor([0.0159, 0.0159, 0.0159,  ..., 2.3254, 2.3254, 2.3254]),\n",
       "   tensor([0.0159, 0.0159, 0.0159,  ..., 2.8532, 2.8532, 2.8532]),\n",
       "   tensor([0.0159, 0.0159, 0.0159,  ..., 2.4087, 2.4087, 2.4087]),\n",
       "   tensor([0.0040, 0.0040, 0.0040,  ..., 2.7024, 2.7024, 2.7024])],\n",
       "  'Implied Volatility': [tensor([0.4187, 0.3716, 0.3716,  ..., 0.3194, 0.3320, 0.3194]),\n",
       "   tensor([0.4101, 0.4484, 0.4101,  ..., 0.3084, 0.2916, 0.3084]),\n",
       "   tensor([0.3347, 0.3347, 0.3347,  ..., 0.2339, 0.2302, 0.2355]),\n",
       "   tensor([0.5199, 0.7914, 0.5199,  ..., 0.3147, 0.2958, 0.3147])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([ 0.7563,  0.2614,  0.4080,  0.2614,  0.7347,  0.4305,  0.7879,  0.4668,\n",
       "            0.3927,  0.1980,  0.7013,  0.3374,  0.1599,  0.7455,  0.1599,  0.1102,\n",
       "            0.5556,  0.2073,  0.4230,  0.1102,  0.4004,  0.1403,  0.1886,  0.3210,\n",
       "            0.1403,  0.0360,  0.3127,  0.0470,  0.1203,  0.6189,  0.1886,  0.0470,\n",
       "            0.2165,  0.0791,  0.8335,  0.7509,  0.4230,  0.7669,  0.4155,  0.3292,\n",
       "            0.6251,  0.1599,  0.5750,  0.1304,  0.1102,  0.1599,  0.3850,  0.0360,\n",
       "            0.0791,  0.2959,  0.7347,  0.7013,  0.0685,  0.2701,  0.0360,  0.6432,\n",
       "            0.3850,  0.1203,  0.1696,  0.0360,  0.2701,  0.6669,  0.2526,  0.0999,\n",
       "            0.2165,  0.4080,  0.3535,  0.3772,  0.2347,  0.1791,  0.0470,  0.2788,\n",
       "            0.6669,  0.2165,  0.5425,  0.2165,  0.1886,  0.2257,  0.0578,  0.4452,\n",
       "            0.0360,  0.2437,  0.3374,  0.2959,  0.2257,  0.0250,  0.5224,  0.5156,\n",
       "            0.7775,  0.2073,  0.0791,  0.6492,  0.2257,  0.2257,  0.1502,  0.6842,\n",
       "            0.0791,  0.6432,  0.1886,  0.4452,  0.0895,  0.1403,  0.5621,  0.5814,\n",
       "            0.1980,  0.6372,  0.2165,  0.7827,  0.3927,  0.0999,  0.1403,  0.6189,\n",
       "            0.6066,  0.6311,  0.1886,  0.2526,  0.4668,  0.2073,  0.6842,  0.7827,\n",
       "            0.0250,  0.7722,  0.0685,  0.3927,  0.7292,  0.3127,  0.1886,  0.2701,\n",
       "            0.0360,  0.0895,  0.1304,  0.2347,  0.1203,  0.2701,  0.3455,  0.4080,\n",
       "            0.0895,  0.1980,  0.5291,  0.2347,  0.2614,  0.0895,  0.8135,  0.1791,\n",
       "            0.6785,  0.4739,  0.2073,  0.1403,  0.3043,  0.1599,  0.2437,  0.2874,\n",
       "            0.3694,  0.7879,  0.0791,  0.2701,  0.2347,  0.6372,  0.2073,  0.6669,\n",
       "            0.0360,  0.3127,  0.2701,  0.2526,  0.1403,  0.1696,  0.2257,  0.3374,\n",
       "           -0.1054, -0.6462, -0.1181, -0.3805, -0.6036, -0.2263, -0.1310, -0.3477,\n",
       "            0.0250, -0.0322, -0.7136, -0.1310, -0.3317, -0.7612, -0.1707, -0.3159,\n",
       "           -0.0560,  0.0360, -0.3805, -0.5430, -0.2121, -0.5045, -0.6247, -0.3973,\n",
       "           -0.4858, -0.1981, -0.6681, -0.6906, -0.1844, -0.0560,  0.0138, -0.2553,\n",
       "           -0.0681, -0.1310, -0.2121, -0.0681, -0.0205, -0.1441, -0.1844, -0.3004,\n",
       "           -0.0681, -0.3973, -0.2852, -0.1441, -0.4495, -0.3317, -0.5236, -0.4318,\n",
       "           -0.1844, -0.1181, -0.3639, -0.1310, -0.3477, -0.7371, -0.7371, -0.3004,\n",
       "           -0.1707, -0.0440, -0.6906, -0.2701, -0.1981, -0.1844, -0.0440, -0.1844,\n",
       "           -0.4675, -0.2263, -0.6036, -0.0205, -0.3159, -0.1181, -0.0440, -0.3159,\n",
       "           -0.6036, -0.3004, -0.4144, -0.4675, -0.1441, -0.7136, -0.4144, -0.4318,\n",
       "           -0.6906,  0.0025, -0.3805, -0.3317, -0.4144, -0.7612, -0.6681, -0.6462,\n",
       "           -0.1441,  0.0138, -0.4858, -0.6036,  0.0025, -0.1573, -0.1441, -0.3639,\n",
       "           -0.6906,  0.0025,  0.0360, -0.2121, -0.5045, -0.6036, -0.0804, -0.3317,\n",
       "           -0.0928, -0.0089, -0.0205, -0.2553, -0.3004, -0.1573, -0.5628, -0.2407,\n",
       "           -0.1573, -0.5830, -0.1707, -0.7859, -0.1054, -0.4858, -0.1981, -0.1981,\n",
       "           -0.0560, -0.1310, -0.2553, -0.1181, -0.0440, -0.8112, -0.6036, -0.2701,\n",
       "           -0.1844, -0.1573, -0.1844,  0.0250, -0.3004,  0.0250, -0.6036, -0.2852,\n",
       "           -0.0560, -0.3477, -0.0440,  0.0025, -0.0440, -0.2263, -0.0440,  0.0025,\n",
       "           -0.0440, -0.5430, -0.6462, -0.2263, -0.3159, -0.5236, -0.0928,  0.0138,\n",
       "           -0.7136, -0.0681, -0.1310, -0.5236,  0.0025, -0.1844, -0.3639, -0.1707,\n",
       "           -0.0560, -0.3159, -0.0804, -0.7859, -0.0928, -0.4675, -0.3973, -0.0681,\n",
       "           -0.1310, -0.0322, -0.3805, -0.1181, -0.6462, -0.0804, -0.3805, -0.1707,\n",
       "           -0.4318, -0.5628, -0.6906,  0.5621,  0.7722,  0.7509,  0.5358,  0.8433,\n",
       "            0.8235,  0.1403,  0.2437,  0.0025,  0.1599,  0.4230,  0.7181,  0.1599,\n",
       "            0.4080,  0.4950,  0.8627,  0.0250,  0.3927,  0.0250,  0.0470,  0.5878,\n",
       "            0.7616,  0.7070,  0.1791,  0.3615, -0.0440,  0.7292,  0.0895,  0.1886,\n",
       "            0.4378,  0.8531,  0.5358,  0.1102,  0.4880,  0.6066,  0.2165,  0.6551,\n",
       "            0.2788,  0.1696,  0.4524,  0.1502,  0.2959,  0.5224,  0.0470,  0.4004,\n",
       "            0.3615,  0.3127,  0.5941,  0.1304,  0.6785,  0.3043,  0.7879,  0.6727,\n",
       "            0.5556,  0.1203,  0.0791,  0.6492,  0.7669,  0.7347,  0.7616,  0.1980,\n",
       "            0.7292,  0.5491,  0.2257,  0.8627,  0.3043,  0.4880,  0.4950,  0.2788,\n",
       "            0.3615,  0.2701,  0.2347,  0.2165,  0.1502,  0.3210,  0.6610,  0.7879,\n",
       "            0.6372,  0.5088,  0.3694,  0.4452,  0.8135,  0.7563,  0.0578,  0.6432,\n",
       "            0.5156,  0.4950,  0.6251,  0.8627,  0.2437,  0.2701,  0.1502,  0.4230,\n",
       "            0.2959,  0.3927,  0.1980,  0.4080,  0.5686,  0.7930,  0.1791,  0.4524,\n",
       "            0.8531,  0.7775,  0.3210,  0.3772,  0.7181,  0.3772,  0.5491,  0.6492,\n",
       "            0.4230,  0.6189,  0.5621,  0.7181], requires_grad=True),\n",
       "   tensor([ 1.4496e-01,  1.3530e-01, -1.0498e-01,  2.2357e-02,  1.1428e-02,\n",
       "           -5.6781e-02,  3.2888e-01,  2.1907e-01,  3.2085e-01,  1.1569e-01,\n",
       "            2.7125e-01,  2.1907e-01,  6.4916e-02,  3.1275e-01, -5.6781e-02,\n",
       "           -1.0795e-02,  2.5416e-01, -6.8615e-02,  3.7784e-04, -2.2095e-02,\n",
       "            7.5279e-02,  5.4445e-02,  9.5688e-02,  1.5453e-01,  5.4445e-02,\n",
       "           -2.2095e-02,  9.5688e-02, -4.5085e-02,  1.1428e-02,  1.7340e-01,\n",
       "           -2.2277e-01,  2.8806e-01,  1.2554e-01,  2.7125e-01,  2.7125e-01,\n",
       "            3.4476e-01, -5.6781e-02,  3.1275e-01,  1.0574e-01,  1.0574e-01,\n",
       "           -3.3524e-02,  1.1569e-01,  1.9192e-01,  2.1907e-01,  3.2888e-01,\n",
       "            2.2357e-02,  3.3168e-02,  1.9192e-01,  8.5536e-02,  2.9636e-01,\n",
       "            1.7340e-01,  1.8270e-01,  2.6274e-01, -1.0498e-01,  1.1569e-01,\n",
       "           -1.9537e-01, -1.1741e-01,  1.8270e-01,  3.3685e-01, -1.0498e-01,\n",
       "            1.1428e-02, -8.0591e-02,  2.8806e-01,  1.8270e-01,  7.5279e-02,\n",
       "            3.3168e-02,  2.8806e-01,  2.1010e-01,  3.3685e-01, -5.6781e-02,\n",
       "            1.2554e-01,  2.2357e-02, -1.0498e-01,  1.1428e-02,  2.5416e-01,\n",
       "            1.6401e-01, -2.0897e-01,  1.9192e-01,  7.5279e-02,  2.6274e-01,\n",
       "            3.2888e-01, -3.3524e-02,  9.5688e-02,  1.1428e-02,  1.1569e-01,\n",
       "           -1.0795e-02,  2.9636e-01, -4.5085e-02,  2.4550e-01,  1.9192e-01,\n",
       "            6.4916e-02,  4.3863e-02,  1.4496e-01,  3.7784e-04,  1.5453e-01,\n",
       "            2.7969e-01, -2.2095e-02,  1.3530e-01,  2.0105e-01,  3.0459e-01,\n",
       "            1.8270e-01, -8.0591e-02, -6.8615e-02,  2.5416e-01,  2.0105e-01,\n",
       "            2.0105e-01,  3.0459e-01,  2.3677e-01,  9.5688e-02, -2.2095e-02,\n",
       "           -1.6870e-01,  1.9192e-01,  7.5279e-02,  6.4916e-02,  3.7784e-04,\n",
       "            1.9192e-01, -1.6870e-01, -2.2095e-02,  1.1428e-02,  4.3863e-02,\n",
       "           -1.5563e-01,  1.4496e-01,  2.3677e-01, -3.3524e-02, -9.2713e-02,\n",
       "           -2.2095e-02,  1.5453e-01,  3.1275e-01,  3.0459e-01,  3.2085e-01,\n",
       "            2.9636e-01,  2.7969e-01,  1.4496e-01,  2.2796e-01, -5.6781e-02,\n",
       "            2.2357e-02,  9.5688e-02, -4.5085e-02,  2.9636e-01,  2.4550e-01,\n",
       "            1.8270e-01, -9.2713e-02,  2.1907e-01,  2.4550e-01,  1.7340e-01,\n",
       "           -2.2095e-02,  3.0459e-01,  1.0574e-01, -1.6870e-01,  5.4445e-02,\n",
       "            8.5536e-02, -1.2998e-01,  1.8270e-01,  1.6401e-01, -1.1741e-01,\n",
       "            2.0105e-01, -3.3524e-02, -9.2713e-02,  2.7969e-01,  2.7125e-01,\n",
       "            2.1010e-01,  1.1569e-01,  1.0574e-01, -1.8194e-01, -5.6781e-02,\n",
       "            9.5688e-02,  2.3677e-01,  1.3530e-01,  2.7125e-01, -1.1741e-01,\n",
       "           -1.4272e-01,  2.6274e-01,  2.5416e-01, -2.3675e-01, -1.6870e-01,\n",
       "            1.3530e-01,  7.0457e-01,  7.3701e-01,  3.5260e-01,  1.5453e-01,\n",
       "            4.0584e-01,  7.4759e-01,  7.4232e-01,  2.0105e-01,  5.5042e-01,\n",
       "            3.7784e-04,  4.6341e-01,  1.4496e-01,  1.2554e-01,  6.3637e-01,\n",
       "            1.1569e-01,  7.7357e-01,  2.1010e-01,  6.6535e-01,  3.6038e-01,\n",
       "            2.9636e-01,  5.0451e-01,  1.7340e-01,  5.1120e-01,  3.7577e-01,\n",
       "            3.0459e-01,  4.0584e-01,  4.4933e-01,  5.8816e-01,  5.3101e-01,\n",
       "            6.9907e-01,  6.5962e-01,  3.6038e-01,  2.7125e-01,  4.2782e-01,\n",
       "            8.3811e-01,  5.8197e-01,  4.9778e-01,  3.5260e-01,  1.6401e-01,\n",
       "            5.9432e-01,  4.8417e-01,  7.4232e-01,  2.3677e-01,  4.2782e-01,\n",
       "            1.6401e-01,  2.3677e-01,  8.5536e-02,  5.8816e-01,  5.3101e-01,\n",
       "            5.6947e-01,  4.2782e-01,  4.9778e-01,  6.3637e-01,  1.2554e-01,\n",
       "            3.5260e-01,  7.1550e-01,  2.6274e-01,  1.8270e-01,  2.2796e-01,\n",
       "            6.9353e-01,  2.8806e-01,  1.0574e-01,  5.4445e-02,  7.6326e-01,\n",
       "            9.5688e-02,  5.8197e-01,  4.7730e-01,  7.5279e-02,  1.8270e-01,\n",
       "            3.5260e-01,  4.7038e-01,  1.2554e-01,  2.1907e-01,  3.6810e-01,\n",
       "            4.9100e-01,  6.5386e-01,  7.2092e-01,  1.9192e-01,  3.3685e-01,\n",
       "            5.4399e-01,  5.5681e-01,  7.4759e-01,  7.8377e-01,  7.6326e-01,\n",
       "            5.3101e-01,  7.2631e-01,  5.7574e-01,  7.2092e-01,  4.7730e-01,\n",
       "            5.3101e-01,  6.9907e-01,  5.0451e-01,  6.4223e-01,  7.9889e-01,\n",
       "            6.5386e-01,  3.9092e-01,  5.5681e-01,  4.5640e-01,  6.7672e-01,\n",
       "            7.5284e-01,  8.2844e-01,  6.6535e-01,  5.2445e-01,  4.5640e-01,\n",
       "            6.2453e-01,  6.8235e-01,  4.4933e-01,  6.3047e-01,  4.2055e-01,\n",
       "            4.3504e-01,  7.2092e-01,  7.7868e-01,  7.2631e-01,  4.7038e-01,\n",
       "            5.2445e-01,  4.4221e-01,  8.2844e-01,  5.3752e-01,  6.0043e-01,\n",
       "            3.2888e-01,  6.1856e-01,  4.4933e-01,  4.4221e-01,  7.0457e-01,\n",
       "            7.8884e-01,  4.3504e-01,  4.5640e-01,  7.4759e-01,  3.7577e-01,\n",
       "            7.5284e-01,  8.0884e-01,  7.7357e-01,  3.9092e-01,  4.9778e-01,\n",
       "            7.4232e-01,  4.0584e-01,  6.5962e-01,  7.3701e-01,  3.6810e-01,\n",
       "            4.4221e-01,  3.7577e-01,  6.3047e-01,  3.7577e-01,  7.5284e-01,\n",
       "            4.1322e-01,  7.7868e-01,  8.4768e-01,  5.3101e-01,  4.1322e-01,\n",
       "            4.1322e-01,  6.8795e-01,  7.3168e-01,  5.2445e-01,  5.8816e-01,\n",
       "            5.1120e-01,  3.9841e-01,  3.6038e-01,  5.3752e-01,  6.8235e-01,\n",
       "            4.7038e-01,  7.1005e-01,  6.5962e-01,  4.1322e-01,  7.5284e-01,\n",
       "            5.7574e-01,  6.7672e-01,  4.4933e-01,  6.6535e-01,  5.6316e-01,\n",
       "            4.7038e-01,  4.3504e-01,  6.0043e-01,  3.4476e-01,  4.1322e-01,\n",
       "            6.8235e-01,  6.3637e-01,  5.8197e-01,  4.0584e-01,  3.9092e-01,\n",
       "            6.7672e-01,  6.1256e-01,  3.8337e-01,  7.7868e-01,  3.8337e-01,\n",
       "            6.0043e-01,  7.2092e-01,  4.2782e-01,  5.0451e-01,  4.0584e-01,\n",
       "            8.4768e-01,  5.3752e-01,  6.3047e-01,  6.4223e-01,  3.6810e-01,\n",
       "            3.4476e-01,  4.2055e-01,  7.6326e-01,  3.7577e-01,  3.6810e-01,\n",
       "            7.6326e-01,  6.4806e-01,  5.3752e-01,  6.7105e-01,  4.9100e-01,\n",
       "            5.0451e-01,  3.5260e-01,  7.6326e-01,  3.8337e-01,  4.4221e-01,\n",
       "            7.1550e-01,  7.3701e-01,  5.3101e-01,  6.4223e-01,  6.6535e-01,\n",
       "            5.3752e-01,  7.6326e-01,  6.0651e-01,  5.5681e-01,  7.0457e-01,\n",
       "            3.4476e-01,  5.7574e-01,  3.8337e-01,  3.5260e-01,  3.9092e-01,\n",
       "            7.8884e-01,  4.4933e-01,  7.6843e-01,  6.6535e-01,  3.1275e-01,\n",
       "            6.7672e-01,  3.3685e-01,  5.5681e-01,  4.8417e-01,  3.2085e-01,\n",
       "            7.1550e-01,  6.8795e-01,  7.0457e-01,  6.7105e-01,  4.0584e-01,\n",
       "            7.9387e-01,  4.8417e-01,  6.1256e-01,  7.3701e-01, -4.3899e-01,\n",
       "           -9.2713e-02,  3.3685e-01, -2.5094e-01, -3.4055e-01,  6.2453e-01,\n",
       "            7.6843e-01,  1.2554e-01,  2.1907e-01,  6.4916e-02,  5.5042e-01,\n",
       "            3.0459e-01, -1.9537e-01, -4.0509e-01,  5.1120e-01,  6.7105e-01,\n",
       "            2.2357e-02, -6.8615e-02,  4.8417e-01,  2.2357e-02,  4.8417e-01,\n",
       "            3.8337e-01, -3.7230e-01, -1.4272e-01, -2.2277e-01,  5.7574e-01,\n",
       "            5.1120e-01, -5.8741e-01,  6.0043e-01,  7.7868e-01,  2.8806e-01,\n",
       "            4.3863e-02, -3.8856e-01, -4.0509e-01, -4.9210e-01, -2.0897e-01,\n",
       "           -3.4055e-01, -3.2504e-01, -4.7408e-01, -2.0897e-01, -4.3899e-01,\n",
       "           -3.2504e-01, -1.0498e-01, -4.7408e-01, -4.2189e-01, -2.3675e-01,\n",
       "           -2.0897e-01, -4.5638e-01, -2.9474e-01, -5.6781e-02, -3.4055e-01,\n",
       "           -4.3899e-01, -4.5638e-01, -1.5563e-01, -3.5630e-01, -5.1045e-01,\n",
       "           -1.1741e-01, -8.0591e-02, -2.5094e-01, -4.9210e-01, -3.0978e-01,\n",
       "           -2.0897e-01, -2.6533e-01, -2.6533e-01, -4.3899e-01, -1.9537e-01,\n",
       "           -4.5638e-01, -5.6761e-01, -4.9210e-01, -5.1045e-01, -3.5630e-01,\n",
       "           -2.6533e-01, -4.7408e-01, -1.5563e-01, -4.9210e-01, -2.9474e-01,\n",
       "           -3.7230e-01, -1.8194e-01, -9.2713e-02, -2.0897e-01, -1.9537e-01,\n",
       "           -5.1045e-01, -4.7408e-01, -9.2713e-02, -6.8615e-02, -5.4819e-01,\n",
       "           -2.3675e-01, -1.2998e-01, -3.7230e-01, -4.3899e-01, -5.2914e-01,\n",
       "           -2.3675e-01,  2.2357e-02, -6.2823e-01, -2.2277e-01, -2.9474e-01,\n",
       "           -5.2914e-01, -5.4819e-01, -1.6870e-01, -2.5094e-01, -2.2277e-01,\n",
       "           -1.4272e-01, -1.5563e-01, -3.4055e-01, -4.0509e-01, -1.8194e-01,\n",
       "           -1.5563e-01, -1.4272e-01, -2.7992e-01, -1.9537e-01, -3.4055e-01,\n",
       "           -1.1741e-01, -3.5630e-01, -3.2504e-01, -2.3675e-01, -3.5630e-01,\n",
       "           -1.8194e-01, -2.0897e-01, -1.6870e-01, -3.4055e-01, -3.2504e-01,\n",
       "           -2.2277e-01, -9.2713e-02, -2.2277e-01, -3.8856e-01, -3.0978e-01,\n",
       "           -3.7230e-01, -3.3524e-02, -9.2713e-02, -4.9210e-01, -4.0509e-01,\n",
       "           -2.0897e-01, -3.5630e-01, -2.5094e-01], requires_grad=True),\n",
       "   tensor([-0.0206, -0.0376, -0.2971, -0.2466, -0.2536,  0.1009, -0.1654, -0.2607,\n",
       "           -0.1785,  0.0180, -0.2120,  0.0234, -0.0666,  0.0808,  0.0234, -0.1086,\n",
       "            0.0446, -0.0904, -0.1025, -0.1148, -0.1851,  0.2699, -0.1720, -0.2971,\n",
       "           -0.0206, -0.1148, -0.1984,  0.0287, -0.2536, -0.0039, -0.3661,  0.0655,\n",
       "           -0.3196, -0.0784, -0.2188, -0.1398, -0.1461,  0.0808, -0.1654, -0.0549,\n",
       "            0.2003, -0.3272,  0.0071,  0.2270,  0.0909, -0.2188,  0.1399, -0.2824,\n",
       "           -0.3504, -0.2188, -0.2971,  0.2093, -0.0607, -0.1590, -0.0376, -0.3504,\n",
       "            0.1351, -0.1398,  0.0959, -0.1590,  0.1303, -0.1461, -0.2052, -0.0784,\n",
       "           -0.3741, -0.0844, -0.2326,  0.2226, -0.1525, -0.1335, -0.1525, -0.0904,\n",
       "           -0.1720,  0.2357,  0.0016, -0.0206, -0.2257, -0.2679, -0.0607,  0.0499,\n",
       "           -0.1720, -0.1335, -0.0607, -0.2257, -0.0376, -0.1086, -0.2188,  0.1912,\n",
       "           -0.2257,  0.2137,  0.1542, -0.2326, -0.0319,  0.0859, -0.3349,  0.0603,\n",
       "           -0.2751, -0.3272,  0.0016,  0.0757, -0.0784, -0.2824,  0.0959,  0.0341,\n",
       "           -0.1272,  0.0125, -0.2971,  0.2357, -0.3426,  0.2093,  0.0125, -0.1335,\n",
       "           -0.3741, -0.0784, -0.2326, -0.1335, -0.3349,  0.0394, -0.0964, -0.2052,\n",
       "            0.1059, -0.3046, -0.2679, -0.1210, -0.1525, -0.0666, -0.1590, -0.1851,\n",
       "           -0.2395, -0.0319,  0.0706, -0.2466, -0.1398,  0.0180, -0.2679,  0.2443,\n",
       "           -0.0491,  0.0655,  0.0706,  0.0394,  0.0016, -0.1525, -0.0319, -0.0725,\n",
       "           -0.3196, -0.2751, -0.2395, -0.1918,  0.0125,  0.1495,  0.0016, -0.3120,\n",
       "           -0.1148, -0.0094, -0.1148, -0.0549, -0.2824, -0.1525, -0.2120,  0.0551,\n",
       "           -0.1210, -0.1851, -0.3349, -0.1984, -0.1335, -0.0491, -0.0094, -0.3120,\n",
       "           -0.2120,  0.0341, -0.3196, -0.0844,  0.0446, -0.3741, -0.2052, -0.1461,\n",
       "           -0.3196, -0.7716, -0.5019, -0.4149, -0.8717, -0.5774, -0.2897, -0.8983,\n",
       "           -0.5390, -0.3349, -0.4839, -0.3196, -0.4149, -0.5019, -0.7716, -0.2466,\n",
       "           -0.4317, -0.4402, -0.7365, -0.9977, -0.3582, -0.7026, -0.3349, -0.5296,\n",
       "           -0.4839, -0.8849, -0.5677, -0.3902, -0.5390, -0.5296, -0.7597, -0.6073,\n",
       "           -0.7716, -0.5019, -0.3349, -0.8849, -0.4929, -0.4149, -0.3984, -0.7957,\n",
       "           -0.7716, -0.6806, -0.4750, -0.5580, -0.8586, -0.7835, -0.5580, -0.4488,\n",
       "           -0.7026, -0.6073, -0.5390, -0.7957, -0.7365, -0.3661, -0.5019, -0.8329,\n",
       "           -0.4149, -0.8204, -0.4233, -0.4750, -0.5110, -0.4662, -0.8204, -0.7480,\n",
       "           -0.4317, -0.5296, -0.3741, -0.4839, -0.8586, -0.8079, -0.7138, -0.9977,\n",
       "           -0.9977, -0.4402, -0.7716, -0.9257, -0.4402, -0.5484, -0.4488, -0.8983,\n",
       "           -0.5484, -0.5110, -0.3661, -0.4149, -0.4317, -0.6698, -0.5296, -0.4066,\n",
       "           -0.9119, -0.5110, -0.3349, -0.6380, -0.7138, -0.6174, -0.7957, -0.4066,\n",
       "           -0.3426, -0.4662, -0.5774, -0.4750, -0.5873, -0.8204, -0.5203, -0.6174,\n",
       "           -0.5484, -0.5019, -0.4839, -0.7597, -0.4402, -0.4750, -0.4750, -0.4317,\n",
       "           -0.7597, -0.9119, -0.4066, -0.3272, -0.6174, -0.5296, -0.4149, -0.6277,\n",
       "           -0.7480, -0.7138, -0.1148, -0.0206, -0.1785,  0.2948, -0.1272,  0.0125,\n",
       "            0.2614, -0.1272,  0.3886,  0.1636,  0.3029,  0.3960,  0.4034, -0.0319,\n",
       "            0.1059,  0.0016,  0.1542,  0.3110,  0.2357,  0.4107,  0.2003,  0.3029,\n",
       "            0.1542,  0.1912,  0.3960,  0.1821, -0.2751,  0.0287, -0.1785,  0.1447,\n",
       "            0.0655,  0.1447, -0.3046,  0.0551, -0.2120, -0.0491, -0.1461, -0.2188,\n",
       "           -0.0150, -0.2326,  0.1157, -0.0206, -0.2824, -0.1851,  0.1108, -0.1720,\n",
       "            0.0287,  0.0341,  0.1729,  0.1059, -0.2897,  0.1447,  0.2003, -0.2257,\n",
       "            0.1821,  0.1912, -0.1785, -0.0964,  0.1059,  0.0909,  0.0446,  0.0655,\n",
       "           -0.1984,  0.0287,  0.1636,  0.0499, -0.0206,  0.1059,  0.1729, -0.0433,\n",
       "            0.0551,  0.2270, -0.3046,  0.0341,  0.2093, -0.2326,  0.0757, -0.2607,\n",
       "           -0.2607, -0.0319,  0.1059,  0.2270,  0.0394,  0.0706,  0.2270, -0.2536,\n",
       "           -0.0433,  0.0603,  0.1351,  0.1636,  0.2357, -0.1398, -0.1086,  0.2357,\n",
       "           -0.2052,  0.0234, -0.0844, -0.2257,  0.1351,  0.0016,  0.1912, -0.0319,\n",
       "            0.1157,  0.2181,  0.0808,  0.2529, -0.1335,  0.2357,  0.0125, -0.0491,\n",
       "            0.2443, -0.0904, -0.2897,  0.1821, -0.0150,  0.0125,  0.2093,  0.0287],\n",
       "          requires_grad=True),\n",
       "   tensor([ 0.8275,  0.7373,  0.8173,  0.4859,  0.6976,  0.7807,  0.7861,  0.7966,\n",
       "            0.5347,  0.8624,  0.7966,  0.8275,  0.5279,  0.4788,  0.6442,  0.6860,\n",
       "            0.7647,  0.6195,  0.7754,  0.8070,  0.6381,  0.4570,  0.4496,  0.8326,\n",
       "            0.5616,  0.6623,  0.7034,  0.6381,  0.7807,  0.5001,  0.5347,  0.7148,\n",
       "            0.5813,  0.4421,  0.6006,  0.8070,  0.6918,  0.4195,  0.6918,  0.5549,\n",
       "            0.5616,  0.7205,  0.8818,  0.8722,  0.6918,  0.6976,  0.2171,  0.6683,\n",
       "            0.7538,  0.4347,  0.5071,  0.5747,  0.5141,  0.5483,  0.4788,  0.6801,\n",
       "            0.3885,  0.8722,  0.6860,  0.4421,  0.6442,  0.6195,  0.8070,  0.6860,\n",
       "            0.7701,  0.5483,  0.6132,  0.5141,  0.6918,  0.4195,  0.7913,  0.7205,\n",
       "            0.5747,  0.3565,  0.6918,  0.5141,  0.4195,  0.6976,  0.4931,  0.8018,\n",
       "            0.4041,  0.5141,  0.5210,  0.5747,  0.7091,  0.5616,  0.4041,  0.7807,\n",
       "            0.4859,  0.7966,  0.4195,  0.7966,  0.4859,  0.7701,  0.4347,  0.5001,\n",
       "            0.4643,  0.5682,  0.6742,  0.6683,  0.7373,  0.5813,  0.6683,  0.6381,\n",
       "            0.5877,  0.6503,  0.8173,  0.7861,  0.5813,  0.3150,  0.8070,  0.2979,\n",
       "            0.2538,  0.4347,  0.4716,  0.6563,  0.6069,  0.2717,  0.6257,  0.7483,\n",
       "            0.5942,  0.7701,  0.6976,  0.5071,  0.5549,  0.3646,  0.5747,  0.5279,\n",
       "            0.5549,  0.4859,  0.8070,  0.3483,  0.4195,  0.7701,  0.3318,  0.5001,\n",
       "            0.6563,  0.5210,  0.7754,  0.5616,  0.5210,  0.4421,  0.3401,  0.6801,\n",
       "            0.5347,  0.8722,  0.1790,  0.6563,  0.7807,  0.4716,  0.6976,  0.6503,\n",
       "            0.5747,  0.7483,  0.8624,  0.3885,  0.6069,  0.2448,  0.3065,  0.4347,\n",
       "            0.6563,  0.2171,  0.6918,  0.8275,  0.7373,  0.3726,  0.5347,  0.4570,\n",
       "            0.4931,  0.8122,  0.6623,  0.4347,  0.7861,  0.4931,  0.3565,  0.7538,\n",
       "            0.2628,  0.4859,  0.6503,  0.5279,  0.6006,  0.5071,  0.4788,  0.6801,\n",
       "            0.5616,  0.8624,  0.5347,  0.7754,  0.5141, -0.3953, -0.3126, -0.2072,\n",
       "           -0.2813, -0.1652, -0.1930, -0.5437, -0.0990,  0.0102, -0.4854, -0.4484,\n",
       "           -0.0862, -0.5437, -0.1119, -0.5437, -0.2510, -0.5639, -0.2216, -0.2362,\n",
       "           -0.2968, -0.6271, -0.5845, -0.2968, -0.4667, -0.5045, -0.2968, -0.5845,\n",
       "           -0.0862, -0.3448, -0.2660, -0.0737, -0.0862, -0.4854,  0.0661, -0.2216,\n",
       "           -0.3953, -0.4484, -0.4126, -0.1930, -0.2510, -0.4303, -0.4303, -0.5639,\n",
       "           -0.5045, -0.6055,  0.1495, -0.3614, -0.1382,  0.0661, -0.5845,  0.1293,\n",
       "           -0.5845, -0.3782, -0.0369, -0.3448, -0.5045, -0.2362, -0.2813, -0.0013,\n",
       "           -0.3448, -0.3126, -0.4126, -0.3782, -0.3448, -0.4667, -0.2072, -0.0612,\n",
       "           -0.4126, -0.4484, -0.4303, -0.0130, -0.2510, -0.5639,  0.0552, -0.4854,\n",
       "           -0.1516, -0.0612, -0.3614, -0.3614, -0.2813, -0.3782, -0.0862, -0.5239,\n",
       "           -0.2510, -0.2660, -0.4126,  0.1087,  0.0329, -0.1930, -0.0990, -0.4303,\n",
       "           -0.1790, -0.6271, -0.4667, -0.3126, -0.4126, -0.1382, -0.2968, -0.2813,\n",
       "           -0.1516, -0.2362, -0.3614, -0.3286,  0.3806,  0.0441,  0.1790,  0.1790,\n",
       "            0.0102, -0.0490,  0.1693,  0.1293,  0.1693,  0.2538, -0.1250, -0.0249,\n",
       "            0.1693,  0.1983,  0.1790, -0.0862, -0.0490, -0.0130,  0.1790,  0.3065,\n",
       "            0.2357,  0.0876,  0.0661,  0.1293,  0.2628,  0.1495,  0.3318,  0.2979,\n",
       "           -0.0130,  0.4041,  0.0982,  0.1594,  0.2805,  0.3318,  0.3483,  0.0769,\n",
       "            0.2357, -0.0737,  0.1495,  0.0102,  0.1983,  0.0876, -0.0990, -0.0990,\n",
       "            0.0102, -0.0249,  0.1495,  0.0216,  0.2077,  0.3234,  0.0876,  0.1394,\n",
       "            0.2448, -0.1382,  0.0661,  0.4271,  0.1495,  0.4041,  0.3646, -0.1119,\n",
       "            0.0441, -0.0490,  0.1887,  0.1394,  0.4118,  0.1594,  0.0982, -0.1250,\n",
       "            0.2628,  0.1087,  0.3565, -0.0862,  0.2357,  0.4195,  0.3401,  0.2357,\n",
       "           -0.1516,  0.2264,  0.2628,  0.2717,  0.3065,  0.1594,  0.2628,  0.0982,\n",
       "            0.2892,  0.3150,  0.0329, -0.1516,  0.0102, -0.0490,  0.0661,  0.3318,\n",
       "            0.0769,  0.1594, -0.0490, -0.0013, -0.0130,  0.0769,  0.1790,  0.1190,\n",
       "            0.0329,  0.3234, -0.0612,  0.0441,  0.0982,  0.1495,  0.0769,  0.2077,\n",
       "            0.2357, -0.1516,  0.2717, -0.0130,  0.2077, -0.1790,  0.1394, -0.0130,\n",
       "           -0.0130, -0.0737,  0.2171, -0.0249,  0.3565,  0.0661,  0.0552,  0.2171,\n",
       "            0.2357,  0.1594,  0.0441,  0.1983,  0.0769, -0.0013,  0.3483,  0.1293,\n",
       "            0.2077, -0.0249,  0.1790,  0.1983,  0.2357,  0.1790,  0.2979,  0.1190,\n",
       "            0.2448,  0.1594,  0.0552,  0.1087,  0.3318, -0.0013,  0.1293,  0.1087,\n",
       "            0.3234,  0.1983,  0.2805, -0.0612, -0.0249, -0.0490, -0.0249,  0.3150,\n",
       "           -0.0612, -0.0249,  0.3150,  0.3318,  0.0329,  0.0329,  0.3234, -0.0369,\n",
       "            0.3726, -0.1250, -0.0130,  0.2892,  0.7701,  0.8722,  0.8426,  0.3150,\n",
       "            0.3483,  0.8526,  0.8526,  0.6442,  0.2805,  0.6918,  0.5942,  0.8624,\n",
       "            0.2077,  0.7373,  0.2979,  0.3318,  0.3318,  0.6069,  0.2077,  0.7592,\n",
       "            0.6563,  0.5141, -0.0249, -0.1250, -0.0737, -0.1516, -0.4303,  0.1495,\n",
       "            0.1293,  0.0441, -0.2362, -0.2362,  0.0661, -0.1516, -0.3953],\n",
       "          requires_grad=True)],\n",
       "  'Time to Maturity': [tensor([0.1587, 0.1587, 0.2698, 0.2698, 0.1587, 0.5198, 0.1587, 0.1587, 0.0476,\n",
       "           0.1587, 0.0476, 0.2698, 0.5198, 0.1587, 0.0159, 0.0476, 0.1587, 0.2698,\n",
       "           0.2698, 0.0714, 0.5198, 0.2698, 0.4087, 0.4087, 0.0476, 0.2698, 0.0476,\n",
       "           0.4087, 0.4087, 0.0476, 0.4087, 0.0159, 0.2698, 0.1587, 0.1587, 0.0476,\n",
       "           0.5198, 0.1587, 0.1587, 0.1587, 0.1587, 0.0714, 0.1587, 0.0159, 0.0992,\n",
       "           0.0476, 0.1587, 0.0992, 0.0159, 0.4087, 0.1587, 0.1587, 0.5198, 0.0714,\n",
       "           0.0714, 0.0476, 0.0476, 0.0476, 0.0714, 0.0159, 0.2698, 0.1587, 0.0476,\n",
       "           0.5198, 0.1587, 0.5198, 0.2698, 0.5198, 0.0992, 0.2698, 0.1587, 0.4087,\n",
       "           0.0476, 0.0714, 0.0476, 0.4087, 0.1587, 0.2698, 0.0476, 0.1587, 0.1587,\n",
       "           0.0714, 0.5198, 0.2698, 0.5198, 0.0159, 0.1587, 0.1587, 0.1587, 0.5198,\n",
       "           0.0714, 0.1587, 0.0714, 0.0992, 0.0476, 0.1587, 0.0992, 0.1587, 0.5198,\n",
       "           0.1587, 0.0476, 0.0159, 0.1587, 0.0476, 0.0476, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.0159, 0.0714, 0.0476, 0.0476, 0.1587, 0.0714, 0.2698, 0.1587,\n",
       "           0.0159, 0.0476, 0.0476, 0.0476, 0.0476, 0.2698, 0.1587, 0.1587, 0.4087,\n",
       "           0.2698, 0.5198, 0.0476, 0.0159, 0.4087, 0.5198, 0.2698, 0.0476, 0.2698,\n",
       "           0.0476, 0.4087, 0.0992, 0.0476, 0.0714, 0.0714, 0.1587, 0.1587, 0.0714,\n",
       "           0.1587, 0.1587, 0.0992, 0.4087, 0.4087, 0.1587, 0.0992, 0.4087, 0.5198,\n",
       "           0.0476, 0.5198, 0.0992, 0.4087, 0.0476, 0.1587, 0.1587, 0.0476, 0.1587,\n",
       "           0.2698, 0.5198, 0.0992, 0.1587, 0.1587, 0.1587, 1.2421, 2.3254, 2.3254,\n",
       "           1.2421, 1.2421, 2.3254, 2.3254, 1.2421, 1.2421, 1.2421, 2.3254, 2.3254,\n",
       "           1.2421, 2.3254, 1.2421, 2.3254, 1.2421, 1.2421, 2.3254, 1.2421, 2.3254,\n",
       "           1.2421, 2.3254, 1.2421, 2.3254, 2.3254, 2.3254, 1.2421, 2.3254, 1.2421,\n",
       "           1.2421, 2.3254, 1.2421, 0.0992, 0.0476, 0.0992, 0.8810, 0.0992, 0.8810,\n",
       "           0.2698, 0.0476, 0.0476, 0.8810, 0.1587, 0.8810, 0.0714, 0.8810, 0.1587,\n",
       "           0.1587, 0.5198, 0.2698, 0.0714, 0.8810, 0.2698, 0.4087, 0.4087, 0.0714,\n",
       "           0.8810, 0.0476, 0.5198, 0.1587, 0.2698, 0.1587, 0.8810, 0.5198, 0.0476,\n",
       "           0.0476, 0.0159, 0.5198, 0.8810, 0.4087, 0.0159, 0.5198, 0.5198, 0.5198,\n",
       "           0.4087, 0.8810, 0.4087, 0.0476, 0.8810, 0.0476, 0.0992, 0.5198, 0.8810,\n",
       "           0.0476, 0.0476, 0.8810, 0.5198, 0.1587, 0.0159, 0.2698, 0.2698, 0.1587,\n",
       "           0.0476, 0.0714, 0.5198, 0.5198, 0.5198, 0.5198, 0.4087, 0.5198, 0.4087,\n",
       "           0.2698, 0.5198, 0.4087, 0.0159, 0.0714, 0.4087, 0.4087, 0.0159, 0.4087,\n",
       "           0.2698, 0.1587, 0.4087, 0.5198, 0.0476, 0.2698, 0.0476, 0.0159, 0.0476,\n",
       "           0.8810, 0.2698, 0.0476, 0.0476, 0.1587, 0.8810, 0.5198, 0.4087, 0.1587,\n",
       "           0.1587, 0.0714, 0.4087, 0.8810, 0.2698, 0.1587, 0.0159, 0.4087, 0.2698,\n",
       "           0.0714, 0.2698, 0.0476, 0.2698, 0.5198, 0.5198, 0.0992, 0.5198, 0.0476,\n",
       "           0.0714, 0.0714, 0.5198, 0.5198, 0.0476, 0.2698, 0.0159, 0.1587, 0.1587,\n",
       "           0.0714, 0.0992, 0.5198, 0.8810, 0.2698, 0.8810, 0.8810, 0.2698, 0.0159,\n",
       "           0.1587, 0.8810, 0.8810, 0.0714, 0.0992, 0.5198, 0.0992, 0.2698, 0.0476,\n",
       "           0.4087, 0.4087, 0.0476, 0.2698, 0.8810, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 0.8810, 0.8810, 1.2421, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           1.2421, 0.8810, 0.8810, 1.2421, 1.2421, 1.2421, 1.2421, 0.5198, 1.2421,\n",
       "           0.5198, 1.2421, 0.8810, 0.8810, 0.8810, 0.8810, 1.2421, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 1.2421, 0.8810, 0.8810, 1.2421, 0.8810, 0.8810, 0.5198,\n",
       "           0.8810, 1.2421, 0.8810, 1.2421, 0.8810, 0.8810, 1.2421, 0.8810, 0.5198,\n",
       "           0.8810, 0.8810, 0.8810, 1.2421, 0.5198, 0.8810, 0.8810, 1.2421, 0.8810,\n",
       "           0.8810, 0.5198, 0.5198, 0.8810, 1.2421, 0.8810, 0.8810, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 0.8810, 0.5198, 0.8810, 1.2421, 0.5198, 0.8810, 0.8810,\n",
       "           1.2421, 0.8810, 0.8810, 1.2421, 0.5198, 0.5198, 1.2421, 0.5198, 0.5198,\n",
       "           0.8810], requires_grad=True),\n",
       "   tensor([0.4365, 0.0754, 0.0159, 0.6865, 0.6865, 0.5754, 0.0754, 0.3254, 0.3254,\n",
       "           0.0992, 0.1270, 0.4365, 0.4365, 0.4365, 0.0437, 0.1865, 0.4365, 0.0992,\n",
       "           0.4365, 0.1865, 0.0159, 0.3254, 0.0159, 0.1270, 0.0992, 0.4365, 0.6865,\n",
       "           0.0437, 0.6865, 0.1865, 0.0159, 0.1865, 0.1270, 0.6865, 0.3254, 0.0754,\n",
       "           0.0754, 0.1865, 0.0159, 0.0754, 0.3254, 0.1270, 0.1865, 0.5754, 0.1865,\n",
       "           0.0159, 0.5754, 0.3254, 0.1270, 0.0754, 0.3254, 0.5754, 0.0159, 0.4365,\n",
       "           0.1865, 0.0159, 0.3254, 0.6865, 0.1865, 0.1865, 0.5754, 0.4365, 0.3254,\n",
       "           0.0992, 0.3254, 0.1865, 0.0754, 0.5754, 0.0754, 0.1865, 0.0159, 0.1865,\n",
       "           0.0754, 0.3254, 0.0992, 0.0754, 0.0754, 0.1270, 0.4365, 0.1865, 0.3254,\n",
       "           0.0754, 0.0754, 0.0159, 0.1865, 0.4365, 0.0754, 0.3254, 0.1270, 0.5754,\n",
       "           0.5754, 0.6865, 0.0754, 0.1270, 0.0437, 0.4365, 0.6865, 0.3254, 0.0754,\n",
       "           0.4365, 0.0992, 0.0159, 0.5754, 0.0754, 0.6865, 0.4365, 0.5754, 0.5754,\n",
       "           0.1270, 0.3254, 0.1270, 0.6865, 0.3254, 0.0754, 0.0159, 0.6865, 0.0159,\n",
       "           0.0437, 0.1865, 0.0159, 0.3254, 0.1270, 0.0437, 0.4365, 0.1865, 0.0992,\n",
       "           0.6865, 0.3254, 0.0437, 0.0159, 0.1865, 0.0754, 0.0159, 0.0754, 0.0159,\n",
       "           0.3254, 0.1865, 0.1270, 0.1865, 0.4365, 0.4365, 0.4365, 0.0992, 0.0159,\n",
       "           0.0437, 0.6865, 0.0437, 0.6865, 0.1270, 0.5754, 0.0992, 0.0159, 0.0159,\n",
       "           0.1865, 0.0992, 0.0992, 0.6865, 0.1865, 0.3254, 0.1865, 0.6865, 0.6865,\n",
       "           0.1270, 0.1865, 0.4365, 0.4365, 0.0992, 0.1270, 0.1270, 0.1270, 0.0159,\n",
       "           0.6865, 0.1865, 0.0159, 0.0754, 0.0159, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.0476, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.0476, 1.0476, 1.4087, 1.4087, 1.4087, 1.0476,\n",
       "           1.4087, 1.4087, 1.0476, 1.0476, 1.4087, 1.0476, 1.0476, 1.4087, 1.4087,\n",
       "           1.0476, 1.4087, 1.4087, 1.4087, 1.0476, 1.0476, 1.0476, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.0476, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.4087, 1.0476, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.0476, 1.0476, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.0476, 1.4087, 1.0476,\n",
       "           1.4087, 1.0476, 1.4087, 1.0476, 1.0476, 0.6865, 0.3254, 0.0754, 0.1865,\n",
       "           0.0754, 0.0754, 0.3254, 0.1865, 0.3254, 0.5754, 0.3254, 0.0754, 0.0754,\n",
       "           0.0754, 0.1865, 0.6865, 0.6865, 0.1865, 0.0754, 0.0754, 0.1865, 0.1865,\n",
       "           0.0754, 0.1865, 0.6865, 0.3254, 0.0754, 0.5754, 0.4365, 0.3254, 0.0754,\n",
       "           0.6865, 0.1865, 0.4365, 0.4365, 0.6865, 0.0754, 0.3254, 0.5754, 0.3254,\n",
       "           0.1865, 0.5754, 0.6865, 0.3254, 0.6865, 0.4365, 0.0754, 0.6865, 0.3254,\n",
       "           0.3254, 0.0754, 0.3254, 0.1865, 0.3254, 0.6865, 0.5754, 0.5754, 0.6865,\n",
       "           0.3254, 0.4365, 0.5754, 0.0754, 0.6865, 0.0754, 0.6865, 0.3254, 0.0754,\n",
       "           0.6865, 0.5754, 0.3254, 0.5754, 0.6865, 0.5754, 0.1865, 0.0754, 0.4365,\n",
       "           0.1865, 0.5754, 0.1865, 0.5754, 0.0754, 0.1865, 0.6865, 0.5754, 0.5754,\n",
       "           0.5754, 0.0754, 0.3254, 0.4365, 0.4365, 0.5754, 0.3254, 0.5754, 0.1865,\n",
       "           0.6865, 0.3254, 0.4365, 0.3254, 0.5754, 0.1865, 0.3254, 0.3254, 0.5754,\n",
       "           0.0754, 0.0754, 0.5754, 0.6865, 0.3254, 0.0754, 0.3254, 0.6865, 0.6865,\n",
       "           0.0159, 0.1865, 0.6865, 0.0754, 0.5754, 0.3254, 0.6865, 0.1865, 0.5754,\n",
       "           0.6865, 0.5754, 0.5754, 0.5754, 0.6865, 0.0754, 0.6865, 0.1865, 0.5754,\n",
       "           0.5754, 0.5754, 0.3254, 0.1865, 0.5754, 0.6865, 0.1865, 0.1865, 0.4365,\n",
       "           0.1865, 0.4365, 0.0754, 0.5754, 0.6865, 0.0754, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.3254, 0.5754, 0.5754, 0.5754, 0.6865, 0.1865, 0.3254, 0.5754,\n",
       "           0.4365, 0.0754, 0.1865, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 0.0754, 0.6865, 1.0476, 0.5754, 0.0754, 0.1865, 0.5754, 0.1865,\n",
       "           0.6865, 0.4365, 1.4087, 0.1865, 0.6865, 1.4087, 0.1865, 0.4365, 1.4087,\n",
       "           1.4087, 0.5754, 0.5754, 1.0476, 1.0476, 0.4365, 1.4087, 0.6865, 0.6865,\n",
       "           1.4087, 0.4365, 0.6865, 0.4365, 0.1865, 0.1865, 1.0476, 1.0476, 0.4365,\n",
       "           1.0476, 0.5754, 1.0476, 0.6865, 0.4365, 0.4365, 0.5754, 0.6865, 0.0754,\n",
       "           1.0476, 0.3254, 1.0476, 0.4365, 0.4365, 0.6865, 0.6865, 0.6865, 1.0476,\n",
       "           0.3254, 0.3254, 0.6865, 0.6865, 0.4365, 0.6865, 0.1865, 1.0476, 1.4087,\n",
       "           0.5754, 1.0476, 1.4087, 1.4087, 0.4365, 0.5754, 0.6865, 0.5754, 0.6865,\n",
       "           0.0754, 0.5754, 0.5754, 1.4087, 1.4087, 1.4087, 1.0476, 0.4365, 1.4087,\n",
       "           0.0754, 0.6865, 0.6865, 0.1865, 1.4087, 0.6865, 0.5754, 0.6865, 1.0476,\n",
       "           1.0476, 1.4087, 0.1865, 0.1865, 0.0754, 0.0754, 1.4087, 1.4087, 0.3254,\n",
       "           0.3254, 0.5754, 0.3254, 1.0476], requires_grad=True),\n",
       "   tensor([0.2421, 0.0437, 0.0159, 0.4921, 0.4921, 0.2421, 0.0714, 0.2421, 0.2421,\n",
       "           0.0714, 0.1310, 0.2421, 0.2421, 0.2421, 0.0159, 0.1310, 0.2421, 0.0714,\n",
       "           0.2421, 0.1310, 0.0159, 0.1310, 0.0159, 0.1310, 0.0714, 0.2421, 0.4921,\n",
       "           0.0159, 0.4921, 0.1310, 0.0159, 0.1310, 0.1310, 0.4921, 0.2421, 0.0714,\n",
       "           0.0437, 0.1310, 0.0159, 0.0437, 0.1310, 0.1310, 0.1310, 0.2421, 0.1310,\n",
       "           0.0159, 0.2421, 0.2421, 0.1310, 0.0714, 0.2421, 0.2421, 0.0159, 0.2421,\n",
       "           0.1310, 0.0159, 0.1310, 0.4921, 0.1310, 0.1310, 0.2421, 0.2421, 0.2421,\n",
       "           0.0992, 0.2421, 0.1310, 0.0714, 0.2421, 0.0714, 0.1310, 0.0159, 0.1310,\n",
       "           0.0437, 0.1310, 0.0992, 0.0437, 0.0437, 0.1310, 0.2421, 0.1310, 0.2421,\n",
       "           0.0437, 0.0437, 0.0159, 0.1310, 0.2421, 0.0714, 0.1310, 0.1310, 0.2421,\n",
       "           0.2421, 0.4921, 0.0437, 0.0992, 0.0437, 0.2421, 0.4921, 0.2421, 0.0437,\n",
       "           0.2421, 0.0992, 0.0159, 0.2421, 0.0437, 0.4921, 0.2421, 0.4921, 0.2421,\n",
       "           0.1310, 0.1310, 0.0992, 0.4921, 0.2421, 0.0437, 0.0159, 0.4921, 0.0159,\n",
       "           0.0159, 0.1310, 0.0159, 0.1310, 0.1310, 0.0437, 0.2421, 0.1310, 0.0714,\n",
       "           0.4921, 0.2421, 0.0437, 0.0159, 0.1310, 0.0714, 0.0159, 0.0437, 0.0159,\n",
       "           0.1310, 0.1310, 0.0992, 0.1310, 0.2421, 0.2421, 0.2421, 0.0992, 0.0159,\n",
       "           0.0437, 0.4921, 0.0437, 0.4921, 0.0992, 0.2421, 0.0714, 0.0159, 0.0159,\n",
       "           0.1310, 0.0714, 0.0992, 0.4921, 0.1310, 0.2421, 0.1310, 0.4921, 0.4921,\n",
       "           0.1310, 0.1310, 0.2421, 0.2421, 0.0992, 0.1310, 0.1310, 0.0992, 0.0159,\n",
       "           0.4921, 0.1310, 0.0159, 0.0437, 0.0159, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 0.4921, 0.4921, 0.9643, 0.9643, 0.8532,\n",
       "           0.9643, 0.2421, 0.2421, 0.1310, 0.4921, 0.9643, 0.1310, 0.1310, 0.1310,\n",
       "           0.2421, 0.4921, 0.8532, 0.9643, 0.9643, 0.9643, 0.4921, 0.2421, 0.1310,\n",
       "           0.1310, 0.8532, 0.1310, 0.8532, 0.9643, 0.1310, 0.9643, 0.8532, 0.8532,\n",
       "           0.8532, 0.4921, 0.9643, 0.9643, 0.4921, 0.8532, 0.9643, 0.1310, 0.4921,\n",
       "           0.4921, 0.4921, 0.9643, 0.2421, 0.9643, 0.8532, 0.8532, 0.2421, 0.8532,\n",
       "           0.8532, 0.1310, 0.4921, 0.9643, 0.9643, 0.1310, 0.1310, 0.9643, 0.1310,\n",
       "           0.4921, 0.4921, 0.1310, 0.9643, 0.2421, 0.1310, 0.4921, 0.4921, 0.9643,\n",
       "           0.1310, 0.8532, 0.2421, 0.9643, 0.2421, 0.8532, 0.1310, 0.9643, 0.1310,\n",
       "           0.1310, 0.4921, 0.9643, 0.9643, 0.4921, 0.9643, 0.8532, 0.1310, 0.4921,\n",
       "           0.8532, 0.4921, 0.2421, 0.8532, 0.9643, 0.2421, 0.8532, 0.4921, 0.9643,\n",
       "           0.8532, 0.1310, 0.9643, 0.8532, 0.9643, 0.8532, 0.8532, 0.1310, 0.1310,\n",
       "           0.1310, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           0.8532, 0.9643, 0.9643, 0.4921, 0.8532, 0.8532, 0.8532, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.8532, 0.9643, 0.9643, 0.9643, 0.4921, 0.9643, 0.8532,\n",
       "           0.8532, 0.8532, 0.9643, 0.4921, 0.8532, 0.9643, 0.8532, 0.9643, 0.8532,\n",
       "           0.8532, 0.8532, 0.4921, 0.8532, 0.9643, 0.9643, 0.4921, 0.8532, 0.9643,\n",
       "           0.8532, 0.4921, 0.8532, 0.8532, 0.9643, 0.8532, 0.4921, 0.4921, 0.9643,\n",
       "           0.8532, 0.9643, 0.9643, 0.8532, 0.8532, 0.9643, 0.8532, 0.9643, 0.4921,\n",
       "           0.4921, 0.8532, 0.9643, 0.8532, 0.9643, 0.9643, 0.9643, 0.8532, 0.4921,\n",
       "           0.8532, 0.4921, 0.8532, 0.9643, 0.9643, 0.9643, 0.4921, 0.8532, 0.8532,\n",
       "           0.4921, 0.8532, 0.4921, 0.4921, 0.9643, 0.8532, 0.4921, 0.9643, 0.8532,\n",
       "           0.4921, 0.9643, 0.8532, 0.4921, 0.8532, 0.9643, 0.4921, 0.9643, 0.8532,\n",
       "           0.4921, 0.4921], requires_grad=True),\n",
       "   tensor([1.2579, 0.5357, 0.5357, 0.0357, 0.0357, 1.2579, 0.4246, 0.5357, 0.4246,\n",
       "           1.2579, 0.1746, 0.1746, 0.5357, 0.2857, 0.4246, 1.2579, 1.2579, 0.8968,\n",
       "           0.5357, 0.4246, 0.0357, 0.8968, 0.1746, 1.2579, 0.4246, 1.2579, 0.1746,\n",
       "           1.2579, 0.1746, 0.4246, 0.8968, 0.0357, 0.5357, 0.4246, 0.8968, 0.1746,\n",
       "           0.0357, 1.2579, 0.8968, 1.2579, 0.8968, 0.0357, 0.1746, 1.2579, 0.4246,\n",
       "           0.5357, 1.2579, 0.0357, 1.2579, 0.5357, 0.2857, 0.8968, 0.8968, 0.5357,\n",
       "           0.0357, 0.5357, 0.8968, 0.5357, 0.1746, 0.5357, 1.2579, 0.0357, 0.4246,\n",
       "           0.4246, 0.5357, 0.0357, 0.0357, 0.5357, 0.8968, 0.8968, 0.4246, 0.4246,\n",
       "           0.0357, 1.2579, 1.2579, 0.0357, 0.8968, 0.0357, 0.1746, 0.1746, 1.2579,\n",
       "           0.2857, 0.0357, 0.5357, 0.4246, 1.2579, 0.4246, 0.1746, 0.2857, 1.2579,\n",
       "           0.5357, 0.4246, 0.8968, 1.2579, 0.4246, 0.1746, 1.2579, 0.1746, 0.0357,\n",
       "           1.2579, 0.5357, 0.4246, 0.8968, 1.2579, 0.5357, 0.4246, 1.2579, 0.1746,\n",
       "           0.1746, 0.8968, 0.5357, 1.2579, 1.2579, 0.1746, 0.4246, 0.8968, 0.8968,\n",
       "           0.8968, 0.5357, 0.1746, 0.4246, 0.5357, 0.1746, 1.2579, 0.1746, 1.2579,\n",
       "           1.2579, 0.0357, 0.8968, 0.4246, 1.2579, 0.5357, 1.2579, 0.1746, 1.2579,\n",
       "           0.8968, 0.4246, 0.4246, 0.5357, 0.2857, 0.8968, 0.4246, 0.8968, 0.4246,\n",
       "           0.8968, 0.5357, 1.2579, 0.5357, 1.2579, 0.5357, 0.4246, 0.5357, 0.5357,\n",
       "           1.2579, 0.1746, 0.5357, 0.0357, 0.8968, 0.8968, 1.2579, 0.0357, 1.2579,\n",
       "           0.4246, 1.2579, 0.4246, 0.4246, 0.2857, 0.2857, 0.0357, 1.2579, 1.2579,\n",
       "           0.4246, 0.5357, 1.2579, 0.8968, 0.5357, 0.8968, 0.4246, 1.2579, 1.2579,\n",
       "           0.8968, 0.5357, 0.1746, 0.5357, 0.0357, 0.5357, 0.1746, 1.2579, 0.0357,\n",
       "           0.0357, 1.2579, 0.0357, 0.0357, 0.2857, 0.0357, 1.2579, 0.4246, 1.2579,\n",
       "           0.5357, 1.2579, 0.5357, 0.2857, 1.2579, 0.5357, 0.0357, 0.8968, 0.2857,\n",
       "           0.0357, 0.4246, 0.5357, 1.2579, 0.2857, 1.2579, 0.0357, 1.2579, 0.8968,\n",
       "           0.4246, 0.5357, 0.8968, 0.5357, 0.8968, 0.2857, 0.8968, 0.0357, 0.5357,\n",
       "           0.5357, 1.2579, 1.2579, 0.5357, 0.2857, 0.4246, 0.8968, 0.1746, 0.4246,\n",
       "           1.2579, 0.5357, 0.5357, 1.2579, 0.4246, 1.2579, 0.5357, 1.2579, 1.2579,\n",
       "           0.5357, 1.2579, 0.1151, 0.2857, 1.2579, 1.2579, 0.8968, 0.8968, 1.2579,\n",
       "           0.8968, 0.8968, 0.8968, 0.8968, 0.0357, 0.1746, 0.1746, 1.2579, 0.1746,\n",
       "           0.5357, 0.8968, 1.2579, 0.5357, 0.5357, 0.0357, 0.4246, 0.1746, 0.5357,\n",
       "           0.5357, 0.8968, 0.0357, 0.2857, 0.8968, 1.2579, 1.2579, 0.2857, 1.2579,\n",
       "           0.1746, 1.2579, 0.5357, 0.5357, 0.2857, 0.4246, 0.8968, 0.1746, 0.1746,\n",
       "           0.1746, 0.8968, 0.8968, 0.2857, 0.0357, 0.0357, 0.0040, 0.0595, 0.0595,\n",
       "           0.1151, 0.0595, 0.0357, 0.8968, 0.4246, 0.0040, 0.0833, 0.1151, 0.1746,\n",
       "           0.4246, 0.0040, 0.1746, 0.1746, 0.5357, 0.2857, 0.0595, 0.1746, 0.0357,\n",
       "           0.1151, 0.4246, 0.4246, 0.1746, 0.0595, 0.0595, 0.0357, 0.8968, 0.1746,\n",
       "           0.0357, 0.1746, 0.0357, 0.4246, 0.1746, 0.2857, 0.0357, 0.2857, 0.8968,\n",
       "           0.0357, 0.1746, 0.0833, 0.4246, 0.1746, 0.8968, 0.0595, 0.1151, 0.4246,\n",
       "           0.2857, 0.0595, 0.0357, 0.1746, 0.2857, 0.0357, 0.0040, 0.0357, 0.0357,\n",
       "           0.0357, 0.1151, 0.2857, 0.2857, 0.1746, 0.1746, 0.0040, 0.1746, 0.1151,\n",
       "           0.1746, 0.1746, 0.0357, 0.0357, 0.0833, 0.1746, 0.0357, 0.0357, 0.0595,\n",
       "           0.0595, 0.0040, 0.0040, 0.0357, 0.0833, 0.1746, 0.1151, 0.4246, 0.0040,\n",
       "           0.0595, 0.0833, 0.0357, 0.0595, 0.2857, 0.0040, 0.0833, 0.0595, 0.2857,\n",
       "           0.0833, 0.0040, 0.8968, 0.0357, 0.4246, 0.2857, 0.5357, 0.0040, 0.2857,\n",
       "           0.1151, 0.1746, 0.0357, 0.4246, 0.4246, 0.0595, 0.2857, 0.0595, 0.0595,\n",
       "           0.0357, 0.1746, 0.0357, 0.0040, 0.0595, 0.1151, 0.1151, 0.1746, 0.0833,\n",
       "           0.4246, 0.5357, 0.1151, 0.0357, 0.4246, 0.1746, 0.2857, 0.5357, 0.4246,\n",
       "           0.0357, 0.2857, 0.0595, 0.5357, 0.2857, 0.5357, 0.1746, 0.1746, 0.0833,\n",
       "           0.5357, 0.5357, 0.0357, 0.8968, 0.4246, 0.1746, 0.0040, 0.0040, 0.4246,\n",
       "           0.0833, 0.0040, 0.1746, 0.5357, 0.0357, 0.2857, 0.2857, 0.1746, 0.0595,\n",
       "           0.1746, 0.4246, 0.1151, 0.5357, 0.2857, 0.2857, 0.2857, 0.0595, 0.0357,\n",
       "           0.2857, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024,\n",
       "           2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024,\n",
       "           2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024,\n",
       "           2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024],\n",
       "          requires_grad=True)],\n",
       "  'Implied Volatility': [tensor([0.2695, 0.2695, 0.3415, 0.3091, 0.3302, 0.2849, 0.2695, 0.3302, 0.2561,\n",
       "           0.2912, 0.3295, 0.3461, 0.2658, 0.3302, 0.3133, 0.3065, 0.3302, 0.2950,\n",
       "           0.3461, 0.2847, 0.3088, 0.2847, 0.2711, 0.2940, 0.2561, 0.2887, 0.2561,\n",
       "           0.2683, 0.2662, 0.3295, 0.2817, 0.3133, 0.3271, 0.2548, 0.3302, 0.3295,\n",
       "           0.2849, 0.3302, 0.3302, 0.2695, 0.3302, 0.2702, 0.3302, 0.3716, 0.2707,\n",
       "           0.3295, 0.3302, 0.2604, 0.3133, 0.3104, 0.2695, 0.3302, 0.2631, 0.2702,\n",
       "           0.2654, 0.3295, 0.2561, 0.3138, 0.2702, 0.3153, 0.3128, 0.2695, 0.2561,\n",
       "           0.2665, 0.3024, 0.2849, 0.3461, 0.2849, 0.2661, 0.3066, 0.2564, 0.2844,\n",
       "           0.2561, 0.2702, 0.3295, 0.2745, 0.2695, 0.3000, 0.2636, 0.3302, 0.2585,\n",
       "           0.3356, 0.2849, 0.3216, 0.2785, 0.3190, 0.3302, 0.2695, 0.3302, 0.2676,\n",
       "           0.2737, 0.2695, 0.3356, 0.2661, 0.3295, 0.3302, 0.2601, 0.2695, 0.2717,\n",
       "           0.2695, 0.2859, 0.3133, 0.2695, 0.2561, 0.3295, 0.3302, 0.2695, 0.2695,\n",
       "           0.3302, 0.3133, 0.3047, 0.2561, 0.2561, 0.2695, 0.3356, 0.3413, 0.2695,\n",
       "           0.3133, 0.2561, 0.3295, 0.2616, 0.3295, 0.2888, 0.2695, 0.3302, 0.3104,\n",
       "           0.3096, 0.2740, 0.2607, 0.3133, 0.2666, 0.2775, 0.2929, 0.3295, 0.3461,\n",
       "           0.2561, 0.2710, 0.2661, 0.2561, 0.3356, 0.2702, 0.2581, 0.3302, 0.2702,\n",
       "           0.3302, 0.3302, 0.3215, 0.2669, 0.2877, 0.2759, 0.2661, 0.2854, 0.2985,\n",
       "           0.3295, 0.2631, 0.2661, 0.2770, 0.2561, 0.2609, 0.3302, 0.2628, 0.2695,\n",
       "           0.3461, 0.2716, 0.2871, 0.2804, 0.3116, 0.3302, 0.2894, 0.4638, 0.3104,\n",
       "           0.3651, 0.3439, 0.3153, 0.3111, 0.3495, 0.2803, 0.2844, 0.3451, 0.3081,\n",
       "           0.3100, 0.3491, 0.2940, 0.3186, 0.2867, 0.2829, 0.3227, 0.4616, 0.3150,\n",
       "           0.4355, 0.4535, 0.3170, 0.3788, 0.3126, 0.4782, 0.3439, 0.3120, 0.2870,\n",
       "           0.2817, 0.3202, 0.2878, 0.3081, 0.3473, 0.2819, 0.2757, 0.3187, 0.2901,\n",
       "           0.4840, 0.2996, 0.3473, 0.3051, 0.3044, 0.4344, 0.3507, 0.4801, 0.3369,\n",
       "           0.3235, 0.2824, 0.3695, 0.3381, 0.3175, 0.3695, 0.3459, 0.3335, 0.3648,\n",
       "           0.2771, 0.3258, 0.3508, 0.3295, 0.3267, 0.2716, 0.2983, 0.3472, 0.3258,\n",
       "           0.3258, 0.3338, 0.3280, 0.2834, 0.2778, 0.3716, 0.3472, 0.3235, 0.3472,\n",
       "           0.3459, 0.2899, 0.3459, 0.3473, 0.3347, 0.3473, 0.2607, 0.3822, 0.3535,\n",
       "           0.3258, 0.3258, 0.3469, 0.3472, 0.3114, 0.3147, 0.3695, 0.3695, 0.2622,\n",
       "           0.3473, 0.3632, 0.3422, 0.3472, 0.2690, 0.2665, 0.3398, 0.3472, 0.6848,\n",
       "           0.3001, 0.3320, 0.2909, 0.3227, 0.2803, 0.3700, 0.4059, 0.4187, 0.3459,\n",
       "           0.3469, 0.3103, 0.3459, 0.2912, 0.3258, 0.3273, 0.3258, 0.4187, 0.3473,\n",
       "           0.2778, 0.3379, 0.3473, 0.3473, 0.2693, 0.6979, 0.3822, 0.3242, 0.3397,\n",
       "           0.3146, 0.3507, 0.2703, 0.3390, 0.2887, 0.3510, 0.4187, 0.2792, 0.5274,\n",
       "           0.2886, 0.2902, 0.2826, 0.3419, 0.2725, 0.2686, 0.2735, 0.3472, 0.3473,\n",
       "           0.3507, 0.3381, 0.3822, 0.2809, 0.2643, 0.3695, 0.3716, 0.2985, 0.3510,\n",
       "           0.2723, 0.3187, 0.3822, 0.2952, 0.2957, 0.3109, 0.2810, 0.5483, 0.4187,\n",
       "           0.3369, 0.3277, 0.2782, 0.3401, 0.2721, 0.3472, 0.3002, 0.3695, 0.3209,\n",
       "           0.3459, 0.2989, 0.3473, 0.5483, 0.3469, 0.3216, 0.3192, 0.3167, 0.3136,\n",
       "           0.3194, 0.3264, 0.3043, 0.3047, 0.3011, 0.3043, 0.2967, 0.3194, 0.2968,\n",
       "           0.2973, 0.3001, 0.3310, 0.3061, 0.2967, 0.3011, 0.3056, 0.3047, 0.3182,\n",
       "           0.3194, 0.3037, 0.2964, 0.3076, 0.3194, 0.2982, 0.3043, 0.2974, 0.3292,\n",
       "           0.3020, 0.3053, 0.3042, 0.3089, 0.2767, 0.3089, 0.2854, 0.2671, 0.2943,\n",
       "           0.2808, 0.2738, 0.3089, 0.2829, 0.2853, 0.2823, 0.2788, 0.3124, 0.2811,\n",
       "           0.2849, 0.2871, 0.3116, 0.3116, 0.3089, 0.2682, 0.2819, 0.3089, 0.3089,\n",
       "           0.3089, 0.3116, 0.2819, 0.3089, 0.3116, 0.2767, 0.3116, 0.2879, 0.2849,\n",
       "           0.3089, 0.2858, 0.2794, 0.2777, 0.2785, 0.2685, 0.2765, 0.2760, 0.3124,\n",
       "           0.3089, 0.3116, 0.3089, 0.2829, 0.2849, 0.3116, 0.3116, 0.2792, 0.3116,\n",
       "           0.3096, 0.3069, 0.3124, 0.3089, 0.2824, 0.2717, 0.2672, 0.2872, 0.2779,\n",
       "           0.3000, 0.2763, 0.3089, 0.3124, 0.3089, 0.2814, 0.3229, 0.3089, 0.3116,\n",
       "           0.2791, 0.2818, 0.3116, 0.2978, 0.2849, 0.2849, 0.2999, 0.2849, 0.2849,\n",
       "           0.3089]),\n",
       "   tensor([0.2906, 0.3365, 0.5096, 0.2726, 0.2813, 0.2881, 0.4041, 0.2970, 0.3017,\n",
       "           0.3056, 0.3919, 0.3156, 0.2899, 0.3284, 0.4003, 0.2914, 0.3032, 0.3120,\n",
       "           0.2953, 0.2862, 0.4731, 0.2803, 0.4101, 0.3146, 0.3002, 0.2974, 0.2711,\n",
       "           0.3724, 0.2737, 0.3042, 0.4484, 0.3578, 0.3025, 0.2807, 0.3123, 0.4041,\n",
       "           0.3174, 0.3151, 0.4731, 0.3529, 0.2829, 0.2985, 0.3123, 0.2841, 0.3578,\n",
       "           0.3676, 0.2858, 0.2881, 0.2924, 0.5174, 0.2851, 0.2802, 0.4101, 0.3086,\n",
       "           0.3042, 0.4101, 0.2949, 0.2717, 0.3578, 0.3110, 0.2854, 0.3037, 0.3211,\n",
       "           0.3597, 0.2819, 0.2819, 0.4041, 0.2829, 0.5174, 0.2948, 0.4731, 0.2908,\n",
       "           0.3375, 0.2759, 0.4201, 0.4041, 0.6361, 0.2893, 0.3017, 0.3151, 0.3017,\n",
       "           0.3124, 0.3546, 0.3681, 0.2894, 0.3011, 0.4041, 0.2805, 0.3755, 0.2891,\n",
       "           0.2829, 0.2718, 0.3510, 0.2962, 0.3968, 0.3091, 0.2810, 0.2859, 0.4136,\n",
       "           0.3163, 0.3556, 0.4101, 0.2838, 0.4041, 0.2839, 0.2957, 0.3016, 0.2990,\n",
       "           0.2940, 0.2829, 0.3308, 0.2726, 0.2756, 0.3271, 0.3696, 0.2801, 0.6375,\n",
       "           0.3452, 0.2908, 0.3983, 0.3010, 0.3096, 0.4799, 0.3030, 0.3020, 0.3011,\n",
       "           0.2828, 0.3276, 0.3968, 0.4731, 0.3151, 0.4041, 0.4731, 0.4456, 0.4101,\n",
       "           0.2811, 0.2869, 0.3084, 0.3578, 0.3271, 0.2935, 0.3068, 0.3597, 0.4101,\n",
       "           0.4562, 0.2751, 0.4799, 0.2696, 0.4300, 0.2833, 0.2944, 0.4101, 0.4101,\n",
       "           0.3013, 0.3262, 0.3597, 0.2774, 0.3007, 0.3017, 0.3151, 0.2728, 0.2706,\n",
       "           0.2875, 0.3255, 0.3013, 0.3018, 0.3597, 0.2893, 0.2893, 0.3695, 0.4101,\n",
       "           0.2791, 0.3151, 0.4484, 0.3801, 0.4731, 0.2950, 0.2950, 0.2879, 0.2686,\n",
       "           0.2922, 0.2675, 0.2950, 0.2749, 0.2950, 0.2827, 0.2675, 0.2659, 0.2781,\n",
       "           0.2950, 0.2811, 0.2675, 0.2753, 0.2768, 0.2864, 0.2832, 0.2675, 0.2768,\n",
       "           0.2675, 0.2718, 0.2800, 0.2751, 0.2675, 0.2768, 0.2768, 0.2950, 0.2675,\n",
       "           0.2703, 0.2664, 0.2675, 0.2950, 0.2938, 0.2938, 0.2668, 0.2675, 0.2675,\n",
       "           0.2675, 0.2675, 0.2775, 0.2929, 0.2671, 0.2646, 0.2672, 0.2675, 0.2950,\n",
       "           0.2675, 0.2768, 0.2768, 0.2768, 0.2636, 0.2903, 0.2675, 0.2788, 0.2781,\n",
       "           0.2671, 0.2675, 0.2680, 0.2664, 0.2693, 0.2675, 0.2708, 0.2675, 0.2950,\n",
       "           0.2804, 0.2651, 0.2708, 0.2950, 0.2660, 0.2781, 0.2938, 0.2950, 0.2768,\n",
       "           0.2675, 0.2740, 0.2700, 0.2938, 0.2938, 0.3270, 0.3017, 0.5174, 0.3578,\n",
       "           0.4041, 0.5174, 0.3017, 0.3151, 0.3017, 0.3016, 0.3784, 0.5174, 0.5174,\n",
       "           0.4041, 0.3578, 0.3048, 0.3207, 0.3578, 0.4041, 0.4041, 0.3578, 0.3578,\n",
       "           0.4041, 0.3578, 0.3270, 0.3017, 0.4041, 0.3315, 0.3619, 0.3784, 0.5174,\n",
       "           0.3048, 0.3578, 0.3678, 0.3284, 0.3048, 0.4041, 0.3784, 0.3016, 0.3784,\n",
       "           0.3151, 0.3378, 0.3270, 0.3017, 0.3151, 0.3689, 0.4041, 0.3048, 0.3784,\n",
       "           0.3784, 0.5174, 0.3017, 0.3151, 0.3784, 0.3048, 0.3701, 0.3016, 0.2963,\n",
       "           0.3784, 0.3284, 0.3701, 0.5174, 0.3048, 0.4041, 0.3048, 0.3784, 0.5174,\n",
       "           0.3048, 0.3016, 0.3017, 0.3016, 0.3270, 0.3016, 0.3151, 0.5174, 0.3284,\n",
       "           0.3578, 0.3016, 0.3151, 0.3016, 0.4041, 0.3151, 0.3270, 0.3701, 0.3701,\n",
       "           0.3411, 0.5174, 0.3784, 0.3678, 0.3284, 0.3701, 0.3434, 0.3277, 0.3578,\n",
       "           0.3270, 0.3017, 0.3284, 0.3662, 0.3016, 0.3578, 0.3017, 0.3784, 0.3181,\n",
       "           0.5174, 0.5174, 0.3336, 0.3270, 0.3703, 0.4041, 0.3784, 0.3048, 0.3270,\n",
       "           0.4101, 0.3151, 0.3048, 0.4041, 0.3016, 0.3017, 0.3048, 0.3578, 0.3016,\n",
       "           0.3048, 0.3543, 0.3016, 0.3093, 0.3270, 0.5174, 0.3163, 0.3151, 0.3701,\n",
       "           0.3669, 0.3701, 0.3784, 0.3151, 0.3701, 0.3270, 0.3578, 0.3151, 0.3284,\n",
       "           0.3151, 0.3413, 0.5174, 0.3205, 0.3270, 0.4041, 0.3270, 0.3048, 0.3048,\n",
       "           0.3270, 0.3017, 0.3016, 0.3016, 0.3016, 0.3270, 0.3578, 0.3784, 0.3701,\n",
       "           0.3511, 0.4041, 0.3151, 0.3046, 0.2916, 0.3240, 0.3065, 0.3002, 0.3105,\n",
       "           0.3118, 0.3075, 0.3084, 0.2999, 0.2997, 0.2924, 0.3028, 0.2899, 0.3026,\n",
       "           0.3173, 0.3020, 0.3065, 0.3036, 0.3047, 0.2881, 0.2925, 0.3054, 0.3008,\n",
       "           0.3165, 0.2992, 0.3081, 0.2915, 0.2908, 0.3932, 0.3090, 0.2916, 0.2999,\n",
       "           0.2935, 0.7247, 0.3792, 0.4230, 0.3071, 0.3801, 0.3962, 0.3439, 0.3909,\n",
       "           0.3370, 0.3464, 0.2896, 0.3962, 0.3792, 0.2993, 0.3374, 0.4610, 0.3030,\n",
       "           0.2798, 0.3346, 0.4089, 0.3296, 0.2859, 0.4352, 0.3372, 0.2892, 0.2870,\n",
       "           0.2988, 0.3484, 0.3353, 0.3237, 0.3962, 0.3481, 0.3264, 0.2927, 0.3484,\n",
       "           0.4797, 0.3532, 0.3400, 0.3274, 0.3358, 0.3484, 0.3030, 0.3370, 0.3801,\n",
       "           0.3088, 0.3068, 0.2844, 0.3413, 0.3221, 0.3370, 0.3792, 0.2834, 0.2795,\n",
       "           0.3632, 0.3194, 0.2912, 0.3690, 0.4610, 0.3792, 0.3962, 0.2711, 0.3372,\n",
       "           0.3226, 0.3030, 0.3372, 0.3372, 0.3161, 0.3260, 0.3081, 0.3005, 0.2949,\n",
       "           0.7247, 0.4089, 0.3063, 0.2854, 0.2914, 0.3025, 0.2935, 0.4214, 0.2888,\n",
       "           0.7247, 0.3232, 0.3071, 0.3962, 0.2934, 0.3024, 0.3044, 0.3534, 0.3227,\n",
       "           0.2948, 0.2795, 0.3962, 0.3481, 0.7247, 0.3801, 0.2745, 0.2870, 0.3355,\n",
       "           0.3632, 0.3162, 0.3632, 0.3043]),\n",
       "   tensor([0.2497, 0.2415, 0.3682, 0.2836, 0.2791, 0.2361, 0.3716, 0.3175, 0.3010,\n",
       "           0.2096, 0.2970, 0.2365, 0.2639, 0.2309, 0.2737, 0.2746, 0.2380, 0.2812,\n",
       "           0.2721, 0.2829, 0.3682, 0.2055, 0.3347, 0.2970, 0.2167, 0.2839, 0.2673,\n",
       "           0.2757, 0.2891, 0.2076, 0.3682, 0.2042, 0.2970, 0.2364, 0.3224, 0.3352,\n",
       "           0.3506, 0.2034, 0.3682, 0.2625, 0.2055, 0.2970, 0.2046, 0.2519, 0.2141,\n",
       "           0.3347, 0.2343, 0.3246, 0.2970, 0.3190, 0.3246, 0.2519, 0.3347, 0.2921,\n",
       "           0.2207, 0.3347, 0.2055, 0.2545, 0.2164, 0.2970, 0.2335, 0.2929, 0.3170,\n",
       "           0.2582, 0.3175, 0.2611, 0.3352, 0.2519, 0.3190, 0.2891, 0.3682, 0.2569,\n",
       "           0.3506, 0.2359, 0.2098, 0.2248, 0.2507, 0.3306, 0.2564, 0.1938, 0.2974,\n",
       "           0.3506, 0.2708, 0.3347, 0.2268, 0.2726, 0.4354, 0.2359, 0.2970, 0.2191,\n",
       "           0.2314, 0.2767, 0.2455, 0.2285, 0.3506, 0.2363, 0.2837, 0.3175, 0.2169,\n",
       "           0.2369, 0.2640, 0.3347, 0.2370, 0.2151, 0.2457, 0.2417, 0.2887, 0.2191,\n",
       "           0.2970, 0.2055, 0.2012, 0.2506, 0.3246, 0.2977, 0.3347, 0.2469, 0.3682,\n",
       "           0.2896, 0.2631, 0.3682, 0.2055, 0.2970, 0.2507, 0.2769, 0.3066, 0.2538,\n",
       "           0.2526, 0.3090, 0.3506, 0.2965, 0.2030, 0.3352, 0.3682, 0.2166, 0.3347,\n",
       "           0.2055, 0.2293, 0.2176, 0.2039, 0.2346, 0.2443, 0.2908, 0.2201, 0.3347,\n",
       "           0.2507, 0.2917, 0.2507, 0.2656, 0.2061, 0.2348, 0.2121, 0.3347, 0.3347,\n",
       "           0.2099, 0.3091, 0.2373, 0.2969, 0.2970, 0.3079, 0.1956, 0.2478, 0.2642,\n",
       "           0.3306, 0.3306, 0.2859, 0.2535, 0.2090, 0.3306, 0.3306, 0.2080, 0.3347,\n",
       "           0.2390, 0.1946, 0.3682, 0.3506, 0.3682, 0.2698, 0.3032, 0.2884, 0.2801,\n",
       "           0.3305, 0.2929, 0.2670, 0.2960, 0.2907, 0.2720, 0.2879, 0.2707, 0.2789,\n",
       "           0.2910, 0.3190, 0.2626, 0.2814, 0.2919, 0.2952, 0.2949, 0.2642, 0.3009,\n",
       "           0.2827, 0.5224, 0.4978, 0.3306, 0.2919, 0.2642, 0.2970, 0.2970, 0.3306,\n",
       "           0.5668, 0.2919, 0.3009, 0.2642, 0.2949, 0.2949, 0.2952, 0.4176, 0.2970,\n",
       "           0.3306, 0.3009, 0.2970, 0.3009, 0.2642, 0.3306, 0.2949, 0.3009, 0.2944,\n",
       "           0.2944, 0.2952, 0.2642, 0.2949, 0.2919, 0.2944, 0.2642, 0.2970, 0.2919,\n",
       "           0.2919, 0.2919, 0.2949, 0.4812, 0.2642, 0.2944, 0.2944, 0.3246, 0.2932,\n",
       "           0.3009, 0.2970, 0.2919, 0.2949, 0.2642, 0.2970, 0.2970, 0.2642, 0.2970,\n",
       "           0.2952, 0.2919, 0.2970, 0.2949, 0.5395, 0.2970, 0.2952, 0.2919, 0.2642,\n",
       "           0.3306, 0.2944, 0.4376, 0.2642, 0.3246, 0.2881, 0.3306, 0.2642, 0.2970,\n",
       "           0.3306, 0.2952, 0.2858, 0.2949, 0.2919, 0.2949, 0.2944, 0.2970, 0.2952,\n",
       "           0.3009, 0.2952, 0.3246, 0.2944, 0.2949, 0.3246, 0.3009, 0.2952, 0.2949,\n",
       "           0.2944, 0.3306, 0.2642, 0.2877, 0.2949, 0.3009, 0.2944, 0.2970, 0.2970,\n",
       "           0.2970, 0.2528, 0.2449, 0.2582, 0.2317, 0.2532, 0.2433, 0.2319, 0.2529,\n",
       "           0.2308, 0.2347, 0.2322, 0.2351, 0.2350, 0.2460, 0.2372, 0.2441, 0.2352,\n",
       "           0.2316, 0.2324, 0.2302, 0.2334, 0.2316, 0.2359, 0.2337, 0.2310, 0.2343,\n",
       "           0.2686, 0.2222, 0.2513, 0.2195, 0.2228, 0.2217, 0.2813, 0.2181, 0.2524,\n",
       "           0.2297, 0.2451, 0.2631, 0.2256, 0.2597, 0.2192, 0.2250, 0.2642, 0.2570,\n",
       "           0.2184, 0.2489, 0.2198, 0.2217, 0.2205, 0.2190, 0.2780, 0.2151, 0.2229,\n",
       "           0.2575, 0.2248, 0.2241, 0.2541, 0.2335, 0.2164, 0.2190, 0.2235, 0.2172,\n",
       "           0.2597, 0.2196, 0.2196, 0.2217, 0.2274, 0.2184, 0.2123, 0.2317, 0.2214,\n",
       "           0.2239, 0.2642, 0.2220, 0.2236, 0.2596, 0.2203, 0.2631, 0.2607, 0.2294,\n",
       "           0.2143, 0.2268, 0.2214, 0.2199, 0.2206, 0.2646, 0.2306, 0.2228, 0.2171,\n",
       "           0.2215, 0.2299, 0.2441, 0.2352, 0.2217, 0.2553, 0.2212, 0.2363, 0.2626,\n",
       "           0.2191, 0.2252, 0.2156, 0.2262, 0.2146, 0.2230, 0.2186, 0.2220, 0.2426,\n",
       "           0.2137, 0.2213, 0.2346, 0.2137, 0.2405, 0.2731, 0.2231, 0.2238, 0.2262,\n",
       "           0.2139, 0.2221]),\n",
       "   tensor([0.3007, 0.2970, 0.2970, 0.5094, 0.5094, 0.3007, 0.3291, 0.3249, 0.3291,\n",
       "           0.3091, 0.3091, 0.3091, 0.2970, 0.3641, 0.3321, 0.3007, 0.3007, 0.2970,\n",
       "           0.3249, 0.3291, 0.5094, 0.2969, 0.3091, 0.3007, 0.3321, 0.3007, 0.3776,\n",
       "           0.3091, 0.3091, 0.3291, 0.2970, 0.3822, 0.2970, 0.3321, 0.2970, 0.3776,\n",
       "           0.5094, 0.3007, 0.2970, 0.3007, 0.2970, 0.3822, 0.3091, 0.3091, 0.3291,\n",
       "           0.3249, 0.2954, 0.5094, 0.3091, 0.3249, 0.3361, 0.2970, 0.2980, 0.2970,\n",
       "           0.5094, 0.2970, 0.2899, 0.3249, 0.3091, 0.3051, 0.3091, 0.3822, 0.3321,\n",
       "           0.3321, 0.3249, 0.5094, 0.3822, 0.3249, 0.2980, 0.2938, 0.3321, 0.3291,\n",
       "           0.3822, 0.2848, 0.3007, 0.5094, 0.2980, 0.3822, 0.3776, 0.3776, 0.2879,\n",
       "           0.3641, 0.5094, 0.2970, 0.3321, 0.3007, 0.3321, 0.3776, 0.3361, 0.3091,\n",
       "           0.3249, 0.3291, 0.2980, 0.3007, 0.3291, 0.3776, 0.2901, 0.3091, 0.3822,\n",
       "           0.3007, 0.3249, 0.3291, 0.2980, 0.3007, 0.2970, 0.3291, 0.3007, 0.3776,\n",
       "           0.3091, 0.2871, 0.2970, 0.2962, 0.2849, 0.3776, 0.3291, 0.2970, 0.2970,\n",
       "           0.2940, 0.3249, 0.3776, 0.3291, 0.2970, 0.3776, 0.3007, 0.3091, 0.3012,\n",
       "           0.3007, 0.3822, 0.2980, 0.3291, 0.3007, 0.3042, 0.2889, 0.3776, 0.3011,\n",
       "           0.2980, 0.3291, 0.3321, 0.2970, 0.3641, 0.2970, 0.3291, 0.2870, 0.3291,\n",
       "           0.2980, 0.2970, 0.2949, 0.2970, 0.3091, 0.3249, 0.3321, 0.2970, 0.3249,\n",
       "           0.3007, 0.3091, 0.3051, 0.3822, 0.2854, 0.2985, 0.3007, 0.3822, 0.2864,\n",
       "           0.3321, 0.3091, 0.3291, 0.3258, 0.3641, 0.3361, 0.5094, 0.3091, 0.3091,\n",
       "           0.3321, 0.3249, 0.2942, 0.3026, 0.3249, 0.2851, 0.3321, 0.3091, 0.2963,\n",
       "           0.2980, 0.2970, 0.3091, 0.3249, 0.5094, 0.2970, 0.3091, 0.3091, 0.3822,\n",
       "           0.4317, 0.3341, 0.4317, 0.3747, 0.3575, 0.3747, 0.4519, 0.3286, 0.3037,\n",
       "           0.5115, 0.4002, 0.3170, 0.4225, 0.3138, 0.5548, 0.3747, 0.5134, 0.4266,\n",
       "           0.4317, 0.4168, 0.4010, 0.3711, 0.4914, 0.4115, 0.3747, 0.3446, 0.5253,\n",
       "           0.3249, 0.3701, 0.3473, 0.3158, 0.3105, 0.4225, 0.2945, 0.4317, 0.3849,\n",
       "           0.4932, 0.3867, 0.3215, 0.3468, 0.6381, 0.4035, 0.3741, 0.4087, 0.4786,\n",
       "           0.2890, 0.4229, 0.3252, 0.3003, 0.4786, 0.2891, 0.5854, 0.3420, 0.3071,\n",
       "           0.4184, 0.4321, 0.3560, 0.3983, 0.2990, 0.3384, 0.3648, 0.3555, 0.3694,\n",
       "           0.3456, 0.4482, 0.3242, 0.3059, 0.3747, 0.4029, 0.4087, 0.3045, 0.3997,\n",
       "           0.5652, 0.2979, 0.3568, 0.3278, 0.3138, 0.4317, 0.3918, 0.4087, 0.4406,\n",
       "           0.3181, 0.3741, 0.4317, 0.4006, 0.4198, 0.2909, 0.3017, 0.4091, 0.3118,\n",
       "           0.4029, 0.3204, 0.6018, 0.4010, 0.4078, 0.4786, 0.3160, 0.4087, 0.4029,\n",
       "           0.3461, 0.3284, 0.3821, 0.5198, 0.3822, 0.3111, 0.9991, 0.3436, 0.3146,\n",
       "           0.3140, 0.4226, 0.3822, 0.2875, 0.3249, 0.7914, 0.3119, 0.3075, 0.3236,\n",
       "           0.3088, 0.5199, 0.3118, 0.3023, 0.2957, 0.3361, 0.5044, 0.2980, 0.3279,\n",
       "           0.3131, 0.3240, 0.3031, 0.3776, 0.3436, 0.3186, 0.5094, 0.2957, 0.3119,\n",
       "           0.3822, 0.3091, 0.5094, 0.3076, 0.3399, 0.3377, 0.4481, 0.3282, 0.2866,\n",
       "           0.3553, 0.3269, 0.3496, 0.3116, 0.3030, 0.2881, 0.3140, 0.3502, 0.3291,\n",
       "           0.3269, 0.3313, 0.5094, 0.3390, 0.3262, 0.3822, 0.9991, 0.3822, 0.5094,\n",
       "           0.3666, 0.2979, 0.3428, 0.3401, 0.3015, 0.3091, 0.9991, 0.2964, 0.3517,\n",
       "           0.3552, 0.3003, 0.5094, 0.3748, 0.3331, 0.3776, 0.3822, 0.3822, 0.4396,\n",
       "           0.4931, 0.9991, 0.9991, 0.3822, 0.3157, 0.3091, 0.3002, 0.3147, 0.9991,\n",
       "           0.3158, 0.4034, 0.2995, 0.3338, 0.3259, 0.9991, 0.3139, 0.3436, 0.3336,\n",
       "           0.3061, 0.5199, 0.2968, 0.3822, 0.3080, 0.3278, 0.3178, 0.5199, 0.3259,\n",
       "           0.3036, 0.3049, 0.3522, 0.3053, 0.3149, 0.4395, 0.3361, 0.3194, 0.3436,\n",
       "           0.3747, 0.3075, 0.3045, 0.5638, 0.3499, 0.3075, 0.3070, 0.3091, 0.3123,\n",
       "           0.3091, 0.3013, 0.3730, 0.4619, 0.3095, 0.3091, 0.3256, 0.3054, 0.3291,\n",
       "           0.4169, 0.3348, 0.3224, 0.2935, 0.3348, 0.2944, 0.3156, 0.3091, 0.3255,\n",
       "           0.2950, 0.2945, 0.3178, 0.2934, 0.3291, 0.2983, 0.9991, 0.9991, 0.3199,\n",
       "           0.3331, 1.1425, 0.3119, 0.3085, 0.3245, 0.3309, 0.3361, 0.3152, 0.3222,\n",
       "           0.3091, 0.3211, 0.2955, 0.3004, 0.3361, 0.3395, 0.3641, 0.4050, 0.3023,\n",
       "           0.3555, 0.3147, 0.3147, 0.3147, 0.2980, 0.3005, 0.3147, 0.2958, 0.2999,\n",
       "           0.2983, 0.3002, 0.3133, 0.2958, 0.3134, 0.3147, 0.3005, 0.3128, 0.2993,\n",
       "           0.3147, 0.3008, 0.3147, 0.3147, 0.2965, 0.3124, 0.3192, 0.3180, 0.3212,\n",
       "           0.3404, 0.3148, 0.3150, 0.3170, 0.3277, 0.3301, 0.3177, 0.3253, 0.3564])]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "class IVSurfaceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data, \n",
    "        proportion, \n",
    "        random_state=0\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.proportion = proportion\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        surface_data = self.data[idx]\n",
    "        \n",
    "        # Extract the surface coordinates and volatilities\n",
    "        points_coordinates = np.stack([\n",
    "            surface_data['Surface']['Log Moneyness'], \n",
    "            surface_data['Surface']['Time to Maturity']\n",
    "        ], axis=1)\n",
    "        points_volatilities = surface_data['Surface']['Implied Volatility']\n",
    "\n",
    "        # Perform clustering\n",
    "        n_clusters = int(np.ceil(1 / self.proportion))\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('kmeans', KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init='auto'))\n",
    "        ])\n",
    "        labels = pipeline.fit_predict(points_coordinates)\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "\n",
    "        masked_indices = np.array([], dtype=int)\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_indices = np.where(labels == cluster)[0]\n",
    "            num_to_mask = int(np.ceil(len(cluster_indices) * proportion))\n",
    "            masked_indices = np.append(masked_indices, [rng.choice(cluster_indices, size=num_to_mask, replace=False)])\n",
    "        \n",
    "        unmasked_indices = np.setdiff1d(range(len(labels)), masked_indices)\n",
    "\n",
    "        data_item = {\n",
    "            'Datetime': surface_data['Datetime'],\n",
    "            'Symbol': surface_data['Symbol'],\n",
    "            'Market Features': {\n",
    "                'Market Return': torch.tensor(surface_data['Market Features']['Market Return'], dtype=torch.float32),\n",
    "                'Market Volatility': torch.tensor(surface_data['Market Features']['Market Volatility'], dtype=torch.float32),\n",
    "                'Treasury Rate': torch.tensor(surface_data['Market Features']['Treasury Rate'], dtype=torch.float32),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[unmasked_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[unmasked_indices, 1], dtype=torch.float32),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[unmasked_indices], dtype=torch.float32)\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[masked_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[masked_indices, 1], dtype=torch.float32),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[masked_indices], dtype=torch.float32)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return data_item\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batched_data = {\n",
    "            'Datetime': [item['Datetime'] for item in batch],\n",
    "            'Symbol': [item['Symbol'] for item in batch],\n",
    "            'Market Features': {\n",
    "                'Market Return': default_collate([item['Market Features']['Market Return'] for item in batch]),\n",
    "                'Market Volatility': default_collate([item['Market Features']['Market Volatility'] for item in batch]),\n",
    "                'Treasury Rate': default_collate([item['Market Features']['Treasury Rate'] for item in batch]),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': [item['Input Surface']['Log Moneyness'].clone().detach() for item in batch],\n",
    "                'Time to Maturity': [item['Input Surface']['Time to Maturity'].clone().detach() for item in batch],\n",
    "                'Implied Volatility': [item['Input Surface']['Implied Volatility'].clone().detach() for item in batch],\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': [item['Query Points']['Log Moneyness'].clone().detach().requires_grad_(True) for item in batch],\n",
    "                'Time to Maturity': [item['Query Points']['Time to Maturity'].clone().detach().requires_grad_(True) for item in batch],\n",
    "                'Implied Volatility': [item['Query Points']['Implied Volatility'].clone().detach() for item in batch],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "# Assuming surfaces is the output from the implied_volatility_surfaces function\n",
    "proportion = 0.2  # example proportion\n",
    "dataset = IVSurfaceDataset(surfaces, proportion)\n",
    "data_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=HYPERPARAMETERS['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Fetch one batch from the DataLoader\n",
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00'),\n",
       "  Timestamp('2013-03-07 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL', 'AAPL'],\n",
       " 'Market Features': {'Market Return': tensor([-0.0216, -0.4603, -0.1269,  0.6087], grad_fn=<SqueezeBackward1>),\n",
       "  'Market Volatility': tensor([ 1.6897, -0.2052, -0.7625, -0.7220], grad_fn=<SqueezeBackward1>),\n",
       "  'Treasury Rate': tensor([-0.7439,  0.1717, -0.9728,  1.5450], grad_fn=<SqueezeBackward1>)},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-1.0316, -0.9561, -0.8829,  ...,  1.8599,  1.8834,  1.8834],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.8722, -0.8722, -0.8374,  ...,  1.7995,  1.8230,  1.8230],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-1.1743, -1.1547, -1.1353,  ...,  0.6968,  0.7331,  0.7511],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.6958, -0.6958, -0.6619,  ...,  1.9068,  1.9303,  1.9303],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.9064, -0.9064, -0.9064,  ...,  2.4460,  2.4460,  2.4460],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9064, -0.9064, -0.9064,  ...,  3.2121,  3.2121,  3.2121],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9064, -0.9064, -0.9064,  ...,  2.5669,  2.5669,  2.5669],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9237, -0.9237, -0.9237,  ...,  2.9932,  2.9932,  2.9932],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.4187, 0.3716, 0.3716,  ..., 0.3194, 0.3320, 0.3194]),\n",
       "   tensor([0.4101, 0.4484, 0.4101,  ..., 0.3084, 0.2916, 0.3084]),\n",
       "   tensor([0.3347, 0.3347, 0.3347,  ..., 0.2339, 0.2302, 0.2355]),\n",
       "   tensor([0.5199, 0.7914, 0.5199,  ..., 0.3147, 0.2958, 0.3147])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([ 1.5988,  0.3847,  0.7443,  0.3847,  1.5458,  0.7995,  1.6763,  0.8887,\n",
       "            0.7069,  0.2292,  1.4640,  0.5711,  0.1358,  1.5724,  0.1358,  0.0137,\n",
       "            1.1065,  0.2520,  0.7812,  0.0137,  0.7257,  0.0877,  0.2062,  0.5309,\n",
       "            0.0877, -0.1681,  0.5105, -0.1413,  0.0386,  1.2618,  0.2062, -0.1413,\n",
       "            0.2747, -0.0626,  1.7881,  1.5857,  0.7812,  1.6249,  0.7629,  0.5511,\n",
       "            1.2769,  0.1358,  1.1542,  0.0632,  0.0137,  0.1358,  0.6879, -0.1681,\n",
       "           -0.0626,  0.4693,  1.5458,  1.4640, -0.0886,  0.4061, -0.1681,  1.3213,\n",
       "            0.6879,  0.0386,  0.1595, -0.1681,  0.4061,  1.3794,  0.3631, -0.0115,\n",
       "            0.2747,  0.7443,  0.6107,  0.6689,  0.3193,  0.1829, -0.1413,  0.4274,\n",
       "            1.3794,  0.2747,  1.0742,  0.2747,  0.2062,  0.2971, -0.1148,  0.8356,\n",
       "           -0.1681,  0.3413,  0.5711,  0.4693,  0.2971, -0.1952,  1.0250,  1.0084,\n",
       "            1.6507,  0.2520, -0.0626,  1.3360,  0.2971,  0.2971,  0.1118,  1.4221,\n",
       "           -0.0626,  1.3213,  0.2062,  0.8356, -0.0369,  0.0877,  1.1225,  1.1698,\n",
       "            0.2292,  1.3066,  0.2747,  1.6636,  0.7069, -0.0115,  0.0877,  1.2618,\n",
       "            1.2316,  1.2918,  0.2062,  0.3631,  0.8887,  0.2520,  1.4221,  1.6636,\n",
       "           -0.1952,  1.6379, -0.0886,  0.7069,  1.5323,  0.5105,  0.2062,  0.4061,\n",
       "           -0.1681, -0.0369,  0.0632,  0.3193,  0.0386,  0.4061,  0.5910,  0.7443,\n",
       "           -0.0369,  0.2292,  1.0415,  0.3193,  0.3847, -0.0369,  1.7390,  0.1829,\n",
       "            1.4079,  0.9062,  0.2520,  0.0877,  0.4900,  0.1358,  0.3413,  0.4484,\n",
       "            0.6496,  1.6763, -0.0626,  0.4061,  0.3193,  1.3066,  0.2520,  1.3794,\n",
       "           -0.1681,  0.5105,  0.4061,  0.3631,  0.0877,  0.1595,  0.2971,  0.5711,\n",
       "           -0.5150, -1.8418, -0.5463, -1.1899, -1.7374, -0.8118, -0.5779, -1.1095,\n",
       "           -0.1952, -0.3355, -2.0072, -0.5779, -1.0702, -2.1241, -0.6754, -1.0316,\n",
       "           -0.3939, -0.1681, -1.1899, -1.5886, -0.7770, -1.4943, -1.7890, -1.2312,\n",
       "           -1.4484, -0.7427, -1.8957, -1.9508, -0.7088, -0.3939, -0.2226, -0.8829,\n",
       "           -0.4236, -0.5779, -0.7770, -0.4236, -0.3068, -0.6100, -0.7088, -0.9936,\n",
       "           -0.4236, -1.2312, -0.9561, -0.6100, -1.3592, -1.0702, -1.5410, -1.3158,\n",
       "           -0.7088, -0.5463, -1.1494, -0.5779, -1.1095, -2.0649, -2.0649, -0.9936,\n",
       "           -0.6754, -0.3645, -1.9508, -0.9192, -0.7427, -0.7088, -0.3645, -0.7088,\n",
       "           -1.4034, -0.8118, -1.7374, -0.3068, -1.0316, -0.5463, -0.3645, -1.0316,\n",
       "           -1.7374, -0.9936, -1.2731, -1.4034, -0.6100, -2.0072, -1.2731, -1.3158,\n",
       "           -1.9508, -0.2504, -1.1899, -1.0702, -1.2731, -2.1241, -1.8957, -1.8418,\n",
       "           -0.6100, -0.2226, -1.4484, -1.7374, -0.2504, -0.6425, -0.6100, -1.1494,\n",
       "           -1.9508, -0.2504, -0.1681, -0.7770, -1.4943, -1.7374, -0.4537, -1.0702,\n",
       "           -0.4842, -0.2784, -0.3068, -0.8829, -0.9936, -0.6425, -1.6372, -0.8471,\n",
       "           -0.6425, -1.6868, -0.6754, -2.1846, -0.5150, -1.4484, -0.7427, -0.7427,\n",
       "           -0.3939, -0.5779, -0.8829, -0.5463, -0.3645, -2.2467, -1.7374, -0.9192,\n",
       "           -0.7088, -0.6425, -0.7088, -0.1952, -0.9936, -0.1952, -1.7374, -0.9561,\n",
       "           -0.3939, -1.1095, -0.3645, -0.2504, -0.3645, -0.8118, -0.3645, -0.2504,\n",
       "           -0.3645, -1.5886, -1.8418, -0.8118, -1.0316, -1.5410, -0.4842, -0.2226,\n",
       "           -2.0072, -0.4236, -0.5779, -1.5410, -0.2504, -0.7088, -1.1494, -0.6754,\n",
       "           -0.3939, -1.0316, -0.4537, -2.1846, -0.4842, -1.4034, -1.2312, -0.4236,\n",
       "           -0.5779, -0.3355, -1.1899, -0.5463, -1.8418, -0.4537, -1.1899, -0.6754,\n",
       "           -1.3158, -1.6372, -1.9508,  1.1225,  1.6379,  1.5857,  1.0579,  1.8123,\n",
       "            1.7637,  0.0877,  0.3413, -0.2504,  0.1358,  0.7812,  1.5052,  0.1358,\n",
       "            0.7443,  0.9578,  1.8599, -0.1952,  0.7069, -0.1952, -0.1413,  1.1854,\n",
       "            1.6119,  1.4778,  0.1829,  0.6302, -0.3645,  1.5323, -0.0369,  0.2062,\n",
       "            0.8176,  1.8362,  1.0579,  0.0137,  0.9407,  1.2316,  0.2747,  1.3505,\n",
       "            0.4274,  0.1595,  0.8534,  0.1118,  0.4693,  1.0250, -0.1413,  0.7257,\n",
       "            0.6302,  0.5105,  1.2009,  0.0632,  1.4079,  0.4900,  1.6763,  1.3937,\n",
       "            1.1065,  0.0386, -0.0626,  1.3360,  1.6249,  1.5458,  1.6119,  0.2292,\n",
       "            1.5323,  1.0904,  0.2971,  1.8599,  0.4900,  0.9407,  0.9578,  0.4274,\n",
       "            0.6302,  0.4061,  0.3193,  0.2747,  0.1118,  0.5309,  1.3650,  1.6763,\n",
       "            1.3066,  0.9916,  0.6496,  0.8356,  1.7390,  1.5988, -0.1148,  1.3213,\n",
       "            1.0084,  0.9578,  1.2769,  1.8599,  0.3413,  0.4061,  0.1118,  0.7812,\n",
       "            0.4693,  0.7069,  0.2292,  0.7443,  1.1384,  1.6890,  0.1829,  0.8534,\n",
       "            1.8362,  1.6507,  0.5309,  0.6689,  1.5052,  0.6689,  1.0904,  1.3360,\n",
       "            0.7812,  1.2618,  1.1225,  1.5052], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([ 0.0991,  0.0754, -0.5141, -0.2017, -0.2285, -0.3959,  0.5503,  0.2809,\n",
       "            0.5306,  0.0273,  0.4089,  0.2809, -0.0973,  0.5107, -0.3959, -0.2830,\n",
       "            0.3670, -0.4249, -0.2556, -0.3108, -0.0719, -0.1230, -0.0218,  0.1225,\n",
       "           -0.1230, -0.3108, -0.0218, -0.3672, -0.2285,  0.1688, -0.8031,  0.4501,\n",
       "            0.0514,  0.4089,  0.4089,  0.5892, -0.3959,  0.5107,  0.0028,  0.0028,\n",
       "           -0.3388,  0.0273,  0.2143,  0.2809,  0.5503, -0.2017, -0.1752,  0.2143,\n",
       "           -0.0467,  0.4705,  0.1688,  0.1916,  0.3880, -0.5141,  0.0273, -0.7358,\n",
       "           -0.5446,  0.1916,  0.5698, -0.5141, -0.2285, -0.4543,  0.4501,  0.1916,\n",
       "           -0.0719, -0.1752,  0.4501,  0.2589,  0.5698, -0.3959,  0.0514, -0.2017,\n",
       "           -0.5141, -0.2285,  0.3670,  0.1458, -0.7692,  0.2143, -0.0719,  0.3880,\n",
       "            0.5503, -0.3388, -0.0218, -0.2285,  0.0273, -0.2830,  0.4705, -0.3672,\n",
       "            0.3457,  0.2143, -0.0973, -0.1490,  0.0991, -0.2556,  0.1225,  0.4296,\n",
       "           -0.3108,  0.0754,  0.2367,  0.4907,  0.1916, -0.4543, -0.4249,  0.3670,\n",
       "            0.2367,  0.2367,  0.4907,  0.3243, -0.0218, -0.3108, -0.6704,  0.2143,\n",
       "           -0.0719, -0.0973, -0.2556,  0.2143, -0.6704, -0.3108, -0.2285, -0.1490,\n",
       "           -0.6383,  0.0991,  0.3243, -0.3388, -0.4840, -0.3108,  0.1225,  0.5107,\n",
       "            0.4907,  0.5306,  0.4705,  0.4296,  0.0991,  0.3027, -0.3959, -0.2017,\n",
       "           -0.0218, -0.3672,  0.4705,  0.3457,  0.1916, -0.4840,  0.2809,  0.3457,\n",
       "            0.1688, -0.3108,  0.4907,  0.0028, -0.6704, -0.1230, -0.0467, -0.5754,\n",
       "            0.1916,  0.1458, -0.5446,  0.2367, -0.3388, -0.4840,  0.4296,  0.4089,\n",
       "            0.2589,  0.0273,  0.0028, -0.7029, -0.3959, -0.0218,  0.3243,  0.0754,\n",
       "            0.4089, -0.5446, -0.6067,  0.3880,  0.3670, -0.8374, -0.6704,  0.0754,\n",
       "            1.4719,  1.5515,  0.6085,  0.1225,  0.7391,  1.5775,  1.5645,  0.2367,\n",
       "            1.0938, -0.2556,  0.8803,  0.0991,  0.0514,  1.3046,  0.0273,  1.6412,\n",
       "            0.2589,  1.3757,  0.6275,  0.4705,  0.9811,  0.1688,  0.9975,  0.6653,\n",
       "            0.4907,  0.7391,  0.8458,  1.1864,  1.0461,  1.4584,  1.3617,  0.6275,\n",
       "            0.4089,  0.7930,  1.7995,  1.1712,  0.9646,  0.6085,  0.1458,  1.2014,\n",
       "            0.9312,  1.5645,  0.3243,  0.7930,  0.1458,  0.3243, -0.0467,  1.1864,\n",
       "            1.0461,  1.1405,  0.7930,  0.9646,  1.3046,  0.0514,  0.6085,  1.4987,\n",
       "            0.3880,  0.1916,  0.3027,  1.4448,  0.4501,  0.0028, -0.1230,  1.6159,\n",
       "           -0.0218,  1.1712,  0.9144, -0.0719,  0.1916,  0.6085,  0.8974,  0.0514,\n",
       "            0.2809,  0.6465,  0.9480,  1.3475,  1.5120,  0.2143,  0.5698,  1.0780,\n",
       "            1.1094,  1.5775,  1.6662,  1.6159,  1.0461,  1.5253,  1.1559,  1.5120,\n",
       "            0.9144,  1.0461,  1.4584,  0.9811,  1.3190,  1.7033,  1.3475,  0.7025,\n",
       "            1.1094,  0.8631,  1.4036,  1.5903,  1.7758,  1.3757,  1.0300,  0.8631,\n",
       "            1.2756,  1.4174,  0.8458,  1.2901,  0.7752,  0.8107,  1.5120,  1.6537,\n",
       "            1.5253,  0.8974,  1.0300,  0.8283,  1.7758,  1.0621,  1.2165,  0.5503,\n",
       "            1.2609,  0.8458,  0.8283,  1.4719,  1.6786,  0.8107,  0.8631,  1.5775,\n",
       "            0.6653,  1.5903,  1.7277,  1.6412,  0.7025,  0.9646,  1.5645,  0.7391,\n",
       "            1.3617,  1.5515,  0.6465,  0.8283,  0.6653,  1.2901,  0.6653,  1.5903,\n",
       "            0.7572,  1.6537,  1.8230,  1.0461,  0.7572,  0.7572,  1.4312,  1.5384,\n",
       "            1.0300,  1.1864,  0.9975,  0.7208,  0.6275,  1.0621,  1.4174,  0.8974,\n",
       "            1.4854,  1.3617,  0.7572,  1.5903,  1.1559,  1.4036,  0.8458,  1.3757,\n",
       "            1.1250,  0.8974,  0.8107,  1.2165,  0.5892,  0.7572,  1.4174,  1.3046,\n",
       "            1.1712,  0.7391,  0.7025,  1.4036,  1.2462,  0.6839,  1.6537,  0.6839,\n",
       "            1.2165,  1.5120,  0.7930,  0.9811,  0.7391,  1.8230,  1.0621,  1.2901,\n",
       "            1.3190,  0.6465,  0.5892,  0.7752,  1.6159,  0.6653,  0.6465,  1.6159,\n",
       "            1.3333,  1.0621,  1.3897,  0.9480,  0.9811,  0.6085,  1.6159,  0.6839,\n",
       "            0.8283,  1.4987,  1.5515,  1.0461,  1.3190,  1.3757,  1.0621,  1.6159,\n",
       "            1.2314,  1.1094,  1.4719,  0.5892,  1.1559,  0.6839,  0.6085,  0.7025,\n",
       "            1.6786,  0.8458,  1.6286,  1.3757,  0.5107,  1.4036,  0.5698,  1.1094,\n",
       "            0.9312,  0.5306,  1.4987,  1.4312,  1.4719,  1.3897,  0.7391,  1.6910,\n",
       "            0.9312,  1.2462,  1.5515, -1.3335, -0.4840,  0.5698, -0.8722, -1.0920,\n",
       "            1.2756,  1.6286,  0.0514,  0.2809, -0.0973,  1.0938,  0.4907, -0.7358,\n",
       "           -1.2503,  0.9975,  1.3897, -0.2017, -0.4249,  0.9312, -0.2017,  0.9312,\n",
       "            0.6839, -1.1699, -0.6067, -0.8031,  1.1559,  0.9975, -1.6976,  1.2165,\n",
       "            1.6537,  0.4501, -0.1490, -1.2098, -1.2503, -1.4638, -0.7692, -1.0920,\n",
       "           -1.0540, -1.4196, -0.7692, -1.3335, -1.0540, -0.5141, -1.4196, -1.2916,\n",
       "           -0.8374, -0.7692, -1.3762, -0.9796, -0.3959, -1.0920, -1.3335, -1.3762,\n",
       "           -0.6383, -1.1306, -1.5088, -0.5446, -0.4543, -0.8722, -1.4638, -1.0165,\n",
       "           -0.7692, -0.9075, -0.9075, -1.3335, -0.7358, -1.3762, -1.6490, -1.4638,\n",
       "           -1.5088, -1.1306, -0.9075, -1.4196, -0.6383, -1.4638, -0.9796, -1.1699,\n",
       "           -0.7029, -0.4840, -0.7692, -0.7358, -1.5088, -1.4196, -0.4840, -0.4249,\n",
       "           -1.6014, -0.8374, -0.5754, -1.1699, -1.3335, -1.5547, -0.8374, -0.2017,\n",
       "           -1.7978, -0.8031, -0.9796, -1.5547, -1.6014, -0.6704, -0.8722, -0.8031,\n",
       "           -0.6067, -0.6383, -1.0920, -1.2503, -0.7029, -0.6383, -0.6067, -0.9433,\n",
       "           -0.7358, -1.0920, -0.5446, -1.1306, -1.0540, -0.8374, -1.1306, -0.7029,\n",
       "           -0.7692, -0.6704, -1.0920, -1.0540, -0.8031, -0.4840, -0.8031, -1.2098,\n",
       "           -1.0165, -1.1699, -0.3388, -0.4840, -1.4638, -1.2503, -0.7692, -1.1306,\n",
       "           -0.8722], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.3071, -0.3488, -0.9855, -0.8614, -0.8788, -0.0090, -0.6624, -0.8962,\n",
       "           -0.6945, -0.2125, -0.7765, -0.1993, -0.4199, -0.0583, -0.1993, -0.5231,\n",
       "           -0.1471, -0.4783, -0.5081, -0.5382, -0.7107,  0.4055, -0.6784, -0.9855,\n",
       "           -0.3071, -0.5382, -0.7434, -0.1861, -0.8788, -0.2661, -1.1547, -0.0959,\n",
       "           -1.0406, -0.4490, -0.7933, -0.5995, -0.6151, -0.0583, -0.6624, -0.3912,\n",
       "            0.2348, -1.0592, -0.2391,  0.3002, -0.0335, -0.7933,  0.0868, -0.9494,\n",
       "           -1.1161, -0.7933, -0.9855,  0.2568, -0.4055, -0.6465, -0.3488, -1.1161,\n",
       "            0.0750, -0.5995, -0.0212, -0.6465,  0.0632, -0.6151, -0.7599, -0.4490,\n",
       "           -1.1743, -0.4636, -0.8271,  0.2894, -0.6308, -0.5840, -0.6308, -0.4783,\n",
       "           -0.6784,  0.3216, -0.2526, -0.3071, -0.8101, -0.9138, -0.4055, -0.1342,\n",
       "           -0.6784, -0.5840, -0.4055, -0.8101, -0.3488, -0.5231, -0.7933,  0.2126,\n",
       "           -0.8101,  0.2677,  0.1217, -0.8271, -0.3348, -0.0459, -1.0780, -0.1086,\n",
       "           -0.9315, -1.0592, -0.2526, -0.0708, -0.4490, -0.9494, -0.0212, -0.1730,\n",
       "           -0.5686, -0.2258, -0.9855,  0.3216, -1.0970,  0.2568, -0.2258, -0.5840,\n",
       "           -1.1743, -0.4490, -0.8271, -0.5840, -1.0780, -0.1600, -0.4932, -0.7599,\n",
       "            0.0032, -1.0037, -0.9138, -0.5534, -0.6308, -0.4199, -0.6465, -0.7107,\n",
       "           -0.8442, -0.3348, -0.0833, -0.8614, -0.5995, -0.2125, -0.9138,  0.3429,\n",
       "           -0.3770, -0.0959, -0.0833, -0.1600, -0.2526, -0.6308, -0.3348, -0.4344,\n",
       "           -1.0406, -0.9315, -0.8442, -0.7270, -0.2258,  0.1101, -0.2526, -1.0221,\n",
       "           -0.5382, -0.2797, -0.5382, -0.3912, -0.9494, -0.6308, -0.7765, -0.1214,\n",
       "           -0.5534, -0.7107, -1.0780, -0.7434, -0.5840, -0.3770, -0.2797, -1.0221,\n",
       "           -0.7765, -0.1730, -1.0406, -0.4636, -0.1471, -1.1743, -0.7599, -0.6151,\n",
       "           -1.0406, -2.1494, -1.4879, -1.2744, -2.3949, -1.6731, -0.9673, -2.4604,\n",
       "           -1.5787, -1.0780, -1.4437, -1.0406, -1.2744, -1.4879, -2.1494, -0.8614,\n",
       "           -1.3156, -1.3365, -2.0633, -2.7042, -1.1353, -1.9802, -1.0780, -1.5557,\n",
       "           -1.4437, -2.4274, -1.6492, -1.2138, -1.5787, -1.5557, -2.1204, -1.7463,\n",
       "           -2.1494, -1.4879, -1.0780, -2.4274, -1.4657, -1.2744, -1.2339, -2.2085,\n",
       "           -2.1494, -1.9262, -1.4219, -1.6255, -2.3629, -2.1788, -1.6255, -1.3576,\n",
       "           -1.9802, -1.7463, -1.5787, -2.2085, -2.0633, -1.1547, -1.4879, -2.3000,\n",
       "           -1.2744, -2.2691, -1.2949, -1.4219, -1.5103, -1.4002, -2.2691, -2.0917,\n",
       "           -1.3156, -1.5557, -1.1743, -1.4437, -2.3629, -2.2386, -2.0076, -2.7042,\n",
       "           -2.7042, -1.3365, -2.1494, -2.5276, -1.3365, -1.6020, -1.3576, -2.4604,\n",
       "           -1.6020, -1.5103, -1.1547, -1.2744, -1.3156, -1.8997, -1.5557, -1.2540,\n",
       "           -2.4937, -1.5103, -1.0780, -1.8218, -2.0076, -1.7712, -2.2085, -1.2540,\n",
       "           -1.0970, -1.4002, -1.6731, -1.4219, -1.6973, -2.2691, -1.5329, -1.7712,\n",
       "           -1.6020, -1.4879, -1.4437, -2.1204, -1.3365, -1.4219, -1.4219, -1.3156,\n",
       "           -2.1204, -2.4937, -1.2540, -1.0592, -1.7712, -1.5557, -1.2744, -1.7964,\n",
       "           -2.0917, -2.0076, -0.5382, -0.3071, -0.6945,  0.4666, -0.5686, -0.2258,\n",
       "            0.3848, -0.5686,  0.6968,  0.1448,  0.4866,  0.7150,  0.7331, -0.3348,\n",
       "            0.0032, -0.2526,  0.1217,  0.5065,  0.3216,  0.7511,  0.2348,  0.4866,\n",
       "            0.1217,  0.2126,  0.7150,  0.1902, -0.9315, -0.1861, -0.6945,  0.0985,\n",
       "           -0.0959,  0.0985, -1.0037, -0.1214, -0.7765, -0.3770, -0.6151, -0.7933,\n",
       "           -0.2934, -0.8271,  0.0274, -0.3071, -0.9494, -0.7107,  0.0153, -0.6784,\n",
       "           -0.1861, -0.1730,  0.1676,  0.0032, -0.9673,  0.0985,  0.2348, -0.8101,\n",
       "            0.1902,  0.2126, -0.6945, -0.4932,  0.0032, -0.0335, -0.1471, -0.0959,\n",
       "           -0.7434, -0.1861,  0.1448, -0.1342, -0.3071,  0.0032,  0.1676, -0.3629,\n",
       "           -0.1214,  0.3002, -1.0037, -0.1730,  0.2568, -0.8271, -0.0708, -0.8962,\n",
       "           -0.8962, -0.3348,  0.0032,  0.3002, -0.1600, -0.0833,  0.3002, -0.8788,\n",
       "           -0.3629, -0.1086,  0.0750,  0.1448,  0.3216, -0.5995, -0.5231,  0.3216,\n",
       "           -0.7599, -0.1993, -0.4636, -0.8101,  0.0750, -0.2526,  0.2126, -0.3348,\n",
       "            0.0274,  0.2786, -0.0583,  0.3639, -0.5840,  0.3216, -0.2258, -0.3770,\n",
       "            0.3429, -0.4783, -0.9673,  0.1902, -0.2934, -0.2258,  0.2568, -0.1861],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([ 1.7735,  1.5521,  1.7485,  0.9356,  1.4548,  1.6588,  1.6718,  1.6976,\n",
       "            1.0553,  1.8592,  1.6976,  1.7735,  1.0385,  0.9180,  1.3237,  1.4263,\n",
       "            1.6193,  1.2632,  1.6457,  1.7232,  1.3087,  0.8645,  0.8464,  1.7859,\n",
       "            1.1211,  1.3682,  1.4690,  1.3087,  1.6588,  0.9704,  1.0553,  1.4970,\n",
       "            1.1694,  0.8281,  1.2167,  1.7232,  1.4406,  0.7726,  1.4406,  1.1048,\n",
       "            1.1211,  1.5109,  1.9068,  1.8831,  1.4406,  1.4548,  0.2761,  1.3829,\n",
       "            1.5927,  0.8098,  0.9876,  1.1534,  1.0047,  1.0884,  0.9180,  1.4119,\n",
       "            0.6965,  1.8831,  1.4263,  0.8281,  1.3237,  1.2632,  1.7232,  1.4263,\n",
       "            1.6326,  1.0884,  1.2478,  1.0047,  1.4406,  0.7726,  1.6848,  1.5109,\n",
       "            1.1534,  0.6180,  1.4406,  1.0047,  0.7726,  1.4548,  0.9530,  1.7105,\n",
       "            0.7348,  1.0047,  1.0217,  1.1534,  1.4830,  1.1211,  0.7348,  1.6588,\n",
       "            0.9356,  1.6976,  0.7726,  1.6976,  0.9356,  1.6326,  0.8098,  0.9704,\n",
       "            0.8824,  1.1373,  1.3974,  1.3829,  1.5521,  1.1694,  1.3829,  1.3087,\n",
       "            1.1853,  1.3387,  1.7485,  1.6718,  1.1694,  0.5162,  1.7232,  0.4743,\n",
       "            0.3662,  0.8098,  0.9003,  1.3535,  1.2323,  0.4100,  1.2785,  1.5792,\n",
       "            1.2011,  1.6326,  1.4548,  0.9876,  1.1048,  0.6379,  1.1534,  1.0385,\n",
       "            1.1048,  0.9356,  1.7232,  0.5980,  0.7726,  1.6326,  0.5574,  0.9704,\n",
       "            1.3535,  1.0217,  1.6457,  1.1211,  1.0217,  0.8281,  0.5778,  1.4119,\n",
       "            1.0553,  1.8831,  0.1827,  1.3535,  1.6588,  0.9003,  1.4548,  1.3387,\n",
       "            1.1534,  1.5792,  1.8592,  0.6965,  1.2323,  0.3440,  0.4953,  0.8098,\n",
       "            1.3535,  0.2761,  1.4406,  1.7735,  1.5521,  0.6576,  1.0553,  0.8645,\n",
       "            0.9530,  1.7359,  1.3682,  0.8098,  1.6718,  0.9530,  0.6180,  1.5927,\n",
       "            0.3882,  0.9356,  1.3387,  1.0385,  1.2167,  0.9876,  0.9180,  1.4119,\n",
       "            1.1211,  1.8592,  1.0553,  1.6457,  1.0047, -1.2262, -1.0233, -0.7649,\n",
       "           -0.9467, -0.6619, -0.7301, -1.5903, -0.4994, -0.2315, -1.4474, -1.3565,\n",
       "           -0.4681, -1.5903, -0.5311, -1.5903, -0.8723, -1.6399, -0.8002, -0.8360,\n",
       "           -0.9847, -1.7949, -1.6905, -0.9847, -1.4015, -1.4941, -0.9847, -1.6905,\n",
       "           -0.4681, -1.1025, -0.9092, -0.4373, -0.4681, -1.4474, -0.0944, -0.8002,\n",
       "           -1.2262, -1.3565, -1.2689, -0.7301, -0.8723, -1.3123, -1.3123, -1.6399,\n",
       "           -1.4941, -1.7421,  0.1101, -1.1430, -0.5956, -0.0944, -1.6905,  0.0606,\n",
       "           -1.6905, -1.1843, -0.3470, -1.1025, -1.4941, -0.8360, -0.9467, -0.2599,\n",
       "           -1.1025, -1.0233, -1.2689, -1.1843, -1.1025, -1.4015, -0.7649, -0.4068,\n",
       "           -1.2689, -1.3565, -1.3123, -0.2886, -0.8723, -1.6399, -0.1212, -1.4474,\n",
       "           -0.6285, -0.4068, -1.1430, -1.1430, -0.9467, -1.1843, -0.4681, -1.5417,\n",
       "           -0.8723, -0.9092, -1.2689,  0.0100, -0.1757, -0.7301, -0.4994, -1.3123,\n",
       "           -0.6958, -1.7949, -1.4015, -1.0233, -1.2689, -0.5956, -0.9847, -0.9467,\n",
       "           -0.6285, -0.8360, -1.1430, -1.0626,  0.6771, -0.1483,  0.1827,  0.1827,\n",
       "           -0.2315, -0.3767,  0.1587,  0.0606,  0.1587,  0.3662, -0.5631, -0.3176,\n",
       "            0.1587,  0.2298,  0.1827, -0.4681, -0.3767, -0.2886,  0.1827,  0.4953,\n",
       "            0.3216, -0.0417, -0.0944,  0.0606,  0.3882,  0.1101,  0.5574,  0.4743,\n",
       "           -0.2886,  0.7348, -0.0157,  0.1346,  0.4316,  0.5574,  0.5980, -0.0679,\n",
       "            0.3216, -0.4373,  0.1101, -0.2315,  0.2298, -0.0417, -0.4994, -0.4994,\n",
       "           -0.2315, -0.3176,  0.1101, -0.2035,  0.2531,  0.5369, -0.0417,  0.0855,\n",
       "            0.3440, -0.5956, -0.0944,  0.7912,  0.1101,  0.7348,  0.6379, -0.5311,\n",
       "           -0.1483, -0.3767,  0.2064,  0.0855,  0.7538,  0.1346, -0.0157, -0.5631,\n",
       "            0.3882,  0.0100,  0.6180, -0.4681,  0.3216,  0.7726,  0.5778,  0.3216,\n",
       "           -0.6285,  0.2989,  0.3882,  0.4100,  0.4953,  0.1346,  0.3882, -0.0157,\n",
       "            0.4530,  0.5162, -0.1757, -0.6285, -0.2315, -0.3767, -0.0944,  0.5574,\n",
       "           -0.0679,  0.1346, -0.3767, -0.2599, -0.2886, -0.0679,  0.1827,  0.0354,\n",
       "           -0.1757,  0.5369, -0.4068, -0.1483, -0.0157,  0.1101, -0.0679,  0.2531,\n",
       "            0.3216, -0.6285,  0.4100, -0.2886,  0.2531, -0.6958,  0.0855, -0.2886,\n",
       "           -0.2886, -0.4373,  0.2761, -0.3176,  0.6180, -0.0944, -0.1212,  0.2761,\n",
       "            0.3216,  0.1346, -0.1483,  0.2298, -0.0679, -0.2599,  0.5980,  0.0606,\n",
       "            0.2531, -0.3176,  0.1827,  0.2298,  0.3216,  0.1827,  0.4743,  0.0354,\n",
       "            0.3440,  0.1346, -0.1212,  0.0100,  0.5574, -0.2599,  0.0606,  0.0100,\n",
       "            0.5369,  0.2298,  0.4316, -0.4068, -0.3176, -0.3767, -0.3176,  0.5162,\n",
       "           -0.4068, -0.3176,  0.5162,  0.5574, -0.1757, -0.1757,  0.5369, -0.3470,\n",
       "            0.6576, -0.5631, -0.2886,  0.4530,  1.6326,  1.8831,  1.8106,  0.5162,\n",
       "            0.5980,  1.8350,  1.8350,  1.3237,  0.4316,  1.4406,  1.2011,  1.8592,\n",
       "            0.2531,  1.5521,  0.4743,  0.5574,  0.5574,  1.2323,  0.2531,  1.6060,\n",
       "            1.3535,  1.0047, -0.3176, -0.5631, -0.4373, -0.6285, -1.3123,  0.1101,\n",
       "            0.0606, -0.1483, -0.8360, -0.8360, -0.0944, -0.6285, -1.2262],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.6991, -0.6991, -0.5378, -0.5378, -0.6991, -0.1749, -0.6991, -0.6991,\n",
       "           -0.8603, -0.6991, -0.8603, -0.5378, -0.1749, -0.6991, -0.9064, -0.8603,\n",
       "           -0.6991, -0.5378, -0.5378, -0.8258, -0.1749, -0.5378, -0.3362, -0.3362,\n",
       "           -0.8603, -0.5378, -0.8603, -0.3362, -0.3362, -0.8603, -0.3362, -0.9064,\n",
       "           -0.5378, -0.6991, -0.6991, -0.8603, -0.1749, -0.6991, -0.6991, -0.6991,\n",
       "           -0.6991, -0.8258, -0.6991, -0.9064, -0.7855, -0.8603, -0.6991, -0.7855,\n",
       "           -0.9064, -0.3362, -0.6991, -0.6991, -0.1749, -0.8258, -0.8258, -0.8603,\n",
       "           -0.8603, -0.8603, -0.8258, -0.9064, -0.5378, -0.6991, -0.8603, -0.1749,\n",
       "           -0.6991, -0.1749, -0.5378, -0.1749, -0.7855, -0.5378, -0.6991, -0.3362,\n",
       "           -0.8603, -0.8258, -0.8603, -0.3362, -0.6991, -0.5378, -0.8603, -0.6991,\n",
       "           -0.6991, -0.8258, -0.1749, -0.5378, -0.1749, -0.9064, -0.6991, -0.6991,\n",
       "           -0.6991, -0.1749, -0.8258, -0.6991, -0.8258, -0.7855, -0.8603, -0.6991,\n",
       "           -0.7855, -0.6991, -0.1749, -0.6991, -0.8603, -0.9064, -0.6991, -0.8603,\n",
       "           -0.8603, -0.6991, -0.6991, -0.6991, -0.6991, -0.9064, -0.8258, -0.8603,\n",
       "           -0.8603, -0.6991, -0.8258, -0.5378, -0.6991, -0.9064, -0.8603, -0.8603,\n",
       "           -0.8603, -0.8603, -0.5378, -0.6991, -0.6991, -0.3362, -0.5378, -0.1749,\n",
       "           -0.8603, -0.9064, -0.3362, -0.1749, -0.5378, -0.8603, -0.5378, -0.8603,\n",
       "           -0.3362, -0.7855, -0.8603, -0.8258, -0.8258, -0.6991, -0.6991, -0.8258,\n",
       "           -0.6991, -0.6991, -0.7855, -0.3362, -0.3362, -0.6991, -0.7855, -0.3362,\n",
       "           -0.1749, -0.8603, -0.1749, -0.7855, -0.3362, -0.8603, -0.6991, -0.6991,\n",
       "           -0.8603, -0.6991, -0.5378, -0.1749, -0.7855, -0.6991, -0.6991, -0.6991,\n",
       "            0.8735,  2.4460,  2.4460,  0.8735,  0.8735,  2.4460,  2.4460,  0.8735,\n",
       "            0.8735,  0.8735,  2.4460,  2.4460,  0.8735,  2.4460,  0.8735,  2.4460,\n",
       "            0.8735,  0.8735,  2.4460,  0.8735,  2.4460,  0.8735,  2.4460,  0.8735,\n",
       "            2.4460,  2.4460,  2.4460,  0.8735,  2.4460,  0.8735,  0.8735,  2.4460,\n",
       "            0.8735, -0.7855, -0.8603, -0.7855,  0.3493, -0.7855,  0.3493, -0.5378,\n",
       "           -0.8603, -0.8603,  0.3493, -0.6991,  0.3493, -0.8258,  0.3493, -0.6991,\n",
       "           -0.6991, -0.1749, -0.5378, -0.8258,  0.3493, -0.5378, -0.3362, -0.3362,\n",
       "           -0.8258,  0.3493, -0.8603, -0.1749, -0.6991, -0.5378, -0.6991,  0.3493,\n",
       "           -0.1749, -0.8603, -0.8603, -0.9064, -0.1749,  0.3493, -0.3362, -0.9064,\n",
       "           -0.1749, -0.1749, -0.1749, -0.3362,  0.3493, -0.3362, -0.8603,  0.3493,\n",
       "           -0.8603, -0.7855, -0.1749,  0.3493, -0.8603, -0.8603,  0.3493, -0.1749,\n",
       "           -0.6991, -0.9064, -0.5378, -0.5378, -0.6991, -0.8603, -0.8258, -0.1749,\n",
       "           -0.1749, -0.1749, -0.1749, -0.3362, -0.1749, -0.3362, -0.5378, -0.1749,\n",
       "           -0.3362, -0.9064, -0.8258, -0.3362, -0.3362, -0.9064, -0.3362, -0.5378,\n",
       "           -0.6991, -0.3362, -0.1749, -0.8603, -0.5378, -0.8603, -0.9064, -0.8603,\n",
       "            0.3493, -0.5378, -0.8603, -0.8603, -0.6991,  0.3493, -0.1749, -0.3362,\n",
       "           -0.6991, -0.6991, -0.8258, -0.3362,  0.3493, -0.5378, -0.6991, -0.9064,\n",
       "           -0.3362, -0.5378, -0.8258, -0.5378, -0.8603, -0.5378, -0.1749, -0.1749,\n",
       "           -0.7855, -0.1749, -0.8603, -0.8258, -0.8258, -0.1749, -0.1749, -0.8603,\n",
       "           -0.5378, -0.9064, -0.6991, -0.6991, -0.8258, -0.7855, -0.1749,  0.3493,\n",
       "           -0.5378,  0.3493,  0.3493, -0.5378, -0.9064, -0.6991,  0.3493,  0.3493,\n",
       "           -0.8258, -0.7855, -0.1749, -0.7855, -0.5378, -0.8603, -0.3362, -0.3362,\n",
       "           -0.8603, -0.5378,  0.3493,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,\n",
       "            2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,\n",
       "            2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,\n",
       "            2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,\n",
       "            2.4460,  2.4460,  2.4460,  2.4460,  0.3493,  0.3493,  0.8735,  0.3493,\n",
       "            0.3493,  0.3493,  0.3493,  0.8735,  0.3493,  0.3493,  0.8735,  0.8735,\n",
       "            0.8735,  0.8735, -0.1749,  0.8735, -0.1749,  0.8735,  0.3493,  0.3493,\n",
       "            0.3493,  0.3493,  0.8735,  0.3493,  0.3493,  0.3493,  0.3493,  0.8735,\n",
       "            0.3493,  0.3493,  0.8735,  0.3493,  0.3493, -0.1749,  0.3493,  0.8735,\n",
       "            0.3493,  0.8735,  0.3493,  0.3493,  0.8735,  0.3493, -0.1749,  0.3493,\n",
       "            0.3493,  0.3493,  0.8735, -0.1749,  0.3493,  0.3493,  0.8735,  0.3493,\n",
       "            0.3493, -0.1749, -0.1749,  0.3493,  0.8735,  0.3493,  0.3493,  0.8735,\n",
       "            0.8735,  0.8735,  0.8735,  0.3493, -0.1749,  0.3493,  0.8735, -0.1749,\n",
       "            0.3493,  0.3493,  0.8735,  0.3493,  0.3493,  0.8735, -0.1749, -0.1749,\n",
       "            0.8735, -0.1749, -0.1749,  0.3493], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.2958, -0.8200, -0.9064,  0.0670,  0.0670, -0.0942, -0.8200, -0.4571,\n",
       "           -0.4571, -0.7855, -0.7451, -0.2958, -0.2958, -0.2958, -0.8661, -0.6587,\n",
       "           -0.2958, -0.7855, -0.2958, -0.6587, -0.9064, -0.4571, -0.9064, -0.7451,\n",
       "           -0.7855, -0.2958,  0.0670, -0.8661,  0.0670, -0.6587, -0.9064, -0.6587,\n",
       "           -0.7451,  0.0670, -0.4571, -0.8200, -0.8200, -0.6587, -0.9064, -0.8200,\n",
       "           -0.4571, -0.7451, -0.6587, -0.0942, -0.6587, -0.9064, -0.0942, -0.4571,\n",
       "           -0.7451, -0.8200, -0.4571, -0.0942, -0.9064, -0.2958, -0.6587, -0.9064,\n",
       "           -0.4571,  0.0670, -0.6587, -0.6587, -0.0942, -0.2958, -0.4571, -0.7855,\n",
       "           -0.4571, -0.6587, -0.8200, -0.0942, -0.8200, -0.6587, -0.9064, -0.6587,\n",
       "           -0.8200, -0.4571, -0.7855, -0.8200, -0.8200, -0.7451, -0.2958, -0.6587,\n",
       "           -0.4571, -0.8200, -0.8200, -0.9064, -0.6587, -0.2958, -0.8200, -0.4571,\n",
       "           -0.7451, -0.0942, -0.0942,  0.0670, -0.8200, -0.7451, -0.8661, -0.2958,\n",
       "            0.0670, -0.4571, -0.8200, -0.2958, -0.7855, -0.9064, -0.0942, -0.8200,\n",
       "            0.0670, -0.2958, -0.0942, -0.0942, -0.7451, -0.4571, -0.7451,  0.0670,\n",
       "           -0.4571, -0.8200, -0.9064,  0.0670, -0.9064, -0.8661, -0.6587, -0.9064,\n",
       "           -0.4571, -0.7451, -0.8661, -0.2958, -0.6587, -0.7855,  0.0670, -0.4571,\n",
       "           -0.8661, -0.9064, -0.6587, -0.8200, -0.9064, -0.8200, -0.9064, -0.4571,\n",
       "           -0.6587, -0.7451, -0.6587, -0.2958, -0.2958, -0.2958, -0.7855, -0.9064,\n",
       "           -0.8661,  0.0670, -0.8661,  0.0670, -0.7451, -0.0942, -0.7855, -0.9064,\n",
       "           -0.9064, -0.6587, -0.7855, -0.7855,  0.0670, -0.6587, -0.4571, -0.6587,\n",
       "            0.0670,  0.0670, -0.7451, -0.6587, -0.2958, -0.2958, -0.7855, -0.7451,\n",
       "           -0.7451, -0.7451, -0.9064,  0.0670, -0.6587, -0.9064, -0.8200, -0.9064,\n",
       "            1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  0.5912,\n",
       "            1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,\n",
       "            0.5912,  0.5912,  1.1154,  1.1154,  1.1154,  0.5912,  1.1154,  1.1154,\n",
       "            0.5912,  0.5912,  1.1154,  0.5912,  0.5912,  1.1154,  1.1154,  0.5912,\n",
       "            1.1154,  1.1154,  1.1154,  0.5912,  0.5912,  0.5912,  1.1154,  1.1154,\n",
       "            1.1154,  1.1154,  1.1154,  1.1154,  0.5912,  1.1154,  1.1154,  1.1154,\n",
       "            1.1154,  1.1154,  0.5912,  0.5912,  0.5912,  0.5912,  0.5912,  1.1154,\n",
       "            0.5912,  1.1154,  1.1154,  1.1154,  1.1154,  0.5912,  0.5912,  1.1154,\n",
       "            1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,\n",
       "            1.1154,  0.5912,  1.1154,  0.5912,  1.1154,  0.5912,  1.1154,  0.5912,\n",
       "            0.5912,  0.0670, -0.4571, -0.8200, -0.6587, -0.8200, -0.8200, -0.4571,\n",
       "           -0.6587, -0.4571, -0.0942, -0.4571, -0.8200, -0.8200, -0.8200, -0.6587,\n",
       "            0.0670,  0.0670, -0.6587, -0.8200, -0.8200, -0.6587, -0.6587, -0.8200,\n",
       "           -0.6587,  0.0670, -0.4571, -0.8200, -0.0942, -0.2958, -0.4571, -0.8200,\n",
       "            0.0670, -0.6587, -0.2958, -0.2958,  0.0670, -0.8200, -0.4571, -0.0942,\n",
       "           -0.4571, -0.6587, -0.0942,  0.0670, -0.4571,  0.0670, -0.2958, -0.8200,\n",
       "            0.0670, -0.4571, -0.4571, -0.8200, -0.4571, -0.6587, -0.4571,  0.0670,\n",
       "           -0.0942, -0.0942,  0.0670, -0.4571, -0.2958, -0.0942, -0.8200,  0.0670,\n",
       "           -0.8200,  0.0670, -0.4571, -0.8200,  0.0670, -0.0942, -0.4571, -0.0942,\n",
       "            0.0670, -0.0942, -0.6587, -0.8200, -0.2958, -0.6587, -0.0942, -0.6587,\n",
       "           -0.0942, -0.8200, -0.6587,  0.0670, -0.0942, -0.0942, -0.0942, -0.8200,\n",
       "           -0.4571, -0.2958, -0.2958, -0.0942, -0.4571, -0.0942, -0.6587,  0.0670,\n",
       "           -0.4571, -0.2958, -0.4571, -0.0942, -0.6587, -0.4571, -0.4571, -0.0942,\n",
       "           -0.8200, -0.8200, -0.0942,  0.0670, -0.4571, -0.8200, -0.4571,  0.0670,\n",
       "            0.0670, -0.9064, -0.6587,  0.0670, -0.8200, -0.0942, -0.4571,  0.0670,\n",
       "           -0.6587, -0.0942,  0.0670, -0.0942, -0.0942, -0.0942,  0.0670, -0.8200,\n",
       "            0.0670, -0.6587, -0.0942, -0.0942, -0.0942, -0.4571, -0.6587, -0.0942,\n",
       "            0.0670, -0.6587, -0.6587, -0.2958, -0.6587, -0.2958, -0.8200, -0.0942,\n",
       "            0.0670, -0.8200,  0.0670,  0.0670,  0.0670,  0.0670, -0.4571, -0.0942,\n",
       "           -0.0942, -0.0942,  0.0670, -0.6587, -0.4571, -0.0942, -0.2958, -0.8200,\n",
       "           -0.6587,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,\n",
       "            3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,\n",
       "            3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,\n",
       "            3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,\n",
       "            3.2121,  3.2121,  3.2121, -0.8200,  0.0670,  0.5912, -0.0942, -0.8200,\n",
       "           -0.6587, -0.0942, -0.6587,  0.0670, -0.2958,  1.1154, -0.6587,  0.0670,\n",
       "            1.1154, -0.6587, -0.2958,  1.1154,  1.1154, -0.0942, -0.0942,  0.5912,\n",
       "            0.5912, -0.2958,  1.1154,  0.0670,  0.0670,  1.1154, -0.2958,  0.0670,\n",
       "           -0.2958, -0.6587, -0.6587,  0.5912,  0.5912, -0.2958,  0.5912, -0.0942,\n",
       "            0.5912,  0.0670, -0.2958, -0.2958, -0.0942,  0.0670, -0.8200,  0.5912,\n",
       "           -0.4571,  0.5912, -0.2958, -0.2958,  0.0670,  0.0670,  0.0670,  0.5912,\n",
       "           -0.4571, -0.4571,  0.0670,  0.0670, -0.2958,  0.0670, -0.6587,  0.5912,\n",
       "            1.1154, -0.0942,  0.5912,  1.1154,  1.1154, -0.2958, -0.0942,  0.0670,\n",
       "           -0.0942,  0.0670, -0.8200, -0.0942, -0.0942,  1.1154,  1.1154,  1.1154,\n",
       "            0.5912, -0.2958,  1.1154, -0.8200,  0.0670,  0.0670, -0.6587,  1.1154,\n",
       "            0.0670, -0.0942,  0.0670,  0.5912,  0.5912,  1.1154, -0.6587, -0.6587,\n",
       "           -0.8200, -0.8200,  1.1154,  1.1154, -0.4571, -0.4571, -0.0942, -0.4571,\n",
       "            0.5912], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.5781, -0.8661, -0.9064, -0.2152, -0.2152, -0.5781, -0.8258, -0.5781,\n",
       "           -0.5781, -0.8258, -0.7394, -0.5781, -0.5781, -0.5781, -0.9064, -0.7394,\n",
       "           -0.5781, -0.8258, -0.5781, -0.7394, -0.9064, -0.7394, -0.9064, -0.7394,\n",
       "           -0.8258, -0.5781, -0.2152, -0.9064, -0.2152, -0.7394, -0.9064, -0.7394,\n",
       "           -0.7394, -0.2152, -0.5781, -0.8258, -0.8661, -0.7394, -0.9064, -0.8661,\n",
       "           -0.7394, -0.7394, -0.7394, -0.5781, -0.7394, -0.9064, -0.5781, -0.5781,\n",
       "           -0.7394, -0.8258, -0.5781, -0.5781, -0.9064, -0.5781, -0.7394, -0.9064,\n",
       "           -0.7394, -0.2152, -0.7394, -0.7394, -0.5781, -0.5781, -0.5781, -0.7855,\n",
       "           -0.5781, -0.7394, -0.8258, -0.5781, -0.8258, -0.7394, -0.9064, -0.7394,\n",
       "           -0.8661, -0.7394, -0.7855, -0.8661, -0.8661, -0.7394, -0.5781, -0.7394,\n",
       "           -0.5781, -0.8661, -0.8661, -0.9064, -0.7394, -0.5781, -0.8258, -0.7394,\n",
       "           -0.7394, -0.5781, -0.5781, -0.2152, -0.8661, -0.7855, -0.8661, -0.5781,\n",
       "           -0.2152, -0.5781, -0.8661, -0.5781, -0.7855, -0.9064, -0.5781, -0.8661,\n",
       "           -0.2152, -0.5781, -0.2152, -0.5781, -0.7394, -0.7394, -0.7855, -0.2152,\n",
       "           -0.5781, -0.8661, -0.9064, -0.2152, -0.9064, -0.9064, -0.7394, -0.9064,\n",
       "           -0.7394, -0.7394, -0.8661, -0.5781, -0.7394, -0.8258, -0.2152, -0.5781,\n",
       "           -0.8661, -0.9064, -0.7394, -0.8258, -0.9064, -0.8661, -0.9064, -0.7394,\n",
       "           -0.7394, -0.7855, -0.7394, -0.5781, -0.5781, -0.5781, -0.7855, -0.9064,\n",
       "           -0.8661, -0.2152, -0.8661, -0.2152, -0.7855, -0.5781, -0.8258, -0.9064,\n",
       "           -0.9064, -0.7394, -0.8258, -0.7855, -0.2152, -0.7394, -0.5781, -0.7394,\n",
       "           -0.2152, -0.2152, -0.7394, -0.7394, -0.5781, -0.5781, -0.7855, -0.7394,\n",
       "           -0.7394, -0.7855, -0.9064, -0.2152, -0.7394, -0.9064, -0.8661, -0.9064,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669, -0.2152, -0.2152,  0.4703,  0.4703,  0.3090,  0.4703, -0.5781,\n",
       "           -0.5781, -0.7394, -0.2152,  0.4703, -0.7394, -0.7394, -0.7394, -0.5781,\n",
       "           -0.2152,  0.3090,  0.4703,  0.4703,  0.4703, -0.2152, -0.5781, -0.7394,\n",
       "           -0.7394,  0.3090, -0.7394,  0.3090,  0.4703, -0.7394,  0.4703,  0.3090,\n",
       "            0.3090,  0.3090, -0.2152,  0.4703,  0.4703, -0.2152,  0.3090,  0.4703,\n",
       "           -0.7394, -0.2152, -0.2152, -0.2152,  0.4703, -0.5781,  0.4703,  0.3090,\n",
       "            0.3090, -0.5781,  0.3090,  0.3090, -0.7394, -0.2152,  0.4703,  0.4703,\n",
       "           -0.7394, -0.7394,  0.4703, -0.7394, -0.2152, -0.2152, -0.7394,  0.4703,\n",
       "           -0.5781, -0.7394, -0.2152, -0.2152,  0.4703, -0.7394,  0.3090, -0.5781,\n",
       "            0.4703, -0.5781,  0.3090, -0.7394,  0.4703, -0.7394, -0.7394, -0.2152,\n",
       "            0.4703,  0.4703, -0.2152,  0.4703,  0.3090, -0.7394, -0.2152,  0.3090,\n",
       "           -0.2152, -0.5781,  0.3090,  0.4703, -0.5781,  0.3090, -0.2152,  0.4703,\n",
       "            0.3090, -0.7394,  0.4703,  0.3090,  0.4703,  0.3090,  0.3090, -0.7394,\n",
       "           -0.7394, -0.7394,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  0.3090,  0.4703,  0.4703, -0.2152,\n",
       "            0.3090,  0.3090,  0.3090,  0.4703,  0.4703,  0.4703,  0.4703,  0.3090,\n",
       "            0.4703,  0.4703,  0.4703, -0.2152,  0.4703,  0.3090,  0.3090,  0.3090,\n",
       "            0.4703, -0.2152,  0.3090,  0.4703,  0.3090,  0.4703,  0.3090,  0.3090,\n",
       "            0.3090, -0.2152,  0.3090,  0.4703,  0.4703, -0.2152,  0.3090,  0.4703,\n",
       "            0.3090, -0.2152,  0.3090,  0.3090,  0.4703,  0.3090, -0.2152, -0.2152,\n",
       "            0.4703,  0.3090,  0.4703,  0.4703,  0.3090,  0.3090,  0.4703,  0.3090,\n",
       "            0.4703, -0.2152, -0.2152,  0.3090,  0.4703,  0.3090,  0.4703,  0.4703,\n",
       "            0.4703,  0.3090, -0.2152,  0.3090, -0.2152,  0.3090,  0.4703,  0.4703,\n",
       "            0.4703, -0.2152,  0.3090,  0.3090, -0.2152,  0.3090, -0.2152, -0.2152,\n",
       "            0.4703,  0.3090, -0.2152,  0.4703,  0.3090, -0.2152,  0.4703,  0.3090,\n",
       "           -0.2152,  0.3090,  0.4703, -0.2152,  0.4703,  0.3090, -0.2152, -0.2152],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([ 0.8965, -0.1518, -0.1518, -0.8776, -0.8776,  0.8965, -0.3131, -0.1518,\n",
       "           -0.3131,  0.8965, -0.6760, -0.6760, -0.1518, -0.5147, -0.3131,  0.8965,\n",
       "            0.8965,  0.3723, -0.1518, -0.3131, -0.8776,  0.3723, -0.6760,  0.8965,\n",
       "           -0.3131,  0.8965, -0.6760,  0.8965, -0.6760, -0.3131,  0.3723, -0.8776,\n",
       "           -0.1518, -0.3131,  0.3723, -0.6760, -0.8776,  0.8965,  0.3723,  0.8965,\n",
       "            0.3723, -0.8776, -0.6760,  0.8965, -0.3131, -0.1518,  0.8965, -0.8776,\n",
       "            0.8965, -0.1518, -0.5147,  0.3723,  0.3723, -0.1518, -0.8776, -0.1518,\n",
       "            0.3723, -0.1518, -0.6760, -0.1518,  0.8965, -0.8776, -0.3131, -0.3131,\n",
       "           -0.1518, -0.8776, -0.8776, -0.1518,  0.3723,  0.3723, -0.3131, -0.3131,\n",
       "           -0.8776,  0.8965,  0.8965, -0.8776,  0.3723, -0.8776, -0.6760, -0.6760,\n",
       "            0.8965, -0.5147, -0.8776, -0.1518, -0.3131,  0.8965, -0.3131, -0.6760,\n",
       "           -0.5147,  0.8965, -0.1518, -0.3131,  0.3723,  0.8965, -0.3131, -0.6760,\n",
       "            0.8965, -0.6760, -0.8776,  0.8965, -0.1518, -0.3131,  0.3723,  0.8965,\n",
       "           -0.1518, -0.3131,  0.8965, -0.6760, -0.6760,  0.3723, -0.1518,  0.8965,\n",
       "            0.8965, -0.6760, -0.3131,  0.3723,  0.3723,  0.3723, -0.1518, -0.6760,\n",
       "           -0.3131, -0.1518, -0.6760,  0.8965, -0.6760,  0.8965,  0.8965, -0.8776,\n",
       "            0.3723, -0.3131,  0.8965, -0.1518,  0.8965, -0.6760,  0.8965,  0.3723,\n",
       "           -0.3131, -0.3131, -0.1518, -0.5147,  0.3723, -0.3131,  0.3723, -0.3131,\n",
       "            0.3723, -0.1518,  0.8965, -0.1518,  0.8965, -0.1518, -0.3131, -0.1518,\n",
       "           -0.1518,  0.8965, -0.6760, -0.1518, -0.8776,  0.3723,  0.3723,  0.8965,\n",
       "           -0.8776,  0.8965, -0.3131,  0.8965, -0.3131, -0.3131, -0.5147, -0.5147,\n",
       "           -0.8776,  0.8965,  0.8965, -0.3131, -0.1518,  0.8965,  0.3723, -0.1518,\n",
       "            0.3723, -0.3131,  0.8965,  0.8965,  0.3723, -0.1518, -0.6760, -0.1518,\n",
       "           -0.8776, -0.1518, -0.6760,  0.8965, -0.8776, -0.8776,  0.8965, -0.8776,\n",
       "           -0.8776, -0.5147, -0.8776,  0.8965, -0.3131,  0.8965, -0.1518,  0.8965,\n",
       "           -0.1518, -0.5147,  0.8965, -0.1518, -0.8776,  0.3723, -0.5147, -0.8776,\n",
       "           -0.3131, -0.1518,  0.8965, -0.5147,  0.8965, -0.8776,  0.8965,  0.3723,\n",
       "           -0.3131, -0.1518,  0.3723, -0.1518,  0.3723, -0.5147,  0.3723, -0.8776,\n",
       "           -0.1518, -0.1518,  0.8965,  0.8965, -0.1518, -0.5147, -0.3131,  0.3723,\n",
       "           -0.6760, -0.3131,  0.8965, -0.1518, -0.1518,  0.8965, -0.3131,  0.8965,\n",
       "           -0.1518,  0.8965,  0.8965, -0.1518,  0.8965, -0.7624, -0.5147,  0.8965,\n",
       "            0.8965,  0.3723,  0.3723,  0.8965,  0.3723,  0.3723,  0.3723,  0.3723,\n",
       "           -0.8776, -0.6760, -0.6760,  0.8965, -0.6760, -0.1518,  0.3723,  0.8965,\n",
       "           -0.1518, -0.1518, -0.8776, -0.3131, -0.6760, -0.1518, -0.1518,  0.3723,\n",
       "           -0.8776, -0.5147,  0.3723,  0.8965,  0.8965, -0.5147,  0.8965, -0.6760,\n",
       "            0.8965, -0.1518, -0.1518, -0.5147, -0.3131,  0.3723, -0.6760, -0.6760,\n",
       "           -0.6760,  0.3723,  0.3723, -0.5147, -0.8776, -0.8776, -0.9237, -0.8431,\n",
       "           -0.8431, -0.7624, -0.8431, -0.8776,  0.3723, -0.3131, -0.9237, -0.8085,\n",
       "           -0.7624, -0.6760, -0.3131, -0.9237, -0.6760, -0.6760, -0.1518, -0.5147,\n",
       "           -0.8431, -0.6760, -0.8776, -0.7624, -0.3131, -0.3131, -0.6760, -0.8431,\n",
       "           -0.8431, -0.8776,  0.3723, -0.6760, -0.8776, -0.6760, -0.8776, -0.3131,\n",
       "           -0.6760, -0.5147, -0.8776, -0.5147,  0.3723, -0.8776, -0.6760, -0.8085,\n",
       "           -0.3131, -0.6760,  0.3723, -0.8431, -0.7624, -0.3131, -0.5147, -0.8431,\n",
       "           -0.8776, -0.6760, -0.5147, -0.8776, -0.9237, -0.8776, -0.8776, -0.8776,\n",
       "           -0.7624, -0.5147, -0.5147, -0.6760, -0.6760, -0.9237, -0.6760, -0.7624,\n",
       "           -0.6760, -0.6760, -0.8776, -0.8776, -0.8085, -0.6760, -0.8776, -0.8776,\n",
       "           -0.8431, -0.8431, -0.9237, -0.9237, -0.8776, -0.8085, -0.6760, -0.7624,\n",
       "           -0.3131, -0.9237, -0.8431, -0.8085, -0.8776, -0.8431, -0.5147, -0.9237,\n",
       "           -0.8085, -0.8431, -0.5147, -0.8085, -0.9237,  0.3723, -0.8776, -0.3131,\n",
       "           -0.5147, -0.1518, -0.9237, -0.5147, -0.7624, -0.6760, -0.8776, -0.3131,\n",
       "           -0.3131, -0.8431, -0.5147, -0.8431, -0.8431, -0.8776, -0.6760, -0.8776,\n",
       "           -0.9237, -0.8431, -0.7624, -0.7624, -0.6760, -0.8085, -0.3131, -0.1518,\n",
       "           -0.7624, -0.8776, -0.3131, -0.6760, -0.5147, -0.1518, -0.3131, -0.8776,\n",
       "           -0.5147, -0.8431, -0.1518, -0.5147, -0.1518, -0.6760, -0.6760, -0.8085,\n",
       "           -0.1518, -0.1518, -0.8776,  0.3723, -0.3131, -0.6760, -0.9237, -0.9237,\n",
       "           -0.3131, -0.8085, -0.9237, -0.6760, -0.1518, -0.8776, -0.5147, -0.5147,\n",
       "           -0.6760, -0.8431, -0.6760, -0.3131, -0.7624, -0.1518, -0.5147, -0.5147,\n",
       "           -0.5147, -0.8431, -0.8776, -0.5147,  2.9932,  2.9932,  2.9932,  2.9932,\n",
       "            2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,\n",
       "            2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,\n",
       "            2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,\n",
       "            2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.2695, 0.2695, 0.3415, 0.3091, 0.3302, 0.2849, 0.2695, 0.3302, 0.2561,\n",
       "           0.2912, 0.3295, 0.3461, 0.2658, 0.3302, 0.3133, 0.3065, 0.3302, 0.2950,\n",
       "           0.3461, 0.2847, 0.3088, 0.2847, 0.2711, 0.2940, 0.2561, 0.2887, 0.2561,\n",
       "           0.2683, 0.2662, 0.3295, 0.2817, 0.3133, 0.3271, 0.2548, 0.3302, 0.3295,\n",
       "           0.2849, 0.3302, 0.3302, 0.2695, 0.3302, 0.2702, 0.3302, 0.3716, 0.2707,\n",
       "           0.3295, 0.3302, 0.2604, 0.3133, 0.3104, 0.2695, 0.3302, 0.2631, 0.2702,\n",
       "           0.2654, 0.3295, 0.2561, 0.3138, 0.2702, 0.3153, 0.3128, 0.2695, 0.2561,\n",
       "           0.2665, 0.3024, 0.2849, 0.3461, 0.2849, 0.2661, 0.3066, 0.2564, 0.2844,\n",
       "           0.2561, 0.2702, 0.3295, 0.2745, 0.2695, 0.3000, 0.2636, 0.3302, 0.2585,\n",
       "           0.3356, 0.2849, 0.3216, 0.2785, 0.3190, 0.3302, 0.2695, 0.3302, 0.2676,\n",
       "           0.2737, 0.2695, 0.3356, 0.2661, 0.3295, 0.3302, 0.2601, 0.2695, 0.2717,\n",
       "           0.2695, 0.2859, 0.3133, 0.2695, 0.2561, 0.3295, 0.3302, 0.2695, 0.2695,\n",
       "           0.3302, 0.3133, 0.3047, 0.2561, 0.2561, 0.2695, 0.3356, 0.3413, 0.2695,\n",
       "           0.3133, 0.2561, 0.3295, 0.2616, 0.3295, 0.2888, 0.2695, 0.3302, 0.3104,\n",
       "           0.3096, 0.2740, 0.2607, 0.3133, 0.2666, 0.2775, 0.2929, 0.3295, 0.3461,\n",
       "           0.2561, 0.2710, 0.2661, 0.2561, 0.3356, 0.2702, 0.2581, 0.3302, 0.2702,\n",
       "           0.3302, 0.3302, 0.3215, 0.2669, 0.2877, 0.2759, 0.2661, 0.2854, 0.2985,\n",
       "           0.3295, 0.2631, 0.2661, 0.2770, 0.2561, 0.2609, 0.3302, 0.2628, 0.2695,\n",
       "           0.3461, 0.2716, 0.2871, 0.2804, 0.3116, 0.3302, 0.2894, 0.4638, 0.3104,\n",
       "           0.3651, 0.3439, 0.3153, 0.3111, 0.3495, 0.2803, 0.2844, 0.3451, 0.3081,\n",
       "           0.3100, 0.3491, 0.2940, 0.3186, 0.2867, 0.2829, 0.3227, 0.4616, 0.3150,\n",
       "           0.4355, 0.4535, 0.3170, 0.3788, 0.3126, 0.4782, 0.3439, 0.3120, 0.2870,\n",
       "           0.2817, 0.3202, 0.2878, 0.3081, 0.3473, 0.2819, 0.2757, 0.3187, 0.2901,\n",
       "           0.4840, 0.2996, 0.3473, 0.3051, 0.3044, 0.4344, 0.3507, 0.4801, 0.3369,\n",
       "           0.3235, 0.2824, 0.3695, 0.3381, 0.3175, 0.3695, 0.3459, 0.3335, 0.3648,\n",
       "           0.2771, 0.3258, 0.3508, 0.3295, 0.3267, 0.2716, 0.2983, 0.3472, 0.3258,\n",
       "           0.3258, 0.3338, 0.3280, 0.2834, 0.2778, 0.3716, 0.3472, 0.3235, 0.3472,\n",
       "           0.3459, 0.2899, 0.3459, 0.3473, 0.3347, 0.3473, 0.2607, 0.3822, 0.3535,\n",
       "           0.3258, 0.3258, 0.3469, 0.3472, 0.3114, 0.3147, 0.3695, 0.3695, 0.2622,\n",
       "           0.3473, 0.3632, 0.3422, 0.3472, 0.2690, 0.2665, 0.3398, 0.3472, 0.6848,\n",
       "           0.3001, 0.3320, 0.2909, 0.3227, 0.2803, 0.3700, 0.4059, 0.4187, 0.3459,\n",
       "           0.3469, 0.3103, 0.3459, 0.2912, 0.3258, 0.3273, 0.3258, 0.4187, 0.3473,\n",
       "           0.2778, 0.3379, 0.3473, 0.3473, 0.2693, 0.6979, 0.3822, 0.3242, 0.3397,\n",
       "           0.3146, 0.3507, 0.2703, 0.3390, 0.2887, 0.3510, 0.4187, 0.2792, 0.5274,\n",
       "           0.2886, 0.2902, 0.2826, 0.3419, 0.2725, 0.2686, 0.2735, 0.3472, 0.3473,\n",
       "           0.3507, 0.3381, 0.3822, 0.2809, 0.2643, 0.3695, 0.3716, 0.2985, 0.3510,\n",
       "           0.2723, 0.3187, 0.3822, 0.2952, 0.2957, 0.3109, 0.2810, 0.5483, 0.4187,\n",
       "           0.3369, 0.3277, 0.2782, 0.3401, 0.2721, 0.3472, 0.3002, 0.3695, 0.3209,\n",
       "           0.3459, 0.2989, 0.3473, 0.5483, 0.3469, 0.3216, 0.3192, 0.3167, 0.3136,\n",
       "           0.3194, 0.3264, 0.3043, 0.3047, 0.3011, 0.3043, 0.2967, 0.3194, 0.2968,\n",
       "           0.2973, 0.3001, 0.3310, 0.3061, 0.2967, 0.3011, 0.3056, 0.3047, 0.3182,\n",
       "           0.3194, 0.3037, 0.2964, 0.3076, 0.3194, 0.2982, 0.3043, 0.2974, 0.3292,\n",
       "           0.3020, 0.3053, 0.3042, 0.3089, 0.2767, 0.3089, 0.2854, 0.2671, 0.2943,\n",
       "           0.2808, 0.2738, 0.3089, 0.2829, 0.2853, 0.2823, 0.2788, 0.3124, 0.2811,\n",
       "           0.2849, 0.2871, 0.3116, 0.3116, 0.3089, 0.2682, 0.2819, 0.3089, 0.3089,\n",
       "           0.3089, 0.3116, 0.2819, 0.3089, 0.3116, 0.2767, 0.3116, 0.2879, 0.2849,\n",
       "           0.3089, 0.2858, 0.2794, 0.2777, 0.2785, 0.2685, 0.2765, 0.2760, 0.3124,\n",
       "           0.3089, 0.3116, 0.3089, 0.2829, 0.2849, 0.3116, 0.3116, 0.2792, 0.3116,\n",
       "           0.3096, 0.3069, 0.3124, 0.3089, 0.2824, 0.2717, 0.2672, 0.2872, 0.2779,\n",
       "           0.3000, 0.2763, 0.3089, 0.3124, 0.3089, 0.2814, 0.3229, 0.3089, 0.3116,\n",
       "           0.2791, 0.2818, 0.3116, 0.2978, 0.2849, 0.2849, 0.2999, 0.2849, 0.2849,\n",
       "           0.3089]),\n",
       "   tensor([0.2906, 0.3365, 0.5096, 0.2726, 0.2813, 0.2881, 0.4041, 0.2970, 0.3017,\n",
       "           0.3056, 0.3919, 0.3156, 0.2899, 0.3284, 0.4003, 0.2914, 0.3032, 0.3120,\n",
       "           0.2953, 0.2862, 0.4731, 0.2803, 0.4101, 0.3146, 0.3002, 0.2974, 0.2711,\n",
       "           0.3724, 0.2737, 0.3042, 0.4484, 0.3578, 0.3025, 0.2807, 0.3123, 0.4041,\n",
       "           0.3174, 0.3151, 0.4731, 0.3529, 0.2829, 0.2985, 0.3123, 0.2841, 0.3578,\n",
       "           0.3676, 0.2858, 0.2881, 0.2924, 0.5174, 0.2851, 0.2802, 0.4101, 0.3086,\n",
       "           0.3042, 0.4101, 0.2949, 0.2717, 0.3578, 0.3110, 0.2854, 0.3037, 0.3211,\n",
       "           0.3597, 0.2819, 0.2819, 0.4041, 0.2829, 0.5174, 0.2948, 0.4731, 0.2908,\n",
       "           0.3375, 0.2759, 0.4201, 0.4041, 0.6361, 0.2893, 0.3017, 0.3151, 0.3017,\n",
       "           0.3124, 0.3546, 0.3681, 0.2894, 0.3011, 0.4041, 0.2805, 0.3755, 0.2891,\n",
       "           0.2829, 0.2718, 0.3510, 0.2962, 0.3968, 0.3091, 0.2810, 0.2859, 0.4136,\n",
       "           0.3163, 0.3556, 0.4101, 0.2838, 0.4041, 0.2839, 0.2957, 0.3016, 0.2990,\n",
       "           0.2940, 0.2829, 0.3308, 0.2726, 0.2756, 0.3271, 0.3696, 0.2801, 0.6375,\n",
       "           0.3452, 0.2908, 0.3983, 0.3010, 0.3096, 0.4799, 0.3030, 0.3020, 0.3011,\n",
       "           0.2828, 0.3276, 0.3968, 0.4731, 0.3151, 0.4041, 0.4731, 0.4456, 0.4101,\n",
       "           0.2811, 0.2869, 0.3084, 0.3578, 0.3271, 0.2935, 0.3068, 0.3597, 0.4101,\n",
       "           0.4562, 0.2751, 0.4799, 0.2696, 0.4300, 0.2833, 0.2944, 0.4101, 0.4101,\n",
       "           0.3013, 0.3262, 0.3597, 0.2774, 0.3007, 0.3017, 0.3151, 0.2728, 0.2706,\n",
       "           0.2875, 0.3255, 0.3013, 0.3018, 0.3597, 0.2893, 0.2893, 0.3695, 0.4101,\n",
       "           0.2791, 0.3151, 0.4484, 0.3801, 0.4731, 0.2950, 0.2950, 0.2879, 0.2686,\n",
       "           0.2922, 0.2675, 0.2950, 0.2749, 0.2950, 0.2827, 0.2675, 0.2659, 0.2781,\n",
       "           0.2950, 0.2811, 0.2675, 0.2753, 0.2768, 0.2864, 0.2832, 0.2675, 0.2768,\n",
       "           0.2675, 0.2718, 0.2800, 0.2751, 0.2675, 0.2768, 0.2768, 0.2950, 0.2675,\n",
       "           0.2703, 0.2664, 0.2675, 0.2950, 0.2938, 0.2938, 0.2668, 0.2675, 0.2675,\n",
       "           0.2675, 0.2675, 0.2775, 0.2929, 0.2671, 0.2646, 0.2672, 0.2675, 0.2950,\n",
       "           0.2675, 0.2768, 0.2768, 0.2768, 0.2636, 0.2903, 0.2675, 0.2788, 0.2781,\n",
       "           0.2671, 0.2675, 0.2680, 0.2664, 0.2693, 0.2675, 0.2708, 0.2675, 0.2950,\n",
       "           0.2804, 0.2651, 0.2708, 0.2950, 0.2660, 0.2781, 0.2938, 0.2950, 0.2768,\n",
       "           0.2675, 0.2740, 0.2700, 0.2938, 0.2938, 0.3270, 0.3017, 0.5174, 0.3578,\n",
       "           0.4041, 0.5174, 0.3017, 0.3151, 0.3017, 0.3016, 0.3784, 0.5174, 0.5174,\n",
       "           0.4041, 0.3578, 0.3048, 0.3207, 0.3578, 0.4041, 0.4041, 0.3578, 0.3578,\n",
       "           0.4041, 0.3578, 0.3270, 0.3017, 0.4041, 0.3315, 0.3619, 0.3784, 0.5174,\n",
       "           0.3048, 0.3578, 0.3678, 0.3284, 0.3048, 0.4041, 0.3784, 0.3016, 0.3784,\n",
       "           0.3151, 0.3378, 0.3270, 0.3017, 0.3151, 0.3689, 0.4041, 0.3048, 0.3784,\n",
       "           0.3784, 0.5174, 0.3017, 0.3151, 0.3784, 0.3048, 0.3701, 0.3016, 0.2963,\n",
       "           0.3784, 0.3284, 0.3701, 0.5174, 0.3048, 0.4041, 0.3048, 0.3784, 0.5174,\n",
       "           0.3048, 0.3016, 0.3017, 0.3016, 0.3270, 0.3016, 0.3151, 0.5174, 0.3284,\n",
       "           0.3578, 0.3016, 0.3151, 0.3016, 0.4041, 0.3151, 0.3270, 0.3701, 0.3701,\n",
       "           0.3411, 0.5174, 0.3784, 0.3678, 0.3284, 0.3701, 0.3434, 0.3277, 0.3578,\n",
       "           0.3270, 0.3017, 0.3284, 0.3662, 0.3016, 0.3578, 0.3017, 0.3784, 0.3181,\n",
       "           0.5174, 0.5174, 0.3336, 0.3270, 0.3703, 0.4041, 0.3784, 0.3048, 0.3270,\n",
       "           0.4101, 0.3151, 0.3048, 0.4041, 0.3016, 0.3017, 0.3048, 0.3578, 0.3016,\n",
       "           0.3048, 0.3543, 0.3016, 0.3093, 0.3270, 0.5174, 0.3163, 0.3151, 0.3701,\n",
       "           0.3669, 0.3701, 0.3784, 0.3151, 0.3701, 0.3270, 0.3578, 0.3151, 0.3284,\n",
       "           0.3151, 0.3413, 0.5174, 0.3205, 0.3270, 0.4041, 0.3270, 0.3048, 0.3048,\n",
       "           0.3270, 0.3017, 0.3016, 0.3016, 0.3016, 0.3270, 0.3578, 0.3784, 0.3701,\n",
       "           0.3511, 0.4041, 0.3151, 0.3046, 0.2916, 0.3240, 0.3065, 0.3002, 0.3105,\n",
       "           0.3118, 0.3075, 0.3084, 0.2999, 0.2997, 0.2924, 0.3028, 0.2899, 0.3026,\n",
       "           0.3173, 0.3020, 0.3065, 0.3036, 0.3047, 0.2881, 0.2925, 0.3054, 0.3008,\n",
       "           0.3165, 0.2992, 0.3081, 0.2915, 0.2908, 0.3932, 0.3090, 0.2916, 0.2999,\n",
       "           0.2935, 0.7247, 0.3792, 0.4230, 0.3071, 0.3801, 0.3962, 0.3439, 0.3909,\n",
       "           0.3370, 0.3464, 0.2896, 0.3962, 0.3792, 0.2993, 0.3374, 0.4610, 0.3030,\n",
       "           0.2798, 0.3346, 0.4089, 0.3296, 0.2859, 0.4352, 0.3372, 0.2892, 0.2870,\n",
       "           0.2988, 0.3484, 0.3353, 0.3237, 0.3962, 0.3481, 0.3264, 0.2927, 0.3484,\n",
       "           0.4797, 0.3532, 0.3400, 0.3274, 0.3358, 0.3484, 0.3030, 0.3370, 0.3801,\n",
       "           0.3088, 0.3068, 0.2844, 0.3413, 0.3221, 0.3370, 0.3792, 0.2834, 0.2795,\n",
       "           0.3632, 0.3194, 0.2912, 0.3690, 0.4610, 0.3792, 0.3962, 0.2711, 0.3372,\n",
       "           0.3226, 0.3030, 0.3372, 0.3372, 0.3161, 0.3260, 0.3081, 0.3005, 0.2949,\n",
       "           0.7247, 0.4089, 0.3063, 0.2854, 0.2914, 0.3025, 0.2935, 0.4214, 0.2888,\n",
       "           0.7247, 0.3232, 0.3071, 0.3962, 0.2934, 0.3024, 0.3044, 0.3534, 0.3227,\n",
       "           0.2948, 0.2795, 0.3962, 0.3481, 0.7247, 0.3801, 0.2745, 0.2870, 0.3355,\n",
       "           0.3632, 0.3162, 0.3632, 0.3043]),\n",
       "   tensor([0.2497, 0.2415, 0.3682, 0.2836, 0.2791, 0.2361, 0.3716, 0.3175, 0.3010,\n",
       "           0.2096, 0.2970, 0.2365, 0.2639, 0.2309, 0.2737, 0.2746, 0.2380, 0.2812,\n",
       "           0.2721, 0.2829, 0.3682, 0.2055, 0.3347, 0.2970, 0.2167, 0.2839, 0.2673,\n",
       "           0.2757, 0.2891, 0.2076, 0.3682, 0.2042, 0.2970, 0.2364, 0.3224, 0.3352,\n",
       "           0.3506, 0.2034, 0.3682, 0.2625, 0.2055, 0.2970, 0.2046, 0.2519, 0.2141,\n",
       "           0.3347, 0.2343, 0.3246, 0.2970, 0.3190, 0.3246, 0.2519, 0.3347, 0.2921,\n",
       "           0.2207, 0.3347, 0.2055, 0.2545, 0.2164, 0.2970, 0.2335, 0.2929, 0.3170,\n",
       "           0.2582, 0.3175, 0.2611, 0.3352, 0.2519, 0.3190, 0.2891, 0.3682, 0.2569,\n",
       "           0.3506, 0.2359, 0.2098, 0.2248, 0.2507, 0.3306, 0.2564, 0.1938, 0.2974,\n",
       "           0.3506, 0.2708, 0.3347, 0.2268, 0.2726, 0.4354, 0.2359, 0.2970, 0.2191,\n",
       "           0.2314, 0.2767, 0.2455, 0.2285, 0.3506, 0.2363, 0.2837, 0.3175, 0.2169,\n",
       "           0.2369, 0.2640, 0.3347, 0.2370, 0.2151, 0.2457, 0.2417, 0.2887, 0.2191,\n",
       "           0.2970, 0.2055, 0.2012, 0.2506, 0.3246, 0.2977, 0.3347, 0.2469, 0.3682,\n",
       "           0.2896, 0.2631, 0.3682, 0.2055, 0.2970, 0.2507, 0.2769, 0.3066, 0.2538,\n",
       "           0.2526, 0.3090, 0.3506, 0.2965, 0.2030, 0.3352, 0.3682, 0.2166, 0.3347,\n",
       "           0.2055, 0.2293, 0.2176, 0.2039, 0.2346, 0.2443, 0.2908, 0.2201, 0.3347,\n",
       "           0.2507, 0.2917, 0.2507, 0.2656, 0.2061, 0.2348, 0.2121, 0.3347, 0.3347,\n",
       "           0.2099, 0.3091, 0.2373, 0.2969, 0.2970, 0.3079, 0.1956, 0.2478, 0.2642,\n",
       "           0.3306, 0.3306, 0.2859, 0.2535, 0.2090, 0.3306, 0.3306, 0.2080, 0.3347,\n",
       "           0.2390, 0.1946, 0.3682, 0.3506, 0.3682, 0.2698, 0.3032, 0.2884, 0.2801,\n",
       "           0.3305, 0.2929, 0.2670, 0.2960, 0.2907, 0.2720, 0.2879, 0.2707, 0.2789,\n",
       "           0.2910, 0.3190, 0.2626, 0.2814, 0.2919, 0.2952, 0.2949, 0.2642, 0.3009,\n",
       "           0.2827, 0.5224, 0.4978, 0.3306, 0.2919, 0.2642, 0.2970, 0.2970, 0.3306,\n",
       "           0.5668, 0.2919, 0.3009, 0.2642, 0.2949, 0.2949, 0.2952, 0.4176, 0.2970,\n",
       "           0.3306, 0.3009, 0.2970, 0.3009, 0.2642, 0.3306, 0.2949, 0.3009, 0.2944,\n",
       "           0.2944, 0.2952, 0.2642, 0.2949, 0.2919, 0.2944, 0.2642, 0.2970, 0.2919,\n",
       "           0.2919, 0.2919, 0.2949, 0.4812, 0.2642, 0.2944, 0.2944, 0.3246, 0.2932,\n",
       "           0.3009, 0.2970, 0.2919, 0.2949, 0.2642, 0.2970, 0.2970, 0.2642, 0.2970,\n",
       "           0.2952, 0.2919, 0.2970, 0.2949, 0.5395, 0.2970, 0.2952, 0.2919, 0.2642,\n",
       "           0.3306, 0.2944, 0.4376, 0.2642, 0.3246, 0.2881, 0.3306, 0.2642, 0.2970,\n",
       "           0.3306, 0.2952, 0.2858, 0.2949, 0.2919, 0.2949, 0.2944, 0.2970, 0.2952,\n",
       "           0.3009, 0.2952, 0.3246, 0.2944, 0.2949, 0.3246, 0.3009, 0.2952, 0.2949,\n",
       "           0.2944, 0.3306, 0.2642, 0.2877, 0.2949, 0.3009, 0.2944, 0.2970, 0.2970,\n",
       "           0.2970, 0.2528, 0.2449, 0.2582, 0.2317, 0.2532, 0.2433, 0.2319, 0.2529,\n",
       "           0.2308, 0.2347, 0.2322, 0.2351, 0.2350, 0.2460, 0.2372, 0.2441, 0.2352,\n",
       "           0.2316, 0.2324, 0.2302, 0.2334, 0.2316, 0.2359, 0.2337, 0.2310, 0.2343,\n",
       "           0.2686, 0.2222, 0.2513, 0.2195, 0.2228, 0.2217, 0.2813, 0.2181, 0.2524,\n",
       "           0.2297, 0.2451, 0.2631, 0.2256, 0.2597, 0.2192, 0.2250, 0.2642, 0.2570,\n",
       "           0.2184, 0.2489, 0.2198, 0.2217, 0.2205, 0.2190, 0.2780, 0.2151, 0.2229,\n",
       "           0.2575, 0.2248, 0.2241, 0.2541, 0.2335, 0.2164, 0.2190, 0.2235, 0.2172,\n",
       "           0.2597, 0.2196, 0.2196, 0.2217, 0.2274, 0.2184, 0.2123, 0.2317, 0.2214,\n",
       "           0.2239, 0.2642, 0.2220, 0.2236, 0.2596, 0.2203, 0.2631, 0.2607, 0.2294,\n",
       "           0.2143, 0.2268, 0.2214, 0.2199, 0.2206, 0.2646, 0.2306, 0.2228, 0.2171,\n",
       "           0.2215, 0.2299, 0.2441, 0.2352, 0.2217, 0.2553, 0.2212, 0.2363, 0.2626,\n",
       "           0.2191, 0.2252, 0.2156, 0.2262, 0.2146, 0.2230, 0.2186, 0.2220, 0.2426,\n",
       "           0.2137, 0.2213, 0.2346, 0.2137, 0.2405, 0.2731, 0.2231, 0.2238, 0.2262,\n",
       "           0.2139, 0.2221]),\n",
       "   tensor([0.3007, 0.2970, 0.2970, 0.5094, 0.5094, 0.3007, 0.3291, 0.3249, 0.3291,\n",
       "           0.3091, 0.3091, 0.3091, 0.2970, 0.3641, 0.3321, 0.3007, 0.3007, 0.2970,\n",
       "           0.3249, 0.3291, 0.5094, 0.2969, 0.3091, 0.3007, 0.3321, 0.3007, 0.3776,\n",
       "           0.3091, 0.3091, 0.3291, 0.2970, 0.3822, 0.2970, 0.3321, 0.2970, 0.3776,\n",
       "           0.5094, 0.3007, 0.2970, 0.3007, 0.2970, 0.3822, 0.3091, 0.3091, 0.3291,\n",
       "           0.3249, 0.2954, 0.5094, 0.3091, 0.3249, 0.3361, 0.2970, 0.2980, 0.2970,\n",
       "           0.5094, 0.2970, 0.2899, 0.3249, 0.3091, 0.3051, 0.3091, 0.3822, 0.3321,\n",
       "           0.3321, 0.3249, 0.5094, 0.3822, 0.3249, 0.2980, 0.2938, 0.3321, 0.3291,\n",
       "           0.3822, 0.2848, 0.3007, 0.5094, 0.2980, 0.3822, 0.3776, 0.3776, 0.2879,\n",
       "           0.3641, 0.5094, 0.2970, 0.3321, 0.3007, 0.3321, 0.3776, 0.3361, 0.3091,\n",
       "           0.3249, 0.3291, 0.2980, 0.3007, 0.3291, 0.3776, 0.2901, 0.3091, 0.3822,\n",
       "           0.3007, 0.3249, 0.3291, 0.2980, 0.3007, 0.2970, 0.3291, 0.3007, 0.3776,\n",
       "           0.3091, 0.2871, 0.2970, 0.2962, 0.2849, 0.3776, 0.3291, 0.2970, 0.2970,\n",
       "           0.2940, 0.3249, 0.3776, 0.3291, 0.2970, 0.3776, 0.3007, 0.3091, 0.3012,\n",
       "           0.3007, 0.3822, 0.2980, 0.3291, 0.3007, 0.3042, 0.2889, 0.3776, 0.3011,\n",
       "           0.2980, 0.3291, 0.3321, 0.2970, 0.3641, 0.2970, 0.3291, 0.2870, 0.3291,\n",
       "           0.2980, 0.2970, 0.2949, 0.2970, 0.3091, 0.3249, 0.3321, 0.2970, 0.3249,\n",
       "           0.3007, 0.3091, 0.3051, 0.3822, 0.2854, 0.2985, 0.3007, 0.3822, 0.2864,\n",
       "           0.3321, 0.3091, 0.3291, 0.3258, 0.3641, 0.3361, 0.5094, 0.3091, 0.3091,\n",
       "           0.3321, 0.3249, 0.2942, 0.3026, 0.3249, 0.2851, 0.3321, 0.3091, 0.2963,\n",
       "           0.2980, 0.2970, 0.3091, 0.3249, 0.5094, 0.2970, 0.3091, 0.3091, 0.3822,\n",
       "           0.4317, 0.3341, 0.4317, 0.3747, 0.3575, 0.3747, 0.4519, 0.3286, 0.3037,\n",
       "           0.5115, 0.4002, 0.3170, 0.4225, 0.3138, 0.5548, 0.3747, 0.5134, 0.4266,\n",
       "           0.4317, 0.4168, 0.4010, 0.3711, 0.4914, 0.4115, 0.3747, 0.3446, 0.5253,\n",
       "           0.3249, 0.3701, 0.3473, 0.3158, 0.3105, 0.4225, 0.2945, 0.4317, 0.3849,\n",
       "           0.4932, 0.3867, 0.3215, 0.3468, 0.6381, 0.4035, 0.3741, 0.4087, 0.4786,\n",
       "           0.2890, 0.4229, 0.3252, 0.3003, 0.4786, 0.2891, 0.5854, 0.3420, 0.3071,\n",
       "           0.4184, 0.4321, 0.3560, 0.3983, 0.2990, 0.3384, 0.3648, 0.3555, 0.3694,\n",
       "           0.3456, 0.4482, 0.3242, 0.3059, 0.3747, 0.4029, 0.4087, 0.3045, 0.3997,\n",
       "           0.5652, 0.2979, 0.3568, 0.3278, 0.3138, 0.4317, 0.3918, 0.4087, 0.4406,\n",
       "           0.3181, 0.3741, 0.4317, 0.4006, 0.4198, 0.2909, 0.3017, 0.4091, 0.3118,\n",
       "           0.4029, 0.3204, 0.6018, 0.4010, 0.4078, 0.4786, 0.3160, 0.4087, 0.4029,\n",
       "           0.3461, 0.3284, 0.3821, 0.5198, 0.3822, 0.3111, 0.9991, 0.3436, 0.3146,\n",
       "           0.3140, 0.4226, 0.3822, 0.2875, 0.3249, 0.7914, 0.3119, 0.3075, 0.3236,\n",
       "           0.3088, 0.5199, 0.3118, 0.3023, 0.2957, 0.3361, 0.5044, 0.2980, 0.3279,\n",
       "           0.3131, 0.3240, 0.3031, 0.3776, 0.3436, 0.3186, 0.5094, 0.2957, 0.3119,\n",
       "           0.3822, 0.3091, 0.5094, 0.3076, 0.3399, 0.3377, 0.4481, 0.3282, 0.2866,\n",
       "           0.3553, 0.3269, 0.3496, 0.3116, 0.3030, 0.2881, 0.3140, 0.3502, 0.3291,\n",
       "           0.3269, 0.3313, 0.5094, 0.3390, 0.3262, 0.3822, 0.9991, 0.3822, 0.5094,\n",
       "           0.3666, 0.2979, 0.3428, 0.3401, 0.3015, 0.3091, 0.9991, 0.2964, 0.3517,\n",
       "           0.3552, 0.3003, 0.5094, 0.3748, 0.3331, 0.3776, 0.3822, 0.3822, 0.4396,\n",
       "           0.4931, 0.9991, 0.9991, 0.3822, 0.3157, 0.3091, 0.3002, 0.3147, 0.9991,\n",
       "           0.3158, 0.4034, 0.2995, 0.3338, 0.3259, 0.9991, 0.3139, 0.3436, 0.3336,\n",
       "           0.3061, 0.5199, 0.2968, 0.3822, 0.3080, 0.3278, 0.3178, 0.5199, 0.3259,\n",
       "           0.3036, 0.3049, 0.3522, 0.3053, 0.3149, 0.4395, 0.3361, 0.3194, 0.3436,\n",
       "           0.3747, 0.3075, 0.3045, 0.5638, 0.3499, 0.3075, 0.3070, 0.3091, 0.3123,\n",
       "           0.3091, 0.3013, 0.3730, 0.4619, 0.3095, 0.3091, 0.3256, 0.3054, 0.3291,\n",
       "           0.4169, 0.3348, 0.3224, 0.2935, 0.3348, 0.2944, 0.3156, 0.3091, 0.3255,\n",
       "           0.2950, 0.2945, 0.3178, 0.2934, 0.3291, 0.2983, 0.9991, 0.9991, 0.3199,\n",
       "           0.3331, 1.1425, 0.3119, 0.3085, 0.3245, 0.3309, 0.3361, 0.3152, 0.3222,\n",
       "           0.3091, 0.3211, 0.2955, 0.3004, 0.3361, 0.3395, 0.3641, 0.4050, 0.3023,\n",
       "           0.3555, 0.3147, 0.3147, 0.3147, 0.2980, 0.3005, 0.3147, 0.2958, 0.2999,\n",
       "           0.2983, 0.3002, 0.3133, 0.2958, 0.3134, 0.3147, 0.3005, 0.3128, 0.2993,\n",
       "           0.3147, 0.3008, 0.3147, 0.3147, 0.2965, 0.3124, 0.3192, 0.3180, 0.3212,\n",
       "           0.3404, 0.3148, 0.3150, 0.3170, 0.3277, 0.3301, 0.3177, 0.3253, 0.3564])]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "class SurfaceBatchNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features=1, \n",
    "        momentum=0.1\n",
    "    ):\n",
    "        super(SurfaceBatchNorm, self).__init__()\n",
    "        self.log_moneyness_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.time_to_maturity_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_return_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_volatility_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.treasury_rate_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Concatenate all tensors from the Input Surface into one tensor for each feature\n",
    "        input_surface_log_moneyness = torch.cat([x for x in batch['Input Surface']['Log Moneyness']])\n",
    "        input_surface_time_to_maturity = torch.cat([x for x in batch['Input Surface']['Time to Maturity']])\n",
    "\n",
    "        # Concatenate Input Surface tensors with Query Points tensors\n",
    "        total_log_moneyness = torch.cat([input_surface_log_moneyness] + [x for x in batch['Query Points']['Log Moneyness']])\n",
    "        total_time_to_maturity = torch.cat([input_surface_time_to_maturity] + [x for x in batch['Query Points']['Time to Maturity']])\n",
    "\n",
    "        # Normalize Log Moneyness and Time to Maturity\n",
    "        norm_log_moneyness = self.log_moneyness_bn(total_log_moneyness.unsqueeze(1)).squeeze(1)\n",
    "        norm_time_to_maturity = self.time_to_maturity_bn(total_time_to_maturity.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Split the normalized results back to corresponding structures\n",
    "        input_surface_sizes = [len(x) for x in batch['Input Surface']['Log Moneyness']]\n",
    "        query_points_sizes = [len(x) for x in batch['Query Points']['Log Moneyness']]\n",
    "        total_input_size = sum(input_surface_sizes)\n",
    "\n",
    "        # Normalizing Market Features\n",
    "        market_features = batch['Market Features']\n",
    "        norm_market_return = self.market_return_bn(market_features['Market Return'].unsqueeze(1)).squeeze(1)\n",
    "        norm_market_volatility = self.market_volatility_bn(market_features['Market Volatility'].unsqueeze(1)).squeeze(1)\n",
    "        norm_treasury_rate = self.treasury_rate_bn(market_features['Treasury Rate'].unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Reconstructing the batch with normalized data\n",
    "        output = {\n",
    "            'Datetime': batch['Datetime'],\n",
    "            'Symbol': batch['Symbol'],\n",
    "            'Market Features': {\n",
    "                'Market Return': norm_market_return,\n",
    "                'Market Volatility': norm_market_volatility,\n",
    "                'Treasury Rate': norm_treasury_rate\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[:total_input_size], input_surface_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[:total_input_size], input_surface_sizes)),\n",
    "                'Implied Volatility': batch['Input Surface']['Implied Volatility']\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[total_input_size:], query_points_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[total_input_size:], query_points_sizes)),\n",
    "                'Implied Volatility': batch['Query Points']['Implied Volatility']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Ensure requires_grad is True for query point values\n",
    "        for key in output['Query Points']:\n",
    "            if key != 'Implied Volatility':  # We only set requires_grad for Log Moneyness and Time to Maturity\n",
    "                for tensor in output['Query Points'][key]:\n",
    "                    tensor.requires_grad_()\n",
    "\n",
    "        return output\n",
    "\n",
    "# Usage\n",
    "surfacebatchnorm = SurfaceBatchNorm()\n",
    "processed_batch = surfacebatchnorm(batch)\n",
    "processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input Surface': [tensor([[ 0.2954,  2.0830, -0.5693,  ...,  0.6434, -0.9860,  0.2933],\n",
       "          [ 0.5820,  2.0090, -0.7119,  ...,  0.6715, -0.9329,  0.3644],\n",
       "          [ 0.8457,  1.9012, -0.8559,  ...,  0.7113, -0.8666,  0.4503],\n",
       "          ...,\n",
       "          [ 2.7806,  0.6177,  1.5776,  ...,  0.3637, -0.9704,  0.8234],\n",
       "          [ 2.7725,  0.5866,  1.5793,  ...,  0.3655, -0.9779,  0.8089],\n",
       "          [ 2.7725,  0.5866,  1.5793,  ...,  0.3655, -0.9779,  0.8089]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[ 1.3062,  1.5115, -1.1618,  ...,  0.7953, -0.7169,  0.6365],\n",
       "          [ 1.3062,  1.5115, -1.1618,  ...,  0.7953, -0.7169,  0.6365],\n",
       "          [ 1.3474,  1.5246, -1.1666,  ...,  0.8006, -0.7135,  0.6377],\n",
       "          ...,\n",
       "          [ 1.2324, -0.5439, -0.4405,  ...,  0.8758,  0.6794,  3.5103],\n",
       "          [ 1.2221, -0.5732, -0.4372,  ...,  0.8731,  0.6769,  3.5101],\n",
       "          [ 1.2221, -0.5732, -0.4372,  ...,  0.8731,  0.6769,  3.5101]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[ 0.3913,  1.8861, -0.6716,  ...,  0.6385, -0.9398,  0.3712],\n",
       "          [ 0.4226,  1.8968, -0.6847,  ...,  0.6412, -0.9348,  0.3779],\n",
       "          [ 0.4560,  1.9053, -0.6986,  ...,  0.6440, -0.9295,  0.3850],\n",
       "          ...,\n",
       "          [-0.0979,  0.1727,  0.0526,  ...,  1.8188,  1.1613,  3.2020],\n",
       "          [-0.0445,  0.1425,  0.0503,  ...,  1.8105,  1.1601,  3.2100],\n",
       "          [-0.0185,  0.1272,  0.0492,  ...,  1.8063,  1.1594,  3.2140]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[ 1.1002,  1.7467, -0.6764,  ...,  0.7350, -1.0008,  0.1581],\n",
       "          [ 1.1002,  1.7467, -0.6764,  ...,  0.7350, -1.0008,  0.1581],\n",
       "          [ 1.1307,  1.7884, -0.6677,  ...,  0.7300, -1.0019,  0.1613],\n",
       "          ...,\n",
       "          [ 0.6806, -1.0847, -0.3904,  ...,  1.3295,  0.9814,  3.5876],\n",
       "          [ 0.6712, -1.1160, -0.3898,  ...,  1.3273,  0.9791,  3.5889],\n",
       "          [ 0.6712, -1.1160, -0.3898,  ...,  1.3273,  0.9791,  3.5889]],\n",
       "         grad_fn=<AddBackward0>)],\n",
       " 'Query Points': [tensor([[ 0.3628, -0.7261, -1.1622,  ...,  1.5613,  0.7390,  2.9457],\n",
       "          [ 0.2811,  0.2374, -1.9163,  ...,  1.9179,  0.9791,  2.8471],\n",
       "          [ 2.2110,  0.5259, -1.7528,  ...,  1.4318,  0.5770,  2.5588],\n",
       "          ...,\n",
       "          [ 0.9030,  0.4426, -0.6452,  ...,  1.3521,  0.6789,  2.9423],\n",
       "          [ 0.9727,  0.5818, -0.8049,  ...,  1.4163,  0.7364,  2.9531],\n",
       "          [ 2.7053,  0.7373,  0.2065,  ...,  0.8003,  0.0166,  2.2001]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[-2.1859,  0.7395, -0.3093,  ...,  2.0384,  0.6405,  2.0523],\n",
       "          [ 1.8877,  2.1180, -0.7651,  ...,  0.8453, -0.8280,  0.3536],\n",
       "          [ 1.5853,  1.7780, -1.0330,  ...,  0.8081, -0.7846,  0.4869],\n",
       "          ...,\n",
       "          [-3.0891,  0.3138, -0.0144,  ...,  1.9991,  0.5966,  1.9889],\n",
       "          [ 0.0909,  2.1668, -0.0764,  ...,  0.6320, -0.9388,  0.3606],\n",
       "          [-2.7189, -0.1926,  0.1882,  ...,  2.1856,  0.9948,  2.5518]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[-2.6095,  0.7538, -0.8107,  ...,  2.0269,  0.6968,  2.1784],\n",
       "          [-2.0862,  1.0376, -0.9952,  ...,  1.8057,  0.4313,  1.8765],\n",
       "          [ 0.7638,  1.9090, -0.8261,  ...,  0.6762, -0.8760,  0.4540],\n",
       "          ...,\n",
       "          [-2.0091,  0.0996, -0.0654,  ...,  2.1989,  0.9467,  2.4689],\n",
       "          [-1.3093,  0.1194, -0.8272,  ...,  2.1935,  0.9736,  2.5359],\n",
       "          [-2.3380,  0.4406, -0.5082,  ...,  2.1261,  0.8276,  2.3280]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[ 0.3350, -1.4008,  0.1436,  ...,  2.1288,  1.1828,  2.9606],\n",
       "          [-0.3864, -1.0711, -0.6797,  ...,  2.1641,  0.9616,  2.5430],\n",
       "          [-0.3876, -1.3149, -0.7108,  ...,  2.1603,  0.9842,  2.5899],\n",
       "          ...,\n",
       "          [-0.9553,  0.6561, -0.4487,  ...,  1.5489,  1.1099,  3.4450],\n",
       "          [-1.6412,  0.4452, -0.1956,  ...,  1.3030,  0.9658,  3.4862],\n",
       "          [-1.1524,  1.6515,  1.1188,  ...,  0.0862, -0.4596,  2.1158]],\n",
       "         grad_fn=<AddBackward0>)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class EllipticalRBFKernel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        bandwidth\n",
    "    ):\n",
    "        super(EllipticalRBFKernel, self).__init__()\n",
    "        self.bandwidth = bandwidth\n",
    "        # Initialize the log of the scale vector to zero, which corresponds to scale factors of one\n",
    "        self.log_scale = nn.Parameter(torch.zeros(input_dim))\n",
    "\n",
    "    def forward(self, distances):\n",
    "        # Convert log scale to actual scale values\n",
    "        scale = torch.exp(self.log_scale)\n",
    "        \n",
    "        # Create a diagonal scale matrix\n",
    "        scale_matrix = torch.diag(scale)\n",
    "\n",
    "        # Calculate the scaled distances\n",
    "        scaled_distances = distances @ scale_matrix @ distances.t()\n",
    "        \n",
    "        # Normalize by the trace of the scale matrix\n",
    "        trace_scale_matrix = torch.trace(scale_matrix)\n",
    "        normalized_distances = scaled_distances / trace_scale_matrix\n",
    "\n",
    "        # Compute the RBF kernel output using the normalized distances\n",
    "        kernel_values = torch.exp(-normalized_distances / (2 * self.bandwidth ** 2))\n",
    "\n",
    "        return kernel_values\n",
    "\n",
    "class SurfaceContinuousKernelPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_embedding):\n",
    "        super(SurfaceContinuousKernelPositionalEmbedding, self).__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "\n",
    "        # Initialize multiple RBF kernels, each with a different fixed bandwidth\n",
    "        self.kernels = nn.ModuleList()\n",
    "        for i in range(1, d_embedding + 1):\n",
    "            bandwidth_value = torch.erfinv(torch.tensor(i / (d_embedding + 1))) * np.sqrt(2)\n",
    "            self.kernels.append(EllipticalRBFKernel(bandwidth=bandwidth_value, input_dim=2))\n",
    "\n",
    "        self.input_surface_layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.query_points_layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "        # Initialize learnable scaling parameter (the base for positional embedding)\n",
    "        self.log_scale = nn.Parameter(torch.log(torch.tensor(10000.0)))\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_surface_batch, \n",
    "        query_points_batch\n",
    "    ):\n",
    "        batch_size = len(input_surface_batch['Log Moneyness'])\n",
    "\n",
    "        input_surface_embeddings = []\n",
    "        query_points_embeddings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Extract the coordinates and implied volatilities for each surface in the batch\n",
    "            surface_coords = torch.stack([\n",
    "                input_surface_batch['Log Moneyness'][i], \n",
    "                input_surface_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "            surface_ivs = input_surface_batch['Implied Volatility'][i]\n",
    "\n",
    "            query_coords = torch.stack([\n",
    "                query_points_batch['Log Moneyness'][i], \n",
    "                query_points_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "\n",
    "            all_coords = torch.cat((surface_coords, query_coords), dim=0)\n",
    "\n",
    "            # Compute the pairwise differences between all points and the input surface points\n",
    "            point_differences = all_coords.unsqueeze(1) - surface_coords.unsqueeze(0)  # (n+m, n, 2)\n",
    "\n",
    "            # Initialize the output embeddings for the current surface with d_embedding channels\n",
    "            all_embedded = torch.zeros((all_coords.shape[0], self.d_embedding), dtype=torch.float32, device=surface_coords.device)\n",
    "\n",
    "            for kernel_idx, kernel in enumerate(self.kernels):\n",
    "                # Apply the RBF kernel to each distance vector using torch.vmap\n",
    "                vmap_kernel = torch.vmap(kernel, in_dims=(0,))\n",
    "                kernel_outputs = vmap_kernel(point_differences.view(-1, point_differences.shape[-1]))  # ((n+m) * n)\n",
    "                kernel_outputs = kernel_outputs.view(all_coords.shape[0], surface_coords.shape[0])  # (n+m, n)\n",
    "\n",
    "                # Compute the weighted sum of IVs based on the kernel outputs\n",
    "                weighted_sum = (kernel_outputs * surface_ivs.unsqueeze(0)).sum(dim=1)\n",
    "                normalization_factor = kernel_outputs.sum(dim=1)\n",
    "\n",
    "                all_embedded[:, kernel_idx] = weighted_sum / normalization_factor\n",
    "\n",
    "            # Split the embeddings into input surface and query points embeddings\n",
    "            input_surface_embedded = all_embedded[:surface_coords.shape[0], :]\n",
    "            query_points_embedded = all_embedded[surface_coords.shape[0]:, :]\n",
    "\n",
    "            # Normalize the embedded surfaces\n",
    "            input_surface_embedded = self.input_surface_layer_norm(input_surface_embedded)\n",
    "            query_points_embedded = self.query_points_layer_norm(query_points_embedded)\n",
    "\n",
    "            # Positional embedding for input surface points\n",
    "            input_surface_pe = self._compute_positional_embedding(surface_coords)\n",
    "\n",
    "            # Positional embedding for query points\n",
    "            query_points_pe = self._compute_positional_embedding(query_coords)\n",
    "\n",
    "            # Add positional embeddings with a factor of sqrt(2)\n",
    "            input_surface_final = input_surface_embedded + input_surface_pe * np.sqrt(2)\n",
    "            query_points_final = query_points_embedded + query_points_pe * np.sqrt(2)\n",
    "\n",
    "            # Append the encoded surface for this input surface to the batch list\n",
    "            input_surface_embeddings.append(input_surface_final)\n",
    "            query_points_embeddings.append(query_points_final)\n",
    "\n",
    "        # Keep all encoded surfaces as lists to handle variable lengths\n",
    "        return {\n",
    "            'Input Surface': input_surface_embeddings,\n",
    "            'Query Points': query_points_embeddings\n",
    "        }\n",
    "\n",
    "    def _compute_positional_embedding(\n",
    "        self, \n",
    "        coords, \n",
    "    ):\n",
    "        positional_embedding = torch.zeros(coords.size(0), self.d_embedding, device=coords.device)\n",
    "\n",
    "        for i in range(self.d_embedding // 4):\n",
    "            div_factor = torch.exp(self.log_scale) ** (4 * i / self.d_embedding)\n",
    "            positional_embedding[:, 4 * i] = torch.sin(coords[:, 0] / div_factor)\n",
    "            positional_embedding[:, 4 * i + 1] = torch.cos(coords[:, 0] / div_factor)\n",
    "            positional_embedding[:, 4 * i + 2] = torch.sin(coords[:, 1] / div_factor)\n",
    "            positional_embedding[:, 4 * i + 3] = torch.cos(coords[:, 1] / div_factor)\n",
    "\n",
    "        return positional_embedding\n",
    "\n",
    "# Example of initializing and using this module\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "\n",
    "continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "kernel_positional_embedded_batch = continuous_kernel_positional_embedding(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "kernel_positional_embedded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0401,  2.0104, -0.9130,  ...,  0.4237, -1.3722,  0.0378],\n",
       "         [ 0.3371,  1.9172, -1.0956,  ...,  0.4362, -1.3403,  0.0962],\n",
       "         [ 0.6080,  1.7753, -1.2737,  ...,  0.4594, -1.2855,  0.1708],\n",
       "         ...,\n",
       "         [-1.0275,  0.5390, -1.3384,  ...,  1.3468, -0.1811,  1.5858],\n",
       "         [-0.9579,  0.6055, -1.4113,  ...,  1.3381, -0.1463,  1.5341],\n",
       "         [ 0.2283,  0.9342, -1.0442,  ...,  1.2355, -0.8853,  1.3790]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[ 1.0984,  1.3221, -1.5918,  ...,  0.5415, -1.1069,  0.3683],\n",
       "         [ 1.0984,  1.3221, -1.5918,  ...,  0.5415, -1.1069,  0.3683],\n",
       "         [ 1.1237,  1.3151, -1.5924,  ...,  0.5329, -1.1028,  0.3570],\n",
       "         ...,\n",
       "         [-2.3258,  0.3800, -0.4917,  ...,  1.2400, -0.0622,  0.6894],\n",
       "         [-1.2907,  1.9219, -0.6732,  ...,  1.0791, -1.0044,  0.0715],\n",
       "         [-2.2406,  0.1332, -0.4394,  ...,  1.3251,  0.0881,  0.9311]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[ 0.1924,  1.9208, -1.0366,  ...,  0.4783, -1.3467,  0.1692],\n",
       "         [ 0.2223,  1.9185, -1.0519,  ...,  0.4737, -1.3397,  0.1708],\n",
       "         [ 0.2543,  1.9143, -1.0683,  ...,  0.4696, -1.3328,  0.1729],\n",
       "         ...,\n",
       "         [-2.1537,  0.2234, -0.6666,  ...,  1.3830,  0.0077,  0.9075],\n",
       "         [-1.8908,  0.2457, -1.0843,  ...,  1.4447,  0.0239,  0.9848],\n",
       "         [-2.1547,  0.4065, -0.7994,  ...,  1.3127, -0.0109,  0.8309]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[ 0.8519,  1.5750, -1.1353,  ...,  0.4434, -1.4982, -0.2019],\n",
       "         [ 0.8519,  1.5750, -1.1353,  ...,  0.4434, -1.4982, -0.2019],\n",
       "         [ 0.8664,  1.5936, -1.1219,  ...,  0.4234, -1.4914, -0.2054],\n",
       "         ...,\n",
       "         [-1.3452,  0.6050, -0.6289,  ...,  1.1251,  0.2319,  1.4180],\n",
       "         [-1.5732,  0.5561, -0.4543,  ...,  1.0528,  0.2182,  1.4677],\n",
       "         [-1.5569,  1.3872,  0.2280,  ...,  0.7000, -0.4154,  1.1181]],\n",
       "        grad_fn=<NativeLayerNormBackward0>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SurfaceEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        momentum=0.1\n",
    "    ):\n",
    "        super(SurfaceEmbedding, self).__init__()\n",
    "        self.batch_norm = SurfaceBatchNorm(num_features=1, momentum=momentum)\n",
    "        self.kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.mask_token = nn.Parameter(torch.randn(d_embedding))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Apply batch normalization\n",
    "        norm_batch = self.batch_norm(batch)\n",
    "\n",
    "        # Extract market features from processed batch and create external_features_batch tensor\n",
    "        market_features = norm_batch['Market Features']\n",
    "        external_features_batch = torch.stack([\n",
    "            market_features['Market Return'],\n",
    "            market_features['Market Volatility'],\n",
    "            market_features['Treasury Rate']\n",
    "        ], dim=-1)  # (batch, features)\n",
    "\n",
    "        # Compute kernel and positional embeddings\n",
    "        embeddings = self.kernel_positional_embedding(norm_batch['Input Surface'], norm_batch['Query Points'])\n",
    "\n",
    "        input_surface_embeddings = embeddings['Input Surface']\n",
    "        query_points_embeddings = embeddings['Query Points']\n",
    "\n",
    "        embedded_sequences = []\n",
    "\n",
    "        for input_surface_embedding, query_points_embedding in zip(input_surface_embeddings, query_points_embeddings):\n",
    "            # Add mask token to the query point embeddings\n",
    "            masked_query_points_embedding = query_points_embedding + self.mask_token\n",
    "\n",
    "            # Combine input surface embeddings and masked query points embeddings\n",
    "            combined_sequence = torch.cat((input_surface_embedding, masked_query_points_embedding), dim=0)\n",
    "\n",
    "            # Apply layer normalization\n",
    "            combined_sequence = self.layer_norm(combined_sequence)\n",
    "\n",
    "            embedded_sequences.append(combined_sequence)\n",
    "\n",
    "        return embedded_sequences, external_features_batch\n",
    "\n",
    "\n",
    "# Example of initializing and using this module\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "surface_embedding = SurfaceEmbedding(d_embedding=d_embedding)\n",
    "embedded_sequences_batch, external_features_batch = surface_embedding(batch)\n",
    "embedded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.3129,  1.4653, -1.2204,  ...,  1.2530, -1.6073,  0.0768],\n",
       "         [ 0.7833,  1.2334, -1.1345,  ...,  0.9228, -1.9027,  0.2909],\n",
       "         [ 1.0559,  1.0879, -1.2278,  ...,  1.2601, -1.4105,  0.1809],\n",
       "         ...,\n",
       "         [-0.2479, -0.0269, -1.6364,  ...,  1.4322, -0.4094,  1.5634],\n",
       "         [-0.4852,  0.2739, -1.4158,  ...,  1.4431, -0.0627,  1.4292],\n",
       "         [ 0.4488, -0.0256, -0.9882,  ...,  1.4369, -0.9499,  1.5133]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 1.6099,  0.6789, -0.8598,  ...,  1.1429, -1.2232,  0.2780],\n",
       "         [ 1.5252,  0.7215, -0.7823,  ...,  1.1578, -1.3290,  0.2657],\n",
       "         [ 1.3831,  0.8062, -0.7033,  ...,  1.0236, -1.6254,  0.5153],\n",
       "         ...,\n",
       "         [-2.2622,  0.6225, -0.3726,  ...,  1.3094, -0.0448,  0.7411],\n",
       "         [-1.3039,  1.6454, -0.1095,  ...,  1.4177, -0.9369, -0.0400],\n",
       "         [-2.1987,  0.4399, -0.2732,  ...,  1.1775,  0.3401,  0.9361]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 0.4655,  1.2916,  0.0053,  ...,  0.8106, -1.8208,  0.8815],\n",
       "         [ 0.7956,  1.3554, -0.2323,  ...,  0.9277, -1.7173,  0.5054],\n",
       "         [ 0.8003,  0.7536, -0.0667,  ...,  1.1793, -1.8821,  0.7242],\n",
       "         ...,\n",
       "         [-2.1999, -0.1294, -0.4774,  ...,  1.3975,  0.7147,  0.6180],\n",
       "         [-1.8627, -0.1811, -0.3648,  ...,  1.4949,  0.4564,  1.1833],\n",
       "         [-2.1973,  0.2117, -0.2396,  ...,  1.4422,  0.3758,  0.7847]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 1.0965,  1.6407,  0.0585,  ...,  0.6868, -1.2844, -1.2153],\n",
       "         [ 1.4626,  1.3596,  0.4783,  ...,  0.1599, -1.5759, -0.8715],\n",
       "         [ 1.1596,  1.5723, -0.2506,  ...,  0.9093, -0.9427, -1.1021],\n",
       "         ...,\n",
       "         [-0.7905,  1.2198,  0.2043,  ...,  1.0828,  0.4998,  0.3213],\n",
       "         [-1.1353,  1.2061,  0.0819,  ...,  1.0894,  0.3485,  0.8114],\n",
       "         [-0.8122,  1.2359,  0.6185,  ...,  0.9655, -0.4713,  0.3421]],\n",
       "        grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualNorm(nn.Module):\n",
    "    def __init__(self, d_embedding):\n",
    "        super(ResidualNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x, \n",
    "        sublayer_output\n",
    "    ):\n",
    "        return self.norm(x + sublayer_output)\n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        gate_dropout\n",
    "    ):\n",
    "        super(GatedAttentionFusion, self).__init__()\n",
    "        self.gate_layer = nn.Sequential(\n",
    "            nn.Linear(d_embedding * 2, d_embedding),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(gate_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        self_attn_output, \n",
    "        ext_attn_output\n",
    "    ):\n",
    "        # Concatenate self-attention and external attention outputs\n",
    "        concatenated_output = torch.cat((self_attn_output, ext_attn_output), dim=-1)\n",
    "        # Compute gate values\n",
    "        gate_values = self.gate_layer(concatenated_output)\n",
    "        # Calculate gated embedding\n",
    "        gated_embedding = gate_values * self_attn_output + (1 - gate_values) * ext_attn_output\n",
    "\n",
    "        return gated_embedding\n",
    "    \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        ffn_hidden_dim, \n",
    "        ffn_dropout\n",
    "    ):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_embedding, ffn_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ffn_dropout),\n",
    "            nn.Linear(ffn_hidden_dim, d_embedding),\n",
    "            nn.Dropout(ffn_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.feedforward(x)    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        n_heads, \n",
    "        ffn_hidden_dim, \n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.external_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            kdim=external_dim, \n",
    "            vdim=external_dim, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.gated_attention_fusion = GatedAttentionFusion(d_embedding, gate_dropout)\n",
    "        self.residual_norm1 = ResidualNorm(d_embedding)\n",
    "        self.feed_forward = FeedForwardNetwork(d_embedding, ffn_hidden_dim, ffn_dropout)\n",
    "        self.residual_norm2 = ResidualNorm(d_embedding)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        surface_embeddings, \n",
    "        external_features\n",
    "    ):\n",
    "        # Self-Attention\n",
    "        self_attn_output, _ = self.self_attention(surface_embeddings, surface_embeddings, surface_embeddings)\n",
    "        # External Attention\n",
    "        ext_attn_output, _ = self.external_attention(surface_embeddings, external_features, external_features) \n",
    "        # Gated Attention Fusion\n",
    "        gated_embedding = self.gated_attention_fusion(self_attn_output, ext_attn_output)\n",
    "        # Residual Connection and Layer Normalization\n",
    "        surface_embeddings = self.residual_norm1(surface_embeddings, gated_embedding)\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.feed_forward(surface_embeddings)\n",
    "        # Final Residual Connection and Layer Normalization\n",
    "        surface_embeddings = self.residual_norm2(surface_embeddings, ffn_output)\n",
    "        \n",
    "        return surface_embeddings\n",
    "\n",
    "class SurfaceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim\n",
    "    ):\n",
    "        super(SurfaceEncoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            Encoder(\n",
    "                d_embedding, \n",
    "                n_heads, \n",
    "                ffn_hidden_dim, \n",
    "                attention_dropout, \n",
    "                gate_dropout,\n",
    "                ffn_dropout,\n",
    "                external_dim\n",
    "            )\n",
    "            for _ in range(num_encoder_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        embedded_sequences_batch, \n",
    "        external_features_batch\n",
    "    ):\n",
    "        batch_size = len(embedded_sequences_batch)\n",
    "        encoded_sequences_batch = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            surface_embeddings = embedded_sequences_batch[i].unsqueeze(1) \n",
    "            external_features = external_features_batch[i].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            for encoder in self.encoders:\n",
    "                surface_embeddings = encoder(surface_embeddings, external_features)\n",
    "                \n",
    "            encoded_sequences_batch.append(surface_embeddings.squeeze(1))\n",
    "        \n",
    "        return encoded_sequences_batch\n",
    "\n",
    "# Example of initializing and using these modules\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "external_dim = 3\n",
    "\n",
    "surface_encoder = SurfaceEncoder(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim, \n",
    "    attention_dropout, \n",
    "    gate_dropout, \n",
    "    ffn_dropout, \n",
    "    external_dim, \n",
    ")\n",
    "\n",
    "# Assume embedded_sequences_batch is the output of the SurfaceEmbedding module and\n",
    "# external_features is the formatted external market features batch\n",
    "encoded_sequences_batch = surface_encoder(embedded_sequences_batch, external_features_batch)\n",
    "encoded_sequences_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.3129,  1.4653, -1.2204,  ...,  1.2530, -1.6073,  0.0768],\n",
       "         [ 0.7833,  1.2334, -1.1345,  ...,  0.9228, -1.9027,  0.2909],\n",
       "         [ 1.0559,  1.0879, -1.2278,  ...,  1.2601, -1.4105,  0.1809],\n",
       "         ...,\n",
       "         [-0.2479, -0.0269, -1.6364,  ...,  1.4322, -0.4094,  1.5634],\n",
       "         [-0.4852,  0.2739, -1.4158,  ...,  1.4431, -0.0627,  1.4292],\n",
       "         [ 0.4488, -0.0256, -0.9882,  ...,  1.4369, -0.9499,  1.5133]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 1.6099,  0.6789, -0.8598,  ...,  1.1429, -1.2232,  0.2780],\n",
       "         [ 1.5252,  0.7215, -0.7823,  ...,  1.1578, -1.3290,  0.2657],\n",
       "         [ 1.3831,  0.8062, -0.7033,  ...,  1.0236, -1.6254,  0.5153],\n",
       "         ...,\n",
       "         [-2.2622,  0.6225, -0.3726,  ...,  1.3094, -0.0448,  0.7411],\n",
       "         [-1.3039,  1.6454, -0.1095,  ...,  1.4177, -0.9369, -0.0400],\n",
       "         [-2.1987,  0.4399, -0.2732,  ...,  1.1775,  0.3401,  0.9361]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 0.4655,  1.2916,  0.0053,  ...,  0.8106, -1.8208,  0.8815],\n",
       "         [ 0.7956,  1.3554, -0.2323,  ...,  0.9277, -1.7173,  0.5054],\n",
       "         [ 0.8003,  0.7536, -0.0667,  ...,  1.1793, -1.8821,  0.7242],\n",
       "         ...,\n",
       "         [-2.1999, -0.1294, -0.4774,  ...,  1.3975,  0.7147,  0.6180],\n",
       "         [-1.8627, -0.1811, -0.3648,  ...,  1.4949,  0.4564,  1.1833],\n",
       "         [-2.1973,  0.2117, -0.2396,  ...,  1.4422,  0.3758,  0.7847]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 1.0965,  1.6407,  0.0585,  ...,  0.6868, -1.2844, -1.2153],\n",
       "         [ 1.4626,  1.3596,  0.4783,  ...,  0.1599, -1.5759, -0.8715],\n",
       "         [ 1.1596,  1.5723, -0.2506,  ...,  0.9093, -0.9427, -1.1021],\n",
       "         ...,\n",
       "         [-0.7905,  1.2198,  0.2043,  ...,  1.0828,  0.4998,  0.3213],\n",
       "         [-1.1353,  1.2061,  0.0819,  ...,  1.0894,  0.3485,  0.8114],\n",
       "         [-0.8122,  1.2359,  0.6185,  ...,  0.9655, -0.4713,  0.3421]],\n",
       "        grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IvySPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.7148, 0.4146, 0.4608, 0.4135, 0.6143, 0.5572, 0.6718, 0.3357, 0.4871,\n",
       "         0.4813, 0.5920, 0.4468, 0.5046, 0.5533, 0.5604, 0.4832, 0.6962, 0.3562,\n",
       "         0.2858, 0.5461, 0.7195, 0.5420, 0.5661, 0.4552, 0.4555, 0.4688, 0.3553,\n",
       "         0.6810, 0.6298, 0.6140, 0.4203, 0.5709, 0.4766, 0.4945, 0.5639, 0.7052,\n",
       "         0.4569, 0.4950, 0.4856, 0.5100, 0.7992, 0.5749, 0.6080, 0.4550, 0.3951,\n",
       "         0.3539, 0.6736, 0.4504, 0.5283, 0.5641, 0.5780, 0.5578, 0.6364, 0.5014,\n",
       "         0.4514, 0.6482, 0.5365, 0.4957, 0.6052, 0.3540, 0.5467, 0.6059, 0.3948,\n",
       "         0.4564, 0.6553, 0.6239, 0.3600, 0.5871, 0.5362, 0.5015, 0.4932, 0.5445,\n",
       "         0.5707, 0.4843, 0.6191, 0.9255, 0.5064, 0.5571, 0.6231, 0.7034, 0.4300,\n",
       "         0.4782, 0.5911, 0.4556, 0.4863, 0.6253, 0.5715, 0.5712, 0.6181, 0.6538,\n",
       "         0.7799, 0.4797, 0.4087, 0.4617, 0.5657, 0.6397, 0.4104, 0.6409, 0.4158,\n",
       "         0.5921, 0.5534, 0.4717, 0.6326, 0.8433, 0.4839, 0.5885, 0.4143, 0.6252,\n",
       "         0.4918, 0.4806, 0.3021, 0.5231, 0.5643, 0.6394, 0.4789, 0.4444, 0.8365,\n",
       "         0.6202, 0.4951, 0.5429, 0.4652, 0.5036, 0.3912, 0.7442, 0.5574, 0.5028,\n",
       "         0.4690, 0.6566, 0.4772, 0.4859, 0.4407, 0.6917, 0.3928, 0.3693, 0.3554,\n",
       "         0.5435, 0.7439, 0.5705, 0.7135, 0.5358, 0.5113, 0.6002, 0.4740, 0.4636,\n",
       "         0.7025, 0.5465, 0.5037, 0.6124, 0.6580, 0.5113, 0.3870, 0.4992, 0.6538,\n",
       "         0.5797, 0.5775, 0.3193, 0.4816, 0.4998, 0.6146, 0.6177, 0.5419, 0.4474,\n",
       "         0.4942, 0.5448, 0.8376, 0.7948, 0.5604, 0.7932, 0.6225, 0.7625, 0.7429,\n",
       "         0.6011, 0.5866, 0.8853, 1.1354, 0.7341, 0.7672, 0.7817, 0.6009, 0.9877,\n",
       "         0.6088, 0.6361, 0.7316, 1.0230, 0.7908, 0.6762, 0.6494, 0.6961, 1.0540,\n",
       "         0.6016, 0.8787, 0.5880, 0.8104, 1.0360, 0.7299, 0.6621, 0.9563, 0.9214,\n",
       "         0.8319, 0.9390, 0.7757, 0.3612, 0.3154, 0.4687, 0.6648, 0.3625, 0.5063,\n",
       "         0.2722, 0.5070, 0.3645, 0.3938, 0.3370, 0.5307, 0.3671, 0.5124, 0.7310,\n",
       "         0.4659, 0.5040, 0.2113, 0.2925, 0.5032, 0.3320, 0.4812, 0.4343, 0.3358,\n",
       "         0.7620, 0.4078, 0.4337, 0.3156, 0.3203, 0.5647, 0.5232, 0.5442, 0.2934,\n",
       "         0.2185, 0.4832, 0.4170, 0.8936, 0.6410, 0.2540, 0.3771, 0.3994, 0.4618,\n",
       "         0.3892, 0.6434, 0.4299, 0.4446, 0.4652, 0.3497, 0.4567, 0.4189, 0.6294,\n",
       "         0.4244, 0.2355, 0.4565, 0.5316, 0.2396, 0.4735, 0.4668, 0.1702, 0.7736,\n",
       "         0.2862, 0.2267, 0.4451, 0.4183, 0.4593, 0.6009, 0.3570, 0.4443, 0.3453,\n",
       "         0.5572, 0.4051, 0.6288, 0.4848, 0.5763, 0.3523, 0.3713, 0.3197, 0.5028,\n",
       "         0.3737, 0.4672, 0.4083, 0.5391, 0.4331, 0.5115, 0.4522, 0.4321, 0.2865,\n",
       "         0.5388, 0.3619, 0.4277, 0.3590, 0.4735, 0.4030, 0.3652, 0.5013, 0.1952,\n",
       "         0.4007, 0.2127, 0.7369, 0.4822, 0.5477, 0.4129, 0.3229, 0.6351, 0.3782,\n",
       "         0.7353, 0.7023, 0.4737, 0.2904, 0.6563, 0.5995, 0.4674, 0.4683, 0.3695,\n",
       "         0.3595, 0.5278, 0.3512, 0.4840, 0.6854, 0.3442, 0.4136, 0.4211, 0.3594,\n",
       "         0.6687, 0.4013, 0.6284, 0.4931, 0.3699, 0.4661, 0.7037, 0.2019, 0.4192,\n",
       "         0.4641, 0.5905, 0.4850, 0.2909, 0.4074, 0.4169, 0.3166, 0.3421, 0.4555,\n",
       "         0.3683, 0.4721, 0.4745, 0.4099, 0.4523, 0.7396, 0.7694, 0.6009, 0.8342,\n",
       "         0.6119, 0.7116, 0.9978, 0.9692, 0.9405, 1.0832, 1.0940, 0.6256, 1.1280,\n",
       "         0.8522, 0.8452, 0.9844, 0.8937, 1.0151, 1.0534, 0.9361, 0.6600, 0.3499,\n",
       "         0.4691, 0.9790, 0.9285, 0.9274, 0.5969, 1.0046, 0.9007, 1.0573, 0.5590,\n",
       "         0.8255, 0.9840, 0.7884, 0.4845, 0.8078, 0.5741, 0.5725, 0.6570, 0.6544,\n",
       "         0.7015, 0.7156, 0.6800, 0.7887, 0.9104, 0.8716, 0.7385, 0.6787, 0.7453,\n",
       "         0.7060, 0.9425, 0.6067, 0.5782, 0.6071, 0.6446, 0.8562, 0.5764, 0.5960,\n",
       "         0.6119, 0.6137, 0.7450, 0.4859, 0.6005, 0.8624, 0.4702, 0.5582, 0.6239,\n",
       "         0.8567, 0.9190, 0.5636, 1.0117, 0.8264, 0.6103, 0.9602, 0.8413, 0.7136,\n",
       "         0.5093, 0.4583, 0.7227, 0.9524, 0.6763, 0.5095, 0.6739, 0.7125, 0.7109,\n",
       "         0.7656, 0.5589, 0.5774, 0.6613, 0.6173, 0.6448, 0.7432, 0.9286, 0.7955,\n",
       "         0.8328, 0.8511, 0.7709, 0.5880, 0.5924, 0.8592, 0.4912, 0.5848, 0.6725,\n",
       "         0.7953, 0.6508, 0.6068, 0.8960, 0.6363, 0.8221, 0.8110, 0.5687, 0.6569,\n",
       "         0.6174], grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.7100, 0.3280, 0.4014, 0.6493, 0.6462, 0.6776, 0.2050, 0.8182, 0.6204,\n",
       "         0.3922, 0.5054, 0.7442, 0.6613, 0.8686, 0.3823, 0.5691, 0.7597, 0.3993,\n",
       "         0.6622, 0.6456, 0.4430, 0.5401, 0.2334, 0.4334, 0.5071, 0.6220, 0.8015,\n",
       "         0.4882, 0.7204, 0.3276, 0.4337, 0.5309, 0.5287, 0.7954, 0.7580, 0.3529,\n",
       "         0.4117, 0.4432, 0.3932, 0.4718, 0.6740, 0.3671, 0.4070, 0.7394, 0.3200,\n",
       "         0.5919, 0.8058, 0.7208, 0.5960, 0.4026, 0.7451, 0.7380, 0.4106, 0.5051,\n",
       "         0.5429, 0.4389, 0.6108, 0.8683, 0.3827, 0.4768, 0.7267, 0.6901, 0.6890,\n",
       "         0.2954, 0.5908, 0.5711, 0.3058, 0.7512, 0.3355, 0.4847, 0.5140, 0.6928,\n",
       "         0.4552, 0.6975, 0.3425, 0.2754, 0.4243, 0.3528, 0.7389, 0.4457, 0.6856,\n",
       "         0.3597, 0.3870, 0.4354, 0.6232, 0.6841, 0.2368, 0.8494, 0.3528, 0.6533,\n",
       "         0.7021, 0.8231, 0.3048, 0.7206, 0.4250, 0.8202, 0.8109, 0.9184, 0.3354,\n",
       "         0.7368, 0.3486, 0.4382, 0.7620, 0.2424, 0.7883, 0.7996, 0.8354, 0.8087,\n",
       "         0.3981, 0.6175, 0.4153, 0.8587, 0.7969, 0.5317, 0.3953, 0.7524, 0.3855,\n",
       "         0.3879, 0.5460, 0.4726, 0.5912, 0.4156, 0.3871, 0.7305, 0.4450, 0.3864,\n",
       "         0.7816, 0.7161, 0.5106, 0.3824, 0.3901, 0.2704, 0.4010, 0.3591, 0.2379,\n",
       "         0.6422, 0.5766, 0.4913, 0.6071, 0.6841, 0.7831, 0.8155, 0.3920, 0.3188,\n",
       "         0.3893, 0.6850, 0.3546, 0.7410, 0.4675, 0.7454, 0.5505, 0.4731, 0.4287,\n",
       "         0.5770, 0.4034, 0.3506, 0.6643, 0.3595, 0.7301, 0.3679, 0.8380, 0.7493,\n",
       "         0.5198, 0.5083, 0.8107, 0.6474, 0.3183, 0.5089, 0.3821, 0.4115, 0.5073,\n",
       "         0.7210, 0.4383, 0.4806, 0.4289, 0.3326, 0.9114, 0.9393, 1.0812, 1.0359,\n",
       "         1.0526, 0.9563, 0.9155, 0.8140, 0.9284, 1.1105, 1.0087, 1.0501, 1.0631,\n",
       "         1.0437, 1.1136, 0.9071, 0.9402, 0.9141, 1.0095, 0.9075, 1.0815, 0.9265,\n",
       "         1.0386, 1.0689, 0.9768, 0.9077, 0.9492, 0.9890, 0.8573, 1.0259, 0.9406,\n",
       "         0.9482, 1.0694, 1.0968, 0.9157, 0.8509, 0.9975, 0.9667, 1.1176, 1.0753,\n",
       "         1.0053, 0.8525, 1.0890, 1.0296, 0.8574, 1.0166, 1.0116, 1.0157, 1.0439,\n",
       "         1.0035, 0.9021, 0.9432, 0.7832, 0.7943, 0.9194, 0.9235, 0.9817, 1.0581,\n",
       "         1.0561, 1.0158, 0.9782, 0.9539, 0.8397, 0.9922, 1.0890, 0.9117, 1.0101,\n",
       "         1.1017, 1.0658, 1.1227, 1.1361, 1.1136, 1.1086, 0.8967, 1.0729, 0.9580,\n",
       "         0.9783, 0.8641, 1.0470, 1.0465, 0.8059, 0.7565, 0.4912, 0.1641, 0.3826,\n",
       "         0.4738, 0.2102, 0.4901, 0.4085, 0.4933, 0.7226, 0.5341, 0.4111, 0.3821,\n",
       "         0.4624, 0.3614, 0.7300, 0.8166, 0.5301, 0.4192, 0.3546, 0.4168, 0.2449,\n",
       "         0.3781, 0.3711, 0.7677, 0.5023, 0.4060, 0.7258, 0.7207, 0.4946, 0.3582,\n",
       "         0.8320, 0.3424, 0.6865, 0.6766, 0.7523, 0.4708, 0.4295, 0.7661, 0.4651,\n",
       "         0.3916, 0.7604, 0.7076, 0.5415, 0.7939, 0.7239, 0.3129, 0.7236, 0.5464,\n",
       "         0.3708, 0.4929, 0.4100, 0.3900, 0.5049, 0.7856, 0.7505, 0.7804, 0.7160,\n",
       "         0.3867, 0.7033, 0.8491, 0.1787, 0.7916, 0.3729, 0.7841, 0.4422, 0.4779,\n",
       "         0.7827, 0.7955, 0.5682, 0.7505, 0.7564, 0.8175, 0.3144, 0.4622, 0.7732,\n",
       "         0.3972, 0.7138, 0.5326, 0.7507, 0.5035, 0.4035, 0.8643, 0.9493, 0.8052,\n",
       "         0.8199, 0.3657, 0.4683, 0.6723, 0.7301, 0.9251, 0.6872, 0.7876, 0.4926,\n",
       "         0.7910, 0.5225, 0.8758, 0.5101, 0.7237, 0.5668, 0.5268, 0.5668, 0.8006,\n",
       "         0.2741, 0.3349, 0.8126, 0.7933, 0.5267, 0.3111, 0.5615, 0.8317, 0.7588,\n",
       "         0.3189, 0.4688, 0.6846, 0.2566, 0.8378, 0.5888, 0.6781, 0.3904, 0.7988,\n",
       "         0.7922, 0.8203, 0.9544, 0.8302, 0.7989, 0.3955, 0.6079, 0.3221, 0.7491,\n",
       "         0.7884, 0.8427, 0.5153, 0.3884, 0.6995, 0.7325, 0.4506, 0.4768, 0.7749,\n",
       "         0.4341, 0.7350, 0.3138, 0.8104, 0.8708, 0.4723, 0.7691, 0.8188, 0.7554,\n",
       "         0.8207, 0.5788, 0.8360, 0.8250, 0.7820, 0.8241, 0.4476, 0.5158, 0.8234,\n",
       "         0.7473, 0.4150, 0.5473, 1.0270, 1.1678, 1.1802, 1.1817, 0.9357, 1.3146,\n",
       "         1.2259, 1.0599, 1.0010, 1.2003, 0.9395, 1.1372, 0.9510, 1.1397, 1.2735,\n",
       "         1.0600, 0.9132, 0.9901, 1.1675, 1.1142, 1.0651, 1.1073, 1.0278, 0.9305,\n",
       "         1.1980, 1.1930, 1.2920, 1.0295, 1.0269, 0.9061, 0.9251, 1.0773, 1.0518,\n",
       "         1.0596, 0.4941, 0.7316, 0.8392, 0.7021, 0.4913, 0.5264, 0.5688, 0.3695,\n",
       "         0.7499, 0.5893, 1.0277, 0.6636, 0.8396, 1.1214, 0.4757, 0.6509, 1.1485,\n",
       "         1.0353, 0.6207, 0.7944, 0.7875, 0.8819, 0.6348, 1.2797, 0.7563, 0.7124,\n",
       "         1.0088, 0.6294, 0.9646, 0.6358, 0.5465, 0.5279, 0.7599, 0.8709, 0.6527,\n",
       "         0.8320, 0.6843, 0.7346, 0.7961, 0.7158, 0.6847, 0.7099, 0.6633, 0.5071,\n",
       "         1.0541, 0.6703, 0.8333, 0.5308, 0.6185, 0.7034, 0.7327, 0.7183, 0.8446,\n",
       "         0.5706, 0.7198, 0.7322, 0.6409, 0.7436, 0.6916, 0.5751, 0.9549, 1.3011,\n",
       "         0.7378, 0.9977, 1.2924, 1.2557, 0.6070, 0.7548, 0.7012, 0.7047, 0.7306,\n",
       "         0.4634, 0.6192, 0.7162, 0.9909, 1.0833, 1.1448, 0.8324, 0.5631, 1.0757,\n",
       "         0.4557, 0.8441, 0.6703, 0.5446, 0.9458, 0.7722, 0.7394, 0.8568, 0.9197,\n",
       "         0.7449, 1.0256, 0.5172, 0.4449, 0.5037, 0.5640, 1.0391, 1.0545, 0.7160,\n",
       "         0.6579, 0.5983, 0.6395, 0.8411], grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.7655, 0.7039, 0.5231, 0.7118, 0.8866, 0.6719, 0.5509, 0.5526, 0.5259,\n",
       "         0.7415, 0.5240, 0.9169, 0.7722, 0.6697, 0.7497, 0.5466, 0.7879, 0.5051,\n",
       "         0.6349, 0.4965, 0.5381, 0.6406, 0.4474, 0.5543, 0.7181, 0.6559, 0.9253,\n",
       "         0.5875, 0.6516, 0.6207, 0.6062, 0.7728, 0.5822, 0.6545, 0.6165, 0.4172,\n",
       "         0.4990, 0.6858, 0.4497, 0.8465, 0.7804, 0.6420, 0.7669, 0.6588, 0.7101,\n",
       "         0.4755, 0.6919, 0.5823, 0.4721, 0.6525, 0.5442, 0.7490, 0.6673, 0.6180,\n",
       "         1.0023, 0.4782, 0.7247, 0.7624, 0.6439, 0.5377, 0.8512, 0.5072, 0.6506,\n",
       "         0.6350, 0.5981, 0.6205, 0.5608, 0.7363, 0.6650, 0.4764, 0.5267, 0.5707,\n",
       "         0.4980, 0.7225, 0.7973, 0.7331, 0.5752, 0.4859, 0.6221, 0.6486, 0.5312,\n",
       "         0.5532, 0.6556, 0.6975, 0.8279, 0.8849, 0.6338, 0.7474, 0.5052, 0.7329,\n",
       "         0.7145, 0.6829, 0.7815, 0.7617, 0.5592, 0.7489, 0.6501, 0.5034, 0.7311,\n",
       "         0.4847, 0.7067, 0.6020, 0.7324, 0.8199, 0.7405, 0.5993, 0.7582, 0.8273,\n",
       "         0.5361, 0.8870, 0.7500, 0.7880, 0.6259, 0.5532, 0.4058, 0.8681, 0.3961,\n",
       "         0.7868, 0.5890, 0.6111, 0.8289, 0.5058, 0.5660, 0.5199, 0.4522, 0.7478,\n",
       "         0.7514, 0.5182, 0.6288, 0.6908, 0.6966, 0.7601, 0.4268, 0.6695, 0.5601,\n",
       "         0.7288, 0.6950, 0.7267, 0.9732, 0.6401, 0.6091, 0.6717, 0.6391, 0.5588,\n",
       "         0.5004, 0.7581, 0.4798, 0.7193, 0.5884, 0.6010, 0.7444, 0.5530, 0.5230,\n",
       "         0.8489, 0.5702, 0.7145, 0.5040, 0.5009, 0.5077, 0.7742, 0.8713, 0.7799,\n",
       "         0.6737, 0.4268, 0.4743, 0.6942, 0.6795, 0.4799, 0.5088, 0.7603, 0.4771,\n",
       "         0.7748, 0.7406, 0.6281, 0.5260, 0.6162, 1.2010, 1.0309, 0.8633, 0.9618,\n",
       "         0.9974, 1.2057, 1.0882, 0.8934, 1.0110, 1.0956, 0.9957, 1.0478, 1.1091,\n",
       "         1.0610, 1.1375, 1.1901, 0.9912, 0.6339, 0.5052, 0.2809, 0.7214, 0.5429,\n",
       "         0.8871, 0.6061, 0.5727, 0.4622, 0.6266, 0.8240, 0.6956, 0.5386, 0.5211,\n",
       "         0.4791, 0.5557, 0.7231, 0.9400, 0.4020, 0.8800, 0.6747, 0.5728, 0.6626,\n",
       "         0.6326, 0.6467, 0.4862, 0.5577, 0.4307, 0.4777, 0.7839, 0.8005, 0.5710,\n",
       "         0.5536, 0.6856, 0.4027, 0.5357, 0.5741, 0.9029, 0.4239, 0.5114, 0.5102,\n",
       "         0.7928, 0.6425, 0.6415, 0.6345, 0.3812, 0.5077, 0.6309, 0.5204, 0.7092,\n",
       "         0.7416, 0.5210, 0.5393, 0.6410, 0.3882, 0.4486, 0.6162, 0.3910, 0.4952,\n",
       "         0.6793, 0.6709, 0.5506, 0.4293, 0.6462, 0.4717, 0.5454, 0.5587, 0.7501,\n",
       "         0.5748, 0.6038, 0.6479, 0.3913, 0.5456, 0.7656, 0.5571, 0.6042, 0.5475,\n",
       "         0.5006, 0.5811, 0.6350, 0.7338, 0.6429, 0.7741, 0.7035, 0.5198, 0.7371,\n",
       "         0.5856, 0.6738, 0.5559, 0.7373, 0.4715, 0.5862, 0.8185, 0.6532, 0.7368,\n",
       "         0.5096, 0.3406, 0.7939, 0.8555, 0.6919, 0.7552, 0.7414, 0.5199, 0.6695,\n",
       "         0.6204, 0.9045, 0.9572, 0.9829, 1.0057, 0.9864, 0.9068, 1.0073, 0.9876,\n",
       "         0.8947, 0.9604, 0.9988, 1.0252, 0.9947, 1.0777, 0.9451, 0.9888, 1.0794,\n",
       "         1.1645, 1.1886, 0.9996, 0.9469, 1.0192, 1.0162, 0.8775, 1.0491, 0.9227,\n",
       "         0.7814, 0.8081, 0.8346, 0.8027, 0.8841, 0.7196, 0.8378, 0.9113, 0.7984,\n",
       "         0.7562, 0.7827, 0.9004, 0.7975, 0.8336, 0.9573, 0.8408, 0.7629, 0.8397,\n",
       "         0.8573, 0.7687, 0.8892, 0.8687, 0.9177, 0.9992, 0.8963, 0.9699, 0.8317,\n",
       "         0.8045, 0.8585, 0.5673, 0.8102, 0.9049, 0.8245, 0.7740, 0.8425, 0.9201,\n",
       "         0.7516, 0.7502, 0.8604, 0.8790, 0.8437, 0.8651, 0.6614, 0.8572, 0.6833,\n",
       "         0.8777, 0.7965, 0.9132, 0.9306, 0.7424, 0.9271, 0.7776, 0.7646, 0.7627,\n",
       "         0.8873, 0.8876, 0.8236, 0.9287, 0.9248, 0.8315, 0.9000, 0.8118, 0.8530,\n",
       "         0.8880, 0.6656, 0.8015, 0.9327, 0.9142, 0.8609, 0.7310, 0.7703, 0.8416,\n",
       "         0.8875, 0.8971, 0.6266, 0.6298, 0.8540, 0.8200, 0.7570, 0.8536, 0.7764,\n",
       "         0.7903, 0.8381, 0.7281, 0.8687, 0.7200, 0.9165, 0.8390, 0.8148, 0.6583,\n",
       "         0.8033, 0.7175], grad_fn=<SqueezeBackward1>),\n",
       " tensor([1.0032, 0.6789, 0.7232, 0.4111, 0.3843, 0.9424, 0.7265, 0.7959, 0.7522,\n",
       "         0.9962, 0.5929, 0.4598, 0.5991, 0.7804, 0.4050, 1.0177, 0.9990, 0.9057,\n",
       "         0.8382, 0.6845, 0.3657, 0.9168, 0.5121, 1.0077, 0.5756, 1.0122, 0.5004,\n",
       "         0.9708, 0.5406, 0.8626, 0.7863, 0.3369, 0.8096, 0.7199, 0.9340, 0.6348,\n",
       "         0.3205, 1.0483, 0.9425, 1.1039, 0.7242, 0.4747, 0.7533, 1.0173, 0.8900,\n",
       "         0.5627, 0.9970, 0.2774, 0.9989, 0.7563, 0.8328, 0.9120, 0.8813, 0.7896,\n",
       "         0.5095, 0.7709, 0.9167, 0.7989, 0.6527, 0.8858, 1.0373, 0.3805, 0.6969,\n",
       "         0.7873, 0.7943, 0.5199, 0.4341, 0.7300, 0.9030, 0.8660, 0.8419, 0.8347,\n",
       "         0.4818, 0.9723, 0.9442, 0.4599, 0.8237, 0.3978, 0.6039, 0.6268, 0.9037,\n",
       "         0.7755, 0.3628, 0.8795, 0.7506, 0.9588, 0.7231, 0.6169, 0.6705, 0.9989,\n",
       "         0.8107, 0.8348, 0.7982, 0.9385, 0.7296, 0.7112, 1.0499, 0.5419, 0.2824,\n",
       "         0.9642, 0.7419, 0.8622, 0.8356, 1.0530, 0.6312, 0.8081, 0.9664, 0.6465,\n",
       "         0.4808, 0.8962, 0.8450, 1.1175, 0.8814, 0.6033, 0.9597, 0.9076, 0.8490,\n",
       "         0.8231, 0.7365, 0.6838, 0.7641, 0.8421, 0.4850, 1.0470, 0.8082, 1.0725,\n",
       "         0.9266, 0.4619, 0.9242, 0.7976, 1.0793, 0.7484, 0.9603, 0.6839, 0.8868,\n",
       "         0.9260, 0.7080, 0.7739, 0.7839, 0.7657, 0.9307, 0.7402, 0.9203, 0.7900,\n",
       "         0.8025, 0.7658, 0.9934, 0.8465, 0.9654, 0.7400, 0.7787, 0.8671, 0.7825,\n",
       "         1.0556, 0.7464, 0.8788, 0.6334, 0.7729, 0.9756, 1.0451, 0.4327, 0.9665,\n",
       "         0.8082, 0.9840, 0.7973, 0.7905, 0.7529, 1.0121, 0.4234, 0.9412, 1.0376,\n",
       "         0.8523, 0.7915, 0.9469, 0.8982, 0.6475, 0.9459, 0.8970, 0.9496, 0.9599,\n",
       "         0.7902, 0.7795, 0.5444, 0.8956, 0.4434, 0.8090, 0.5577, 0.9841, 0.4921,\n",
       "         0.5261, 1.0208, 0.3852, 0.3606, 0.6160, 0.4136, 0.9260, 0.6724, 1.0039,\n",
       "         0.6352, 1.0307, 0.6198, 0.5630, 1.0318, 0.6758, 0.3344, 0.8058, 0.3943,\n",
       "         0.3349, 0.3518, 0.6407, 0.8699, 0.5529, 1.0359, 0.4436, 0.9196, 0.7156,\n",
       "         0.6463, 0.7303, 0.7793, 0.7970, 0.8489, 0.5771, 0.9167, 0.4083, 0.5342,\n",
       "         0.5485, 1.0984, 1.1038, 0.2642, 0.5081, 0.6406, 0.7392, 0.4988, 0.5877,\n",
       "         0.9752, 0.6536, 0.8057, 0.9290, 0.6710, 1.0090, 0.5874, 1.0476, 0.8869,\n",
       "         0.5684, 0.9955, 0.5208, 0.5611, 1.0676, 0.8519, 0.5714, 0.8638, 1.0343,\n",
       "         0.7982, 0.6249, 0.7599, 0.8256, 0.4639, 0.4638, 0.5468, 0.9856, 0.4224,\n",
       "         0.7852, 0.8904, 0.9643, 0.7326, 0.7559, 0.5111, 0.6723, 0.3787, 0.5897,\n",
       "         0.7086, 0.7264, 0.4352, 0.5072, 0.8526, 1.0165, 1.0086, 0.2331, 0.9707,\n",
       "         0.5123, 0.9689, 0.6989, 0.6750, 0.6231, 0.5830, 0.8509, 0.5868, 0.3956,\n",
       "         0.4920, 0.7426, 0.8026, 0.4670, 0.5587, 0.4886, 0.7924, 0.4630, 0.6099,\n",
       "         0.4986, 0.5164, 0.4156, 0.7855, 0.8403, 0.3263, 0.4751, 0.4137, 0.5302,\n",
       "         0.7952, 0.4360, 0.5282, 0.5988, 0.7611, 0.6383, 0.6517, 0.4094, 0.5412,\n",
       "         0.2395, 0.9139, 0.7788, 0.6023, 0.5269, 0.4228, 0.5129, 1.0094, 0.5587,\n",
       "         0.5554, 0.3872, 0.5080, 0.8248, 0.4493, 0.6122, 0.4912, 0.6362, 0.8927,\n",
       "         0.4799, 0.5985, 0.5368, 0.7608, 0.6007, 0.9413, 0.4564, 0.6553, 1.0245,\n",
       "         0.8256, 0.4460, 0.4529, 0.5432, 0.7274, 0.4725, 0.5957, 0.3028, 0.4570,\n",
       "         0.4176, 0.4030, 0.6526, 0.7389, 0.4664, 0.4325, 0.4426, 0.5814, 0.3952,\n",
       "         0.5198, 0.4164, 0.5089, 0.6248, 0.5664, 0.6704, 0.5203, 0.6648, 0.3214,\n",
       "         0.1600, 0.4012, 0.4951, 0.5068, 0.5129, 0.5695, 0.5495, 0.8362, 0.5572,\n",
       "         0.4840, 0.4095, 0.4654, 0.5203, 0.6918, 0.2793, 0.4834, 0.6243, 0.6685,\n",
       "         0.5860, 0.3265, 0.8239, 0.4743, 0.7206, 0.8597, 0.7367, 0.5115, 0.7403,\n",
       "         0.5903, 0.5546, 0.4713, 0.9043, 0.7951, 0.4469, 0.6781, 0.4981, 0.5067,\n",
       "         0.3994, 0.4805, 0.4669, 0.6171, 0.4906, 0.3452, 0.5202, 0.5713, 0.5251,\n",
       "         0.7459, 0.7844, 0.4401, 0.6286, 0.7357, 0.5456, 0.5880, 0.8054, 0.6823,\n",
       "         0.4872, 0.7038, 0.4486, 0.6408, 0.8115, 0.8009, 0.6317, 0.4581, 0.4896,\n",
       "         0.8553, 0.7996, 0.4787, 0.8241, 0.8541, 0.5685, 0.4970, 0.6431, 0.7332,\n",
       "         0.7675, 0.4224, 0.6184, 0.7013, 0.4644, 0.7010, 0.5653, 0.5933, 0.5134,\n",
       "         0.5100, 0.7878, 0.5349, 0.8225, 0.7091, 0.6992, 0.6443, 0.4060, 0.3846,\n",
       "         0.5665, 0.8831, 1.0638, 1.0518, 1.0030, 0.9664, 1.0737, 1.0109, 0.9967,\n",
       "         1.0151, 0.9906, 0.9570, 1.0142, 1.1994, 0.8923, 0.7611, 1.0150, 0.9773,\n",
       "         1.0022, 1.0296, 0.9543, 1.0535, 0.9602, 0.9525, 0.9706, 1.0402, 1.1007,\n",
       "         1.1708, 1.0879, 0.9400, 1.1009, 1.1218, 1.1403, 1.0531, 1.0304, 1.1509],\n",
       "        grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class IvySPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim\n",
    "    ):\n",
    "        super(IvySPT, self).__init__()\n",
    "        self.surface_embedding = SurfaceEmbedding(d_embedding)\n",
    "        self.surface_encoder = SurfaceEncoder(\n",
    "            d_embedding, \n",
    "            num_encoder_blocks,\n",
    "            n_heads, \n",
    "            ffn_hidden_dim,\n",
    "            attention_dropout, \n",
    "            gate_dropout,\n",
    "            ffn_dropout,\n",
    "            external_dim\n",
    "        )\n",
    "        self.final_layer = nn.Linear(d_embedding, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Obtain the embedded sequences and external features from the SurfaceEmbedding module\n",
    "        embedded_sequences_batch, external_features_batch = self.surface_embedding(batch)\n",
    "\n",
    "        # Encode the sequences using the SurfaceEncoder module\n",
    "        encoded_sequences_batch = self.surface_encoder(embedded_sequences_batch, external_features_batch)\n",
    "\n",
    "        # List to hold the implied volatility estimates for each query point in the batch\n",
    "        iv_estimates_batch = []\n",
    "\n",
    "        for i in range(len(encoded_sequences_batch)):\n",
    "            # Extract the encoded sequence\n",
    "            encoded_sequence = encoded_sequences_batch[i]\n",
    "\n",
    "            # Determine the number of query points for this sequence\n",
    "            num_query_points = len(batch['Query Points']['Log Moneyness'][i])\n",
    "\n",
    "            # Extract the encoded query points (last num_query_points elements in the sequence)\n",
    "            encoded_query_points = encoded_sequence[-num_query_points:]\n",
    "\n",
    "            # Estimate the implied volatility for each query point using the fully connected layer\n",
    "            iv_estimates = self.final_layer(encoded_query_points).squeeze(-1)\n",
    "\n",
    "            # Append the estimates to the batch list\n",
    "            iv_estimates_batch.append(iv_estimates)\n",
    "\n",
    "        return iv_estimates_batch\n",
    "\n",
    "# Example of initializing and using this module\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "external_dim = 3\n",
    "\n",
    "ivy_spt = IvySPT(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim,\n",
    "    attention_dropout, \n",
    "    gate_dropout,\n",
    "    ffn_dropout,\n",
    "    external_dim\n",
    ")\n",
    "\n",
    "# Pass the batch through the IvySPT model to get implied volatility estimates\n",
    "iv_estimates_batch = ivy_spt(batch)\n",
    "iv_estimates_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.2695, 0.2695, 0.3415, 0.3091, 0.3302, 0.2849, 0.2695, 0.3302, 0.2561,\n",
       "         0.2912, 0.3295, 0.3461, 0.2658, 0.3302, 0.3133, 0.3065, 0.3302, 0.2950,\n",
       "         0.3461, 0.2847, 0.3088, 0.2847, 0.2711, 0.2940, 0.2561, 0.2887, 0.2561,\n",
       "         0.2683, 0.2662, 0.3295, 0.2817, 0.3133, 0.3271, 0.2548, 0.3302, 0.3295,\n",
       "         0.2849, 0.3302, 0.3302, 0.2695, 0.3302, 0.2702, 0.3302, 0.3716, 0.2707,\n",
       "         0.3295, 0.3302, 0.2604, 0.3133, 0.3104, 0.2695, 0.3302, 0.2631, 0.2702,\n",
       "         0.2654, 0.3295, 0.2561, 0.3138, 0.2702, 0.3153, 0.3128, 0.2695, 0.2561,\n",
       "         0.2665, 0.3024, 0.2849, 0.3461, 0.2849, 0.2661, 0.3066, 0.2564, 0.2844,\n",
       "         0.2561, 0.2702, 0.3295, 0.2745, 0.2695, 0.3000, 0.2636, 0.3302, 0.2585,\n",
       "         0.3356, 0.2849, 0.3216, 0.2785, 0.3190, 0.3302, 0.2695, 0.3302, 0.2676,\n",
       "         0.2737, 0.2695, 0.3356, 0.2661, 0.3295, 0.3302, 0.2601, 0.2695, 0.2717,\n",
       "         0.2695, 0.2859, 0.3133, 0.2695, 0.2561, 0.3295, 0.3302, 0.2695, 0.2695,\n",
       "         0.3302, 0.3133, 0.3047, 0.2561, 0.2561, 0.2695, 0.3356, 0.3413, 0.2695,\n",
       "         0.3133, 0.2561, 0.3295, 0.2616, 0.3295, 0.2888, 0.2695, 0.3302, 0.3104,\n",
       "         0.3096, 0.2740, 0.2607, 0.3133, 0.2666, 0.2775, 0.2929, 0.3295, 0.3461,\n",
       "         0.2561, 0.2710, 0.2661, 0.2561, 0.3356, 0.2702, 0.2581, 0.3302, 0.2702,\n",
       "         0.3302, 0.3302, 0.3215, 0.2669, 0.2877, 0.2759, 0.2661, 0.2854, 0.2985,\n",
       "         0.3295, 0.2631, 0.2661, 0.2770, 0.2561, 0.2609, 0.3302, 0.2628, 0.2695,\n",
       "         0.3461, 0.2716, 0.2871, 0.2804, 0.3116, 0.3302, 0.2894, 0.4638, 0.3104,\n",
       "         0.3651, 0.3439, 0.3153, 0.3111, 0.3495, 0.2803, 0.2844, 0.3451, 0.3081,\n",
       "         0.3100, 0.3491, 0.2940, 0.3186, 0.2867, 0.2829, 0.3227, 0.4616, 0.3150,\n",
       "         0.4355, 0.4535, 0.3170, 0.3788, 0.3126, 0.4782, 0.3439, 0.3120, 0.2870,\n",
       "         0.2817, 0.3202, 0.2878, 0.3081, 0.3473, 0.2819, 0.2757, 0.3187, 0.2901,\n",
       "         0.4840, 0.2996, 0.3473, 0.3051, 0.3044, 0.4344, 0.3507, 0.4801, 0.3369,\n",
       "         0.3235, 0.2824, 0.3695, 0.3381, 0.3175, 0.3695, 0.3459, 0.3335, 0.3648,\n",
       "         0.2771, 0.3258, 0.3508, 0.3295, 0.3267, 0.2716, 0.2983, 0.3472, 0.3258,\n",
       "         0.3258, 0.3338, 0.3280, 0.2834, 0.2778, 0.3716, 0.3472, 0.3235, 0.3472,\n",
       "         0.3459, 0.2899, 0.3459, 0.3473, 0.3347, 0.3473, 0.2607, 0.3822, 0.3535,\n",
       "         0.3258, 0.3258, 0.3469, 0.3472, 0.3114, 0.3147, 0.3695, 0.3695, 0.2622,\n",
       "         0.3473, 0.3632, 0.3422, 0.3472, 0.2690, 0.2665, 0.3398, 0.3472, 0.6848,\n",
       "         0.3001, 0.3320, 0.2909, 0.3227, 0.2803, 0.3700, 0.4059, 0.4187, 0.3459,\n",
       "         0.3469, 0.3103, 0.3459, 0.2912, 0.3258, 0.3273, 0.3258, 0.4187, 0.3473,\n",
       "         0.2778, 0.3379, 0.3473, 0.3473, 0.2693, 0.6979, 0.3822, 0.3242, 0.3397,\n",
       "         0.3146, 0.3507, 0.2703, 0.3390, 0.2887, 0.3510, 0.4187, 0.2792, 0.5274,\n",
       "         0.2886, 0.2902, 0.2826, 0.3419, 0.2725, 0.2686, 0.2735, 0.3472, 0.3473,\n",
       "         0.3507, 0.3381, 0.3822, 0.2809, 0.2643, 0.3695, 0.3716, 0.2985, 0.3510,\n",
       "         0.2723, 0.3187, 0.3822, 0.2952, 0.2957, 0.3109, 0.2810, 0.5483, 0.4187,\n",
       "         0.3369, 0.3277, 0.2782, 0.3401, 0.2721, 0.3472, 0.3002, 0.3695, 0.3209,\n",
       "         0.3459, 0.2989, 0.3473, 0.5483, 0.3469, 0.3216, 0.3192, 0.3167, 0.3136,\n",
       "         0.3194, 0.3264, 0.3043, 0.3047, 0.3011, 0.3043, 0.2967, 0.3194, 0.2968,\n",
       "         0.2973, 0.3001, 0.3310, 0.3061, 0.2967, 0.3011, 0.3056, 0.3047, 0.3182,\n",
       "         0.3194, 0.3037, 0.2964, 0.3076, 0.3194, 0.2982, 0.3043, 0.2974, 0.3292,\n",
       "         0.3020, 0.3053, 0.3042, 0.3089, 0.2767, 0.3089, 0.2854, 0.2671, 0.2943,\n",
       "         0.2808, 0.2738, 0.3089, 0.2829, 0.2853, 0.2823, 0.2788, 0.3124, 0.2811,\n",
       "         0.2849, 0.2871, 0.3116, 0.3116, 0.3089, 0.2682, 0.2819, 0.3089, 0.3089,\n",
       "         0.3089, 0.3116, 0.2819, 0.3089, 0.3116, 0.2767, 0.3116, 0.2879, 0.2849,\n",
       "         0.3089, 0.2858, 0.2794, 0.2777, 0.2785, 0.2685, 0.2765, 0.2760, 0.3124,\n",
       "         0.3089, 0.3116, 0.3089, 0.2829, 0.2849, 0.3116, 0.3116, 0.2792, 0.3116,\n",
       "         0.3096, 0.3069, 0.3124, 0.3089, 0.2824, 0.2717, 0.2672, 0.2872, 0.2779,\n",
       "         0.3000, 0.2763, 0.3089, 0.3124, 0.3089, 0.2814, 0.3229, 0.3089, 0.3116,\n",
       "         0.2791, 0.2818, 0.3116, 0.2978, 0.2849, 0.2849, 0.2999, 0.2849, 0.2849,\n",
       "         0.3089]),\n",
       " tensor([0.2906, 0.3365, 0.5096, 0.2726, 0.2813, 0.2881, 0.4041, 0.2970, 0.3017,\n",
       "         0.3056, 0.3919, 0.3156, 0.2899, 0.3284, 0.4003, 0.2914, 0.3032, 0.3120,\n",
       "         0.2953, 0.2862, 0.4731, 0.2803, 0.4101, 0.3146, 0.3002, 0.2974, 0.2711,\n",
       "         0.3724, 0.2737, 0.3042, 0.4484, 0.3578, 0.3025, 0.2807, 0.3123, 0.4041,\n",
       "         0.3174, 0.3151, 0.4731, 0.3529, 0.2829, 0.2985, 0.3123, 0.2841, 0.3578,\n",
       "         0.3676, 0.2858, 0.2881, 0.2924, 0.5174, 0.2851, 0.2802, 0.4101, 0.3086,\n",
       "         0.3042, 0.4101, 0.2949, 0.2717, 0.3578, 0.3110, 0.2854, 0.3037, 0.3211,\n",
       "         0.3597, 0.2819, 0.2819, 0.4041, 0.2829, 0.5174, 0.2948, 0.4731, 0.2908,\n",
       "         0.3375, 0.2759, 0.4201, 0.4041, 0.6361, 0.2893, 0.3017, 0.3151, 0.3017,\n",
       "         0.3124, 0.3546, 0.3681, 0.2894, 0.3011, 0.4041, 0.2805, 0.3755, 0.2891,\n",
       "         0.2829, 0.2718, 0.3510, 0.2962, 0.3968, 0.3091, 0.2810, 0.2859, 0.4136,\n",
       "         0.3163, 0.3556, 0.4101, 0.2838, 0.4041, 0.2839, 0.2957, 0.3016, 0.2990,\n",
       "         0.2940, 0.2829, 0.3308, 0.2726, 0.2756, 0.3271, 0.3696, 0.2801, 0.6375,\n",
       "         0.3452, 0.2908, 0.3983, 0.3010, 0.3096, 0.4799, 0.3030, 0.3020, 0.3011,\n",
       "         0.2828, 0.3276, 0.3968, 0.4731, 0.3151, 0.4041, 0.4731, 0.4456, 0.4101,\n",
       "         0.2811, 0.2869, 0.3084, 0.3578, 0.3271, 0.2935, 0.3068, 0.3597, 0.4101,\n",
       "         0.4562, 0.2751, 0.4799, 0.2696, 0.4300, 0.2833, 0.2944, 0.4101, 0.4101,\n",
       "         0.3013, 0.3262, 0.3597, 0.2774, 0.3007, 0.3017, 0.3151, 0.2728, 0.2706,\n",
       "         0.2875, 0.3255, 0.3013, 0.3018, 0.3597, 0.2893, 0.2893, 0.3695, 0.4101,\n",
       "         0.2791, 0.3151, 0.4484, 0.3801, 0.4731, 0.2950, 0.2950, 0.2879, 0.2686,\n",
       "         0.2922, 0.2675, 0.2950, 0.2749, 0.2950, 0.2827, 0.2675, 0.2659, 0.2781,\n",
       "         0.2950, 0.2811, 0.2675, 0.2753, 0.2768, 0.2864, 0.2832, 0.2675, 0.2768,\n",
       "         0.2675, 0.2718, 0.2800, 0.2751, 0.2675, 0.2768, 0.2768, 0.2950, 0.2675,\n",
       "         0.2703, 0.2664, 0.2675, 0.2950, 0.2938, 0.2938, 0.2668, 0.2675, 0.2675,\n",
       "         0.2675, 0.2675, 0.2775, 0.2929, 0.2671, 0.2646, 0.2672, 0.2675, 0.2950,\n",
       "         0.2675, 0.2768, 0.2768, 0.2768, 0.2636, 0.2903, 0.2675, 0.2788, 0.2781,\n",
       "         0.2671, 0.2675, 0.2680, 0.2664, 0.2693, 0.2675, 0.2708, 0.2675, 0.2950,\n",
       "         0.2804, 0.2651, 0.2708, 0.2950, 0.2660, 0.2781, 0.2938, 0.2950, 0.2768,\n",
       "         0.2675, 0.2740, 0.2700, 0.2938, 0.2938, 0.3270, 0.3017, 0.5174, 0.3578,\n",
       "         0.4041, 0.5174, 0.3017, 0.3151, 0.3017, 0.3016, 0.3784, 0.5174, 0.5174,\n",
       "         0.4041, 0.3578, 0.3048, 0.3207, 0.3578, 0.4041, 0.4041, 0.3578, 0.3578,\n",
       "         0.4041, 0.3578, 0.3270, 0.3017, 0.4041, 0.3315, 0.3619, 0.3784, 0.5174,\n",
       "         0.3048, 0.3578, 0.3678, 0.3284, 0.3048, 0.4041, 0.3784, 0.3016, 0.3784,\n",
       "         0.3151, 0.3378, 0.3270, 0.3017, 0.3151, 0.3689, 0.4041, 0.3048, 0.3784,\n",
       "         0.3784, 0.5174, 0.3017, 0.3151, 0.3784, 0.3048, 0.3701, 0.3016, 0.2963,\n",
       "         0.3784, 0.3284, 0.3701, 0.5174, 0.3048, 0.4041, 0.3048, 0.3784, 0.5174,\n",
       "         0.3048, 0.3016, 0.3017, 0.3016, 0.3270, 0.3016, 0.3151, 0.5174, 0.3284,\n",
       "         0.3578, 0.3016, 0.3151, 0.3016, 0.4041, 0.3151, 0.3270, 0.3701, 0.3701,\n",
       "         0.3411, 0.5174, 0.3784, 0.3678, 0.3284, 0.3701, 0.3434, 0.3277, 0.3578,\n",
       "         0.3270, 0.3017, 0.3284, 0.3662, 0.3016, 0.3578, 0.3017, 0.3784, 0.3181,\n",
       "         0.5174, 0.5174, 0.3336, 0.3270, 0.3703, 0.4041, 0.3784, 0.3048, 0.3270,\n",
       "         0.4101, 0.3151, 0.3048, 0.4041, 0.3016, 0.3017, 0.3048, 0.3578, 0.3016,\n",
       "         0.3048, 0.3543, 0.3016, 0.3093, 0.3270, 0.5174, 0.3163, 0.3151, 0.3701,\n",
       "         0.3669, 0.3701, 0.3784, 0.3151, 0.3701, 0.3270, 0.3578, 0.3151, 0.3284,\n",
       "         0.3151, 0.3413, 0.5174, 0.3205, 0.3270, 0.4041, 0.3270, 0.3048, 0.3048,\n",
       "         0.3270, 0.3017, 0.3016, 0.3016, 0.3016, 0.3270, 0.3578, 0.3784, 0.3701,\n",
       "         0.3511, 0.4041, 0.3151, 0.3046, 0.2916, 0.3240, 0.3065, 0.3002, 0.3105,\n",
       "         0.3118, 0.3075, 0.3084, 0.2999, 0.2997, 0.2924, 0.3028, 0.2899, 0.3026,\n",
       "         0.3173, 0.3020, 0.3065, 0.3036, 0.3047, 0.2881, 0.2925, 0.3054, 0.3008,\n",
       "         0.3165, 0.2992, 0.3081, 0.2915, 0.2908, 0.3932, 0.3090, 0.2916, 0.2999,\n",
       "         0.2935, 0.7247, 0.3792, 0.4230, 0.3071, 0.3801, 0.3962, 0.3439, 0.3909,\n",
       "         0.3370, 0.3464, 0.2896, 0.3962, 0.3792, 0.2993, 0.3374, 0.4610, 0.3030,\n",
       "         0.2798, 0.3346, 0.4089, 0.3296, 0.2859, 0.4352, 0.3372, 0.2892, 0.2870,\n",
       "         0.2988, 0.3484, 0.3353, 0.3237, 0.3962, 0.3481, 0.3264, 0.2927, 0.3484,\n",
       "         0.4797, 0.3532, 0.3400, 0.3274, 0.3358, 0.3484, 0.3030, 0.3370, 0.3801,\n",
       "         0.3088, 0.3068, 0.2844, 0.3413, 0.3221, 0.3370, 0.3792, 0.2834, 0.2795,\n",
       "         0.3632, 0.3194, 0.2912, 0.3690, 0.4610, 0.3792, 0.3962, 0.2711, 0.3372,\n",
       "         0.3226, 0.3030, 0.3372, 0.3372, 0.3161, 0.3260, 0.3081, 0.3005, 0.2949,\n",
       "         0.7247, 0.4089, 0.3063, 0.2854, 0.2914, 0.3025, 0.2935, 0.4214, 0.2888,\n",
       "         0.7247, 0.3232, 0.3071, 0.3962, 0.2934, 0.3024, 0.3044, 0.3534, 0.3227,\n",
       "         0.2948, 0.2795, 0.3962, 0.3481, 0.7247, 0.3801, 0.2745, 0.2870, 0.3355,\n",
       "         0.3632, 0.3162, 0.3632, 0.3043]),\n",
       " tensor([0.2497, 0.2415, 0.3682, 0.2836, 0.2791, 0.2361, 0.3716, 0.3175, 0.3010,\n",
       "         0.2096, 0.2970, 0.2365, 0.2639, 0.2309, 0.2737, 0.2746, 0.2380, 0.2812,\n",
       "         0.2721, 0.2829, 0.3682, 0.2055, 0.3347, 0.2970, 0.2167, 0.2839, 0.2673,\n",
       "         0.2757, 0.2891, 0.2076, 0.3682, 0.2042, 0.2970, 0.2364, 0.3224, 0.3352,\n",
       "         0.3506, 0.2034, 0.3682, 0.2625, 0.2055, 0.2970, 0.2046, 0.2519, 0.2141,\n",
       "         0.3347, 0.2343, 0.3246, 0.2970, 0.3190, 0.3246, 0.2519, 0.3347, 0.2921,\n",
       "         0.2207, 0.3347, 0.2055, 0.2545, 0.2164, 0.2970, 0.2335, 0.2929, 0.3170,\n",
       "         0.2582, 0.3175, 0.2611, 0.3352, 0.2519, 0.3190, 0.2891, 0.3682, 0.2569,\n",
       "         0.3506, 0.2359, 0.2098, 0.2248, 0.2507, 0.3306, 0.2564, 0.1938, 0.2974,\n",
       "         0.3506, 0.2708, 0.3347, 0.2268, 0.2726, 0.4354, 0.2359, 0.2970, 0.2191,\n",
       "         0.2314, 0.2767, 0.2455, 0.2285, 0.3506, 0.2363, 0.2837, 0.3175, 0.2169,\n",
       "         0.2369, 0.2640, 0.3347, 0.2370, 0.2151, 0.2457, 0.2417, 0.2887, 0.2191,\n",
       "         0.2970, 0.2055, 0.2012, 0.2506, 0.3246, 0.2977, 0.3347, 0.2469, 0.3682,\n",
       "         0.2896, 0.2631, 0.3682, 0.2055, 0.2970, 0.2507, 0.2769, 0.3066, 0.2538,\n",
       "         0.2526, 0.3090, 0.3506, 0.2965, 0.2030, 0.3352, 0.3682, 0.2166, 0.3347,\n",
       "         0.2055, 0.2293, 0.2176, 0.2039, 0.2346, 0.2443, 0.2908, 0.2201, 0.3347,\n",
       "         0.2507, 0.2917, 0.2507, 0.2656, 0.2061, 0.2348, 0.2121, 0.3347, 0.3347,\n",
       "         0.2099, 0.3091, 0.2373, 0.2969, 0.2970, 0.3079, 0.1956, 0.2478, 0.2642,\n",
       "         0.3306, 0.3306, 0.2859, 0.2535, 0.2090, 0.3306, 0.3306, 0.2080, 0.3347,\n",
       "         0.2390, 0.1946, 0.3682, 0.3506, 0.3682, 0.2698, 0.3032, 0.2884, 0.2801,\n",
       "         0.3305, 0.2929, 0.2670, 0.2960, 0.2907, 0.2720, 0.2879, 0.2707, 0.2789,\n",
       "         0.2910, 0.3190, 0.2626, 0.2814, 0.2919, 0.2952, 0.2949, 0.2642, 0.3009,\n",
       "         0.2827, 0.5224, 0.4978, 0.3306, 0.2919, 0.2642, 0.2970, 0.2970, 0.3306,\n",
       "         0.5668, 0.2919, 0.3009, 0.2642, 0.2949, 0.2949, 0.2952, 0.4176, 0.2970,\n",
       "         0.3306, 0.3009, 0.2970, 0.3009, 0.2642, 0.3306, 0.2949, 0.3009, 0.2944,\n",
       "         0.2944, 0.2952, 0.2642, 0.2949, 0.2919, 0.2944, 0.2642, 0.2970, 0.2919,\n",
       "         0.2919, 0.2919, 0.2949, 0.4812, 0.2642, 0.2944, 0.2944, 0.3246, 0.2932,\n",
       "         0.3009, 0.2970, 0.2919, 0.2949, 0.2642, 0.2970, 0.2970, 0.2642, 0.2970,\n",
       "         0.2952, 0.2919, 0.2970, 0.2949, 0.5395, 0.2970, 0.2952, 0.2919, 0.2642,\n",
       "         0.3306, 0.2944, 0.4376, 0.2642, 0.3246, 0.2881, 0.3306, 0.2642, 0.2970,\n",
       "         0.3306, 0.2952, 0.2858, 0.2949, 0.2919, 0.2949, 0.2944, 0.2970, 0.2952,\n",
       "         0.3009, 0.2952, 0.3246, 0.2944, 0.2949, 0.3246, 0.3009, 0.2952, 0.2949,\n",
       "         0.2944, 0.3306, 0.2642, 0.2877, 0.2949, 0.3009, 0.2944, 0.2970, 0.2970,\n",
       "         0.2970, 0.2528, 0.2449, 0.2582, 0.2317, 0.2532, 0.2433, 0.2319, 0.2529,\n",
       "         0.2308, 0.2347, 0.2322, 0.2351, 0.2350, 0.2460, 0.2372, 0.2441, 0.2352,\n",
       "         0.2316, 0.2324, 0.2302, 0.2334, 0.2316, 0.2359, 0.2337, 0.2310, 0.2343,\n",
       "         0.2686, 0.2222, 0.2513, 0.2195, 0.2228, 0.2217, 0.2813, 0.2181, 0.2524,\n",
       "         0.2297, 0.2451, 0.2631, 0.2256, 0.2597, 0.2192, 0.2250, 0.2642, 0.2570,\n",
       "         0.2184, 0.2489, 0.2198, 0.2217, 0.2205, 0.2190, 0.2780, 0.2151, 0.2229,\n",
       "         0.2575, 0.2248, 0.2241, 0.2541, 0.2335, 0.2164, 0.2190, 0.2235, 0.2172,\n",
       "         0.2597, 0.2196, 0.2196, 0.2217, 0.2274, 0.2184, 0.2123, 0.2317, 0.2214,\n",
       "         0.2239, 0.2642, 0.2220, 0.2236, 0.2596, 0.2203, 0.2631, 0.2607, 0.2294,\n",
       "         0.2143, 0.2268, 0.2214, 0.2199, 0.2206, 0.2646, 0.2306, 0.2228, 0.2171,\n",
       "         0.2215, 0.2299, 0.2441, 0.2352, 0.2217, 0.2553, 0.2212, 0.2363, 0.2626,\n",
       "         0.2191, 0.2252, 0.2156, 0.2262, 0.2146, 0.2230, 0.2186, 0.2220, 0.2426,\n",
       "         0.2137, 0.2213, 0.2346, 0.2137, 0.2405, 0.2731, 0.2231, 0.2238, 0.2262,\n",
       "         0.2139, 0.2221]),\n",
       " tensor([0.3007, 0.2970, 0.2970, 0.5094, 0.5094, 0.3007, 0.3291, 0.3249, 0.3291,\n",
       "         0.3091, 0.3091, 0.3091, 0.2970, 0.3641, 0.3321, 0.3007, 0.3007, 0.2970,\n",
       "         0.3249, 0.3291, 0.5094, 0.2969, 0.3091, 0.3007, 0.3321, 0.3007, 0.3776,\n",
       "         0.3091, 0.3091, 0.3291, 0.2970, 0.3822, 0.2970, 0.3321, 0.2970, 0.3776,\n",
       "         0.5094, 0.3007, 0.2970, 0.3007, 0.2970, 0.3822, 0.3091, 0.3091, 0.3291,\n",
       "         0.3249, 0.2954, 0.5094, 0.3091, 0.3249, 0.3361, 0.2970, 0.2980, 0.2970,\n",
       "         0.5094, 0.2970, 0.2899, 0.3249, 0.3091, 0.3051, 0.3091, 0.3822, 0.3321,\n",
       "         0.3321, 0.3249, 0.5094, 0.3822, 0.3249, 0.2980, 0.2938, 0.3321, 0.3291,\n",
       "         0.3822, 0.2848, 0.3007, 0.5094, 0.2980, 0.3822, 0.3776, 0.3776, 0.2879,\n",
       "         0.3641, 0.5094, 0.2970, 0.3321, 0.3007, 0.3321, 0.3776, 0.3361, 0.3091,\n",
       "         0.3249, 0.3291, 0.2980, 0.3007, 0.3291, 0.3776, 0.2901, 0.3091, 0.3822,\n",
       "         0.3007, 0.3249, 0.3291, 0.2980, 0.3007, 0.2970, 0.3291, 0.3007, 0.3776,\n",
       "         0.3091, 0.2871, 0.2970, 0.2962, 0.2849, 0.3776, 0.3291, 0.2970, 0.2970,\n",
       "         0.2940, 0.3249, 0.3776, 0.3291, 0.2970, 0.3776, 0.3007, 0.3091, 0.3012,\n",
       "         0.3007, 0.3822, 0.2980, 0.3291, 0.3007, 0.3042, 0.2889, 0.3776, 0.3011,\n",
       "         0.2980, 0.3291, 0.3321, 0.2970, 0.3641, 0.2970, 0.3291, 0.2870, 0.3291,\n",
       "         0.2980, 0.2970, 0.2949, 0.2970, 0.3091, 0.3249, 0.3321, 0.2970, 0.3249,\n",
       "         0.3007, 0.3091, 0.3051, 0.3822, 0.2854, 0.2985, 0.3007, 0.3822, 0.2864,\n",
       "         0.3321, 0.3091, 0.3291, 0.3258, 0.3641, 0.3361, 0.5094, 0.3091, 0.3091,\n",
       "         0.3321, 0.3249, 0.2942, 0.3026, 0.3249, 0.2851, 0.3321, 0.3091, 0.2963,\n",
       "         0.2980, 0.2970, 0.3091, 0.3249, 0.5094, 0.2970, 0.3091, 0.3091, 0.3822,\n",
       "         0.4317, 0.3341, 0.4317, 0.3747, 0.3575, 0.3747, 0.4519, 0.3286, 0.3037,\n",
       "         0.5115, 0.4002, 0.3170, 0.4225, 0.3138, 0.5548, 0.3747, 0.5134, 0.4266,\n",
       "         0.4317, 0.4168, 0.4010, 0.3711, 0.4914, 0.4115, 0.3747, 0.3446, 0.5253,\n",
       "         0.3249, 0.3701, 0.3473, 0.3158, 0.3105, 0.4225, 0.2945, 0.4317, 0.3849,\n",
       "         0.4932, 0.3867, 0.3215, 0.3468, 0.6381, 0.4035, 0.3741, 0.4087, 0.4786,\n",
       "         0.2890, 0.4229, 0.3252, 0.3003, 0.4786, 0.2891, 0.5854, 0.3420, 0.3071,\n",
       "         0.4184, 0.4321, 0.3560, 0.3983, 0.2990, 0.3384, 0.3648, 0.3555, 0.3694,\n",
       "         0.3456, 0.4482, 0.3242, 0.3059, 0.3747, 0.4029, 0.4087, 0.3045, 0.3997,\n",
       "         0.5652, 0.2979, 0.3568, 0.3278, 0.3138, 0.4317, 0.3918, 0.4087, 0.4406,\n",
       "         0.3181, 0.3741, 0.4317, 0.4006, 0.4198, 0.2909, 0.3017, 0.4091, 0.3118,\n",
       "         0.4029, 0.3204, 0.6018, 0.4010, 0.4078, 0.4786, 0.3160, 0.4087, 0.4029,\n",
       "         0.3461, 0.3284, 0.3821, 0.5198, 0.3822, 0.3111, 0.9991, 0.3436, 0.3146,\n",
       "         0.3140, 0.4226, 0.3822, 0.2875, 0.3249, 0.7914, 0.3119, 0.3075, 0.3236,\n",
       "         0.3088, 0.5199, 0.3118, 0.3023, 0.2957, 0.3361, 0.5044, 0.2980, 0.3279,\n",
       "         0.3131, 0.3240, 0.3031, 0.3776, 0.3436, 0.3186, 0.5094, 0.2957, 0.3119,\n",
       "         0.3822, 0.3091, 0.5094, 0.3076, 0.3399, 0.3377, 0.4481, 0.3282, 0.2866,\n",
       "         0.3553, 0.3269, 0.3496, 0.3116, 0.3030, 0.2881, 0.3140, 0.3502, 0.3291,\n",
       "         0.3269, 0.3313, 0.5094, 0.3390, 0.3262, 0.3822, 0.9991, 0.3822, 0.5094,\n",
       "         0.3666, 0.2979, 0.3428, 0.3401, 0.3015, 0.3091, 0.9991, 0.2964, 0.3517,\n",
       "         0.3552, 0.3003, 0.5094, 0.3748, 0.3331, 0.3776, 0.3822, 0.3822, 0.4396,\n",
       "         0.4931, 0.9991, 0.9991, 0.3822, 0.3157, 0.3091, 0.3002, 0.3147, 0.9991,\n",
       "         0.3158, 0.4034, 0.2995, 0.3338, 0.3259, 0.9991, 0.3139, 0.3436, 0.3336,\n",
       "         0.3061, 0.5199, 0.2968, 0.3822, 0.3080, 0.3278, 0.3178, 0.5199, 0.3259,\n",
       "         0.3036, 0.3049, 0.3522, 0.3053, 0.3149, 0.4395, 0.3361, 0.3194, 0.3436,\n",
       "         0.3747, 0.3075, 0.3045, 0.5638, 0.3499, 0.3075, 0.3070, 0.3091, 0.3123,\n",
       "         0.3091, 0.3013, 0.3730, 0.4619, 0.3095, 0.3091, 0.3256, 0.3054, 0.3291,\n",
       "         0.4169, 0.3348, 0.3224, 0.2935, 0.3348, 0.2944, 0.3156, 0.3091, 0.3255,\n",
       "         0.2950, 0.2945, 0.3178, 0.2934, 0.3291, 0.2983, 0.9991, 0.9991, 0.3199,\n",
       "         0.3331, 1.1425, 0.3119, 0.3085, 0.3245, 0.3309, 0.3361, 0.3152, 0.3222,\n",
       "         0.3091, 0.3211, 0.2955, 0.3004, 0.3361, 0.3395, 0.3641, 0.4050, 0.3023,\n",
       "         0.3555, 0.3147, 0.3147, 0.3147, 0.2980, 0.3005, 0.3147, 0.2958, 0.2999,\n",
       "         0.2983, 0.3002, 0.3133, 0.2958, 0.3134, 0.3147, 0.3005, 0.3128, 0.2993,\n",
       "         0.3147, 0.3008, 0.3147, 0.3147, 0.2965, 0.3124, 0.3192, 0.3180, 0.3212,\n",
       "         0.3404, 0.3148, 0.3150, 0.3170, 0.3277, 0.3301, 0.3177, 0.3253, 0.3564])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['Query Points']['Implied Volatility']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
