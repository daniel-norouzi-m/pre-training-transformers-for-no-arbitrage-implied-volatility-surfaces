{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Number of Query Points' : 1,\n",
    "        'Batch Size' : 4\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 8,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 16,\n",
    "        'Attention Dropout' : 0.1,\n",
    "        'Gate Dropout' : 0.1,\n",
    "        'FFN Dropout' : 0.1,\n",
    "        'Number of Blocks' : 2,\n",
    "        'External Feature Dimension' : 5,\n",
    "    },\n",
    "    'Adaptive Loss Weights' : {\n",
    "        'Asymmetry' : 1.5,\n",
    "    },\n",
    "    'Trainer' : {\n",
    "        'Pre-Train' : {\n",
    "            'Number of Epochs' : 10,\n",
    "            'Warmup Ratio' : 0.15,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-5,\n",
    "            'Gradient Clipping' : 10,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.05,\n",
    "            'Layer-Wise Decay' : None,\n",
    "        },\n",
    "        'Fine-Tune' : {\n",
    "            'Number of Epochs' : 10,\n",
    "            'Warmup Ratio' : 0.1,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-6,\n",
    "            'Gradient Clipping' : 0,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.05,\n",
    "            'Layer-Wise Decay' : 0.9,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Log Moneyness</th>\n",
       "      <th>Time to Maturity</th>\n",
       "      <th>Implied Volatility</th>\n",
       "      <th>Market Return</th>\n",
       "      <th>Market Volatility</th>\n",
       "      <th>Treasury Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.157580</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.157580</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.145163</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.145163</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.132898</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-06-28</th>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.401026</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.407931</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.407931</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.414785</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.414785</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574326 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Log Moneyness  Time to Maturity  Implied Volatility  \\\n",
       "Datetime   Symbol                                                        \n",
       "2013-01-02 AAPL        -0.157580          0.007937              0.3726   \n",
       "           AAPL        -0.157580          0.007937              0.6095   \n",
       "           AAPL        -0.145163          0.007937              0.3726   \n",
       "           AAPL        -0.145163          0.007937              0.6095   \n",
       "           AAPL        -0.132898          0.007937              0.3726   \n",
       "...                          ...               ...                 ...   \n",
       "2013-06-28 GOOGL        0.401026          2.253968              0.2430   \n",
       "           GOOGL        0.407931          2.253968              0.2383   \n",
       "           GOOGL        0.407931          2.253968              0.2426   \n",
       "           GOOGL        0.414785          2.253968              0.2402   \n",
       "           GOOGL        0.414785          2.253968              0.2433   \n",
       "\n",
       "                   Market Return  Market Volatility  Treasury Rate  \n",
       "Datetime   Symbol                                                   \n",
       "2013-01-02 AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "...                          ...                ...            ...  \n",
       "2013-06-28 GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "\n",
       "[574326 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_googl_data = pd.read_csv('volatility_surface_AAPL_GOOGL_2013_01_2013_06.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "aapl_googl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Log Moneyness</th>\n",
       "      <th>Time to Maturity</th>\n",
       "      <th>Implied Volatility</th>\n",
       "      <th>Market Return</th>\n",
       "      <th>Market Volatility</th>\n",
       "      <th>Treasury Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.157580</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.157580</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.145163</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.145163</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.132898</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-06-28</th>\n",
       "      <th>XLE</th>\n",
       "      <td>0.707787</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLE</th>\n",
       "      <td>0.749373</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.1724</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLE</th>\n",
       "      <td>0.749373</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLE</th>\n",
       "      <td>0.789190</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.1618</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLE</th>\n",
       "      <td>0.789190</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1327104 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Log Moneyness  Time to Maturity  Implied Volatility  \\\n",
       "Datetime   Symbol                                                        \n",
       "2013-01-02 AAPL        -0.157580          0.007937              0.3726   \n",
       "           AAPL        -0.157580          0.007937              0.6095   \n",
       "           AAPL        -0.145163          0.007937              0.3726   \n",
       "           AAPL        -0.145163          0.007937              0.6095   \n",
       "           AAPL        -0.132898          0.007937              0.3726   \n",
       "...                          ...               ...                 ...   \n",
       "2013-06-28 XLE          0.707787          2.253968              0.1794   \n",
       "           XLE          0.749373          2.253968              0.1724   \n",
       "           XLE          0.749373          2.253968              0.1824   \n",
       "           XLE          0.789190          2.253968              0.1618   \n",
       "           XLE          0.789190          2.253968              0.1822   \n",
       "\n",
       "                   Market Return  Market Volatility  Treasury Rate  \n",
       "Datetime   Symbol                                                   \n",
       "2013-01-02 AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "...                          ...                ...            ...  \n",
       "2013-06-28 XLE         -0.004299          16.860001          0.030  \n",
       "           XLE         -0.004299          16.860001          0.030  \n",
       "           XLE         -0.004299          16.860001          0.030  \n",
       "           XLE         -0.004299          16.860001          0.030  \n",
       "           XLE         -0.004299          16.860001          0.030  \n",
       "\n",
       "[1327104 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data = pd.read_csv('pre_train_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "fine_tune_data = pd.read_csv('fine_tune_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "pre_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \\begin{table}[h!]\n",
    "\\centering\n",
    "\\begin{tabular}{|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\textbf{Category} & \\textbf{Symbol} & \\textbf{Count} & \\textbf{Sector/Type} \\\\ \\hline\n",
    "\\multirow{5}{*}{Pre-Train} & SPX & 373,295 & Index \\\\ \\cline{2-4} \n",
    " & GLD & 346,588 & Commodity \\\\ \\cline{2-4} \n",
    " & AAPL & 306,950 & Technology \\\\ \\cline{2-4} \n",
    " & TLT & 161,764 & Bond \\\\ \\cline{2-4} \n",
    " & XLE & 138,507 & Energy \\\\ \\hline\n",
    "\\multirow{5}{*}{Fine-Tune} & SCOR & 5,300 & Technology \\\\ \\cline{2-4} \n",
    " & AIN & 5,288 & Industrial \\\\ \\cline{2-4} \n",
    " & AZPN & 5,284 & Software \\\\ \\cline{2-4} \n",
    " & RWO & 5,258 & Real Estate \\\\ \\cline{2-4} \n",
    " & PNNT & 5,232 & Financials \\\\ \\hline\n",
    "\\end{tabular}\n",
    "\\caption{Pre-Train and Fine-Tune Symbols with their count and sector/type.}\n",
    "\\label{table:symbols}\n",
    "\\end{table} -->\n",
    "| Category    | Symbol | Count  | Sector/Type   |\n",
    "|-------------|--------|--------|---------------|\n",
    "| Pre-Train   | SPX    | 373,295| Index         |\n",
    "| Pre-Train   | GLD    | 346,588| Commodity     |\n",
    "| Pre-Train   | AAPL   | 306,950| Technology    |\n",
    "| Pre-Train   | TLT    | 161,764| Bond          |\n",
    "| Pre-Train   | XLE    | 138,507| Energy        |\n",
    "| Fine-Tune   | SCOR   | 5,300  | Technology    |\n",
    "| Fine-Tune   | AIN    | 5,288  | Industrial    |\n",
    "| Fine-Tune   | AZPN   | 5,284  | Software      |\n",
    "| Fine-Tune   | RWO    | 5,258  | Real Estate   |\n",
    "| Fine-Tune   | PNNT   | 5,232  | Financials    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implied_volatility_surfaces(\n",
    "    options_market_data,\n",
    "    toy_sample=False,\n",
    "    max_points=30,\n",
    "    max_surfaces=10,\n",
    "    random_state=0,\n",
    "):\n",
    "    # Group the data by Datetime and Symbol\n",
    "    grouped_data = options_market_data.groupby(level=['Datetime', 'Symbol'])\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    surfaces = []\n",
    "    for (date, symbol), surface in grouped_data:\n",
    "\n",
    "        if toy_sample:\n",
    "            entire_size = len(surface['Implied Volatility'])\n",
    "            sample = rng.choice(range(entire_size), size=min(max_points, entire_size), replace=False)\n",
    "\n",
    "        surface_dict = {\n",
    "            'Datetime': date,\n",
    "            'Symbol': symbol,\n",
    "            'Market Features': {\n",
    "                'Market Return': surface['Market Return'].values[0],\n",
    "                'Market Volatility': surface['Market Volatility'].values[0],\n",
    "                'Treasury Rate': surface['Treasury Rate'].values[0],\n",
    "            },\n",
    "            'Surface': {\n",
    "                'Log Moneyness': surface['Log Moneyness'].values if not toy_sample else surface['Log Moneyness'].values[sample],\n",
    "                'Time to Maturity': surface['Time to Maturity'].values if not toy_sample else surface['Time to Maturity'].values[sample],\n",
    "                'Implied Volatility': surface['Implied Volatility'].values if not toy_sample else surface['Implied Volatility'].values[sample],\n",
    "            }\n",
    "        }\n",
    "        surfaces.append(surface_dict)\n",
    "\n",
    "    if toy_sample:\n",
    "        entire_size = len(surfaces)\n",
    "        sample = rng.choice(range(entire_size), size=min(max_surfaces, entire_size), replace=False)  \n",
    "\n",
    "        return [surfaces[i] for i in sample]\n",
    "\n",
    "    return surfaces\n",
    "\n",
    "def split_surfaces(\n",
    "    data, \n",
    "    n_partitions=6,\n",
    "    toy_sample=False,\n",
    "    max_points=30,\n",
    "    max_surfaces=10,\n",
    "    random_state=0,\n",
    "):\n",
    "    # Extract unique sorted timestamps\n",
    "    unique_timestamps = np.sort(data.index.get_level_values('Datetime').unique())\n",
    "    \n",
    "    # Split timestamps into partitions\n",
    "    partitions = np.array_split(unique_timestamps, n_partitions)\n",
    "    \n",
    "    # Initialize lists to hold the final timestamps for each dataset\n",
    "    train_times = []\n",
    "    validation_times = []\n",
    "    test_times = []\n",
    "\n",
    "    for partition in partitions:\n",
    "        # Remove the first day of the partition to avoid leakage\n",
    "        if len(partition) > 1:\n",
    "            partition = partition[1:]\n",
    "        \n",
    "        # Determine the number of timestamps for training, validation, and test sets\n",
    "        partition_len = len(partition)\n",
    "        train_len = int(0.8 * partition_len)\n",
    "        valid_len = int(0.1 * partition_len)\n",
    "        \n",
    "        # Assign timestamps to train, validation, and test sets\n",
    "        train_times.extend(partition[:train_len])\n",
    "        validation_times.extend(partition[train_len:train_len + valid_len])\n",
    "        test_times.extend(partition[train_len + valid_len:])\n",
    "    \n",
    "    # Now, use the timestamps to filter the data for each set\n",
    "    train_set = data.query('Datetime in @train_times')\n",
    "    validation_set = data.query('Datetime in @validation_times')\n",
    "    test_set = data.query('Datetime in @test_times')\n",
    "\n",
    "    train_surface = implied_volatility_surfaces(\n",
    "        train_set,\n",
    "        toy_sample,\n",
    "        max_points,\n",
    "        max_surfaces,\n",
    "        random_state,\n",
    "    )\n",
    "    validation_surface = implied_volatility_surfaces(\n",
    "        validation_set,\n",
    "        toy_sample,\n",
    "        max_points,\n",
    "        max_surfaces,\n",
    "        random_state,\n",
    "    )\n",
    "    test_surface = implied_volatility_surfaces(\n",
    "        test_set,\n",
    "        toy_sample,\n",
    "        max_points,\n",
    "        max_surfaces,\n",
    "        random_state,\n",
    "    )\n",
    "    \n",
    "    return train_surface, validation_surface, test_surface\n",
    "\n",
    "pre_train_surfaces_train, pre_train_surfaces_validation, pre_train_surfaces_test = split_surfaces(pre_train_data)\n",
    "fine_tune_surfaces_train, fine_tune_surfaces_validation, fine_tune_surfaces_test = split_surfaces(fine_tune_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Datetime': Timestamp('2013-06-12 00:00:00'),\n",
       "  'Symbol': 'TLT',\n",
       "  'Market Features': {'Market Return': -0.0084047769080509,\n",
       "   'Market Volatility': 18.59000015258789,\n",
       "   'Treasury Rate': 0.0399999991059303},\n",
       "  'Surface': {'Log Moneyness': array([-0.21866899,  0.22485181, -0.18183408,  0.22421516,  0.67381015,\n",
       "          -0.14574866,  0.62940656, -0.25274877,  0.32355592,  0.78129185,\n",
       "           0.60916763,  0.3536221 ,  0.26678606,  0.17065863,  0.6094393 ,\n",
       "           0.34843174,  0.31065653,  0.5022846 ,  0.38062512,  0.33057863,\n",
       "           0.56565948, -0.18972653,  0.53271615,  0.40711401,  0.34975436,\n",
       "          -0.00768851,  0.24323508,  0.28716713,  0.34187851,  0.44386138]),\n",
       "   'Time to Maturity': array([0.03968254, 0.00793651, 1.12301587, 1.12301587, 0.87301587,\n",
       "          0.40079365, 0.03968254, 1.12301587, 0.00793651, 0.87301587,\n",
       "          0.87301587, 0.26190476, 0.40079365, 0.87301587, 2.31746032,\n",
       "          0.03968254, 0.00793651, 0.40079365, 1.12301587, 0.76190476,\n",
       "          0.87301587, 0.03968254, 0.26190476, 0.87301587, 1.12301587,\n",
       "          0.03968254, 0.03968254, 0.15079365, 1.12301587, 0.87301587]),\n",
       "   'Implied Volatility': array([0.2059, 0.2597, 0.2039, 0.1463, 0.2402, 0.1533, 0.2059, 0.2039,\n",
       "          0.2535, 0.2402, 0.2402, 0.1904, 0.1496, 0.1573, 0.1625, 0.2059,\n",
       "          0.2535, 0.1795, 0.2037, 0.1857, 0.1743, 0.2394, 0.1799, 0.1544,\n",
       "          0.1946, 0.2059, 0.1828, 0.163 , 0.1933, 0.1605])}},\n",
       " {'Datetime': Timestamp('2013-06-05 00:00:00'),\n",
       "  'Symbol': 'TLT',\n",
       "  'Market Features': {'Market Return': -0.0138755545439222,\n",
       "   'Market Volatility': 17.5,\n",
       "   'Treasury Rate': 0.0450000017881393},\n",
       "  'Surface': {'Log Moneyness': array([ 0.04619413,  0.63345466,  0.34581734,  0.42599264,  0.28366032,\n",
       "           0.68094149,  0.34360024, -0.26418774, -0.16730176,  0.36090666,\n",
       "           0.16969634,  0.45167709,  0.13369547,  0.06753854,  0.33526912,\n",
       "           0.60258423, -0.06987106,  0.03462234,  0.50134575,  0.60476159,\n",
       "           0.09903866,  0.32200616,  0.53760805,  0.00133297,  0.5804843 ,\n",
       "           0.33683503,  0.11891351,  0.43295682,  0.51607671, -0.08185543]),\n",
       "   'Time to Maturity': array([0.06746032, 0.06746032, 1.15079365, 1.15079365, 0.42857143,\n",
       "          0.06746032, 0.78968254, 0.90079365, 0.90079365, 1.15079365,\n",
       "          0.06746032, 0.90079365, 1.15079365, 0.17857143, 0.00793651,\n",
       "          0.06746032, 0.06746032, 0.42857143, 0.78968254, 0.90079365,\n",
       "          0.17857143, 0.06746032, 0.90079365, 0.78968254, 1.15079365,\n",
       "          0.17857143, 0.28968254, 1.15079365, 0.42857143, 0.17857143]),\n",
       "   'Implied Volatility': array([0.2201, 0.2201, 0.1864, 0.2126, 0.162 , 0.2201, 0.181 , 0.2216,\n",
       "          0.2216, 0.1906, 0.2171, 0.1562, 0.1678, 0.1684, 0.3811, 0.1876,\n",
       "          0.2201, 0.186 , 0.1498, 0.1752, 0.2172, 0.1886, 0.2201, 0.1498,\n",
       "          0.1542, 0.155 , 0.2064, 0.1451, 0.1508, 0.2172])}},\n",
       " {'Datetime': Timestamp('2013-05-09 00:00:00'),\n",
       "  'Symbol': 'AAPL',\n",
       "  'Market Features': {'Market Return': -0.0036939180287733,\n",
       "   'Market Volatility': 13.130000114440918,\n",
       "   'Treasury Rate': 0.0350000001490116},\n",
       "  'Surface': {'Log Moneyness': array([ 0.14245869,  0.1956509 ,  0.05053135,  0.66127764, -0.64754602,\n",
       "           0.08572204,  0.49265413,  0.36448942,  0.20864479, -0.14858082,\n",
       "          -0.37685434,  0.35576496,  0.91115831, -0.07136717, -0.05031755,\n",
       "           0.5676887 , -0.59867986, -0.06317537,  0.41310626,  0.62704716,\n",
       "           0.46359397,  0.75079587,  0.86908573,  0.68140644,  0.53921996,\n",
       "           0.01391593,  0.19599843,  0.60267207,  0.17943707,  0.47142184]),\n",
       "   'Time to Maturity': array([0.05952381, 0.08730159, 0.11507937, 2.45238095, 2.45238095,\n",
       "          0.17460317, 0.3968254 , 0.28571429, 2.45238095, 0.00396825,\n",
       "          0.17460317, 0.28571429, 0.28571429, 2.45238095, 0.17460317,\n",
       "          1.00793651, 0.3968254 , 0.3968254 , 1.00793651, 1.00793651,\n",
       "          0.28571429, 0.6468254 , 1.00793651, 0.28571429, 0.28571429,\n",
       "          0.00396825, 0.03571429, 0.28571429, 1.00793651, 0.28571429]),\n",
       "   'Implied Volatility': array([0.2485, 0.2191, 0.2753, 0.2842, 0.3073, 0.2888, 0.3038, 0.2757,\n",
       "          0.2751, 0.791 , 0.3034, 0.2732, 0.188 , 0.2975, 0.419 , 0.2712,\n",
       "          0.3267, 0.2938, 0.2393, 0.231 , 0.3114, 0.2827, 0.231 , 0.188 ,\n",
       "          0.3406, 0.791 , 0.1671, 0.188 , 0.2592, 0.3169])}},\n",
       " {'Datetime': Timestamp('2013-03-13 00:00:00'),\n",
       "  'Symbol': 'SPX',\n",
       "  'Market Features': {'Market Return': 0.0013131891571707,\n",
       "   'Market Volatility': 11.829999923706056,\n",
       "   'Treasury Rate': 0.0850000008940696},\n",
       "  'Surface': {'Log Moneyness': array([-0.21738898, -0.29752251,  0.11993025, -0.11078194, -0.93949041,\n",
       "           0.10355613, -0.01855568, -0.33473114, -0.15637723,  0.19823338,\n",
       "          -0.06330464, -0.10037933,  0.01621072, -0.08247157, -0.07615302,\n",
       "          -0.04808627, -0.09443827, -0.5673053 , -0.09899169, -0.03213378,\n",
       "          -0.89462168,  0.08545813,  0.42961762, -1.16775022,  0.11787517,\n",
       "           0.00981302, -0.29677812, -0.04747966,  0.13378412, -0.06212411]),\n",
       "   'Time to Maturity': array([0.03571429, 1.8452381 , 1.23412698, 0.11904762, 1.23412698,\n",
       "          0.05952381, 0.40079365, 0.40079365, 0.26190476, 0.15079365,\n",
       "          1.16269841, 0.09126984, 0.03571429, 0.15079365, 0.05952381,\n",
       "          0.26190476, 0.40079365, 0.76190476, 0.26190476, 0.09126984,\n",
       "          0.76190476, 0.15079365, 1.23412698, 1.8452381 , 0.05952381,\n",
       "          0.11904762, 0.01190476, 1.23412698, 1.12301587, 0.11904762]),\n",
       "   'Implied Volatility': array([0.2069, 0.249 , 0.1691, 0.2162, 0.1197, 0.1258, 0.1411, 0.242 ,\n",
       "          0.1254, 0.0873, 0.1339, 0.2177, 0.0781, 0.1763, 0.1896, 0.1514,\n",
       "          0.1268, 0.3307, 0.1254, 0.1343, 0.2639, 0.0873, 0.3562, 0.1036,\n",
       "          0.075 , 0.099 , 0.1633, 0.1785, 0.1042, 0.1545])}},\n",
       " {'Datetime': Timestamp('2013-04-05 00:00:00'),\n",
       "  'Symbol': 'TLT',\n",
       "  'Market Features': {'Market Return': -0.0043041451689076,\n",
       "   'Market Volatility': 13.920000076293944,\n",
       "   'Treasury Rate': 0.0599999986588954},\n",
       "  'Surface': {'Log Moneyness': array([ 0.01966988,  0.29701464, -0.20210073,  0.03030713,  0.36787778,\n",
       "           0.02943689,  0.28075412,  0.28438343,  0.42273155,  0.31690897,\n",
       "           0.34086153,  0.09094484, -0.13214101, -0.05716021,  0.56629339,\n",
       "           0.54270233,  0.33600119,  0.3634505 ,  0.42482858,  0.38151053,\n",
       "           0.32762504, -0.06933389, -0.02303798,  0.15277607,  0.37086706,\n",
       "           0.33139622,  0.2778451 ,  0.2739565 ,  0.50450409,  0.192211  ]),\n",
       "   'Time to Maturity': array([0.30952381, 0.        , 0.30952381, 0.17063492, 0.30952381,\n",
       "          0.67063492, 0.        , 0.02777778, 0.67063492, 1.03174603,\n",
       "          0.17063492, 0.17063492, 1.14285714, 0.05952381, 0.30952381,\n",
       "          0.67063492, 0.02777778, 0.17063492, 0.30952381, 0.67063492,\n",
       "          0.05952381, 2.58730159, 0.30952381, 0.67063492, 0.17063492,\n",
       "          1.03174603, 0.67063492, 0.42063492, 0.30952381, 1.03174603]),\n",
       "   'Implied Volatility': array([0.1769, 0.3   , 0.1769, 0.1848, 0.1149, 0.1608, 0.3   , 0.1266,\n",
       "          0.1322, 0.1349, 0.1322, 0.1848, 0.4357, 0.1656, 0.1149, 0.1123,\n",
       "          0.1302, 0.1436, 0.1363, 0.1322, 0.1278, 0.185 , 0.1769, 0.1442,\n",
       "          0.149 , 0.135 , 0.1333, 0.128 , 0.1363, 0.1602])}},\n",
       " {'Datetime': Timestamp('2013-04-11 00:00:00'),\n",
       "  'Symbol': 'TLT',\n",
       "  'Market Features': {'Market Return': 0.0035459561795765,\n",
       "   'Market Volatility': 12.239999771118164,\n",
       "   'Treasury Rate': 0.0630000010132789},\n",
       "  'Surface': {'Log Moneyness': array([ 0.26941434,  0.31495983,  0.6378691 , -0.00134122, -0.02345512,\n",
       "           0.51363104,  0.53963777,  0.59265727,  0.29757607,  0.3675207 ,\n",
       "           0.2856628 ,  0.14070821,  0.13032579,  0.224163  ,  0.37736744,\n",
       "           0.15271836,  0.39577034, -0.1003801 ,  0.48525372,  0.2309271 ,\n",
       "           0.39550739,  0.20472033,  0.40874521, -0.0580308 ,  0.54554859,\n",
       "           0.41503622,  0.31322331,  0.15254515,  0.5762443 ,  0.67978163]),\n",
       "   'Time to Maturity': array([0.00396825, 0.00396825, 0.6468254 , 0.6468254 , 0.28571429,\n",
       "          0.28571429, 0.6468254 , 1.11904762, 0.28571429, 0.28571429,\n",
       "          0.03571429, 0.28571429, 0.3968254 , 0.1468254 , 1.11904762,\n",
       "          0.00396825, 0.03571429, 1.00793651, 1.00793651, 0.28571429,\n",
       "          0.6468254 , 0.28571429, 0.3968254 , 0.1468254 , 1.00793651,\n",
       "          0.1468254 , 2.56349206, 1.00793651, 0.28571429, 0.28571429]),\n",
       "   'Implied Volatility': array([0.2108, 0.2108, 0.198 , 0.1582, 0.0978, 0.0978, 0.0816, 0.371 ,\n",
       "          0.0978, 0.0978, 0.1143, 0.0978, 0.0818, 0.1036, 0.1157, 0.2579,\n",
       "          0.1527, 0.183 , 0.2612, 0.0978, 0.198 , 0.0978, 0.0818, 0.1583,\n",
       "          0.2612, 0.1766, 0.2342, 0.0568, 0.0978, 0.0978])}},\n",
       " {'Datetime': Timestamp('2013-02-11 00:00:00'),\n",
       "  'Symbol': 'AAPL',\n",
       "  'Market Features': {'Market Return': -0.0006063012476382,\n",
       "   'Market Volatility': 12.9399995803833,\n",
       "   'Treasury Rate': 0.0649999976158142},\n",
       "  'Surface': {'Log Moneyness': array([-0.33923947, -0.11663115,  0.66561834,  0.3898211 ,  0.36439331,\n",
       "           0.85277886,  0.72444919,  0.80912555, -0.01650213, -0.09271604,\n",
       "           0.79331382,  0.35926353,  0.15318475,  0.40241293,  0.50034811,\n",
       "           0.46364572,  0.51954701,  0.32993201,  0.00780407,  0.58903272,\n",
       "          -0.39643083,  0.60994591,  0.50999821,  0.26659038,  0.33807875,\n",
       "           0.14084758,  0.25970634,  0.00756217,  0.13123613, -0.40468262]),\n",
       "   'Time to Maturity': array([0.51984127, 0.51984127, 1.3531746 , 0.13095238, 0.51984127,\n",
       "          0.63095238, 0.63095238, 0.51984127, 0.01984127, 0.13095238,\n",
       "          0.63095238, 0.01984127, 0.01984127, 0.26984127, 1.3531746 ,\n",
       "          0.01984127, 1.3531746 , 0.99206349, 0.13095238, 0.63095238,\n",
       "          0.99206349, 1.3531746 , 0.63095238, 0.26984127, 1.3531746 ,\n",
       "          1.3531746 , 0.13095238, 0.63095238, 0.13095238, 1.3531746 ]),\n",
       "   'Implied Volatility': array([0.3159, 0.3116, 0.3533, 0.3225, 0.3224, 0.3186, 0.3186, 0.3361,\n",
       "          0.3892, 0.3348, 0.3186, 0.4347, 0.3367, 0.3008, 0.3518, 0.6264,\n",
       "          0.2359, 0.3039, 0.3187, 0.3086, 0.2504, 0.3533, 0.2927, 0.2809,\n",
       "          0.3141, 0.2359, 0.3047, 0.2827, 0.2903, 0.2359])}},\n",
       " {'Datetime': Timestamp('2013-01-23 00:00:00'),\n",
       "  'Symbol': 'TLT',\n",
       "  'Market Features': {'Market Return': 0.0015063419242931,\n",
       "   'Market Volatility': 12.460000038146973,\n",
       "   'Treasury Rate': 0.0700000002980232},\n",
       "  'Surface': {'Log Moneyness': array([0.46343289, 0.64989843, 0.29723401, 0.7230541 , 0.30959866,\n",
       "          0.47144278, 0.17314338, 0.17444991, 0.3575821 , 0.43005424,\n",
       "          0.62745321, 0.48929735, 0.26176349, 0.37673241, 0.27465304,\n",
       "          0.52127474, 0.29723401, 0.42299041, 0.53366881, 0.07859737,\n",
       "          0.36935902, 0.476591  , 0.15601706, 0.3418422 , 0.33387831,\n",
       "          0.17609695, 0.32059572, 0.46557881, 0.00533436, 0.15484858]),\n",
       "   'Time to Maturity': array([0.5952381 , 0.20634921, 0.00793651, 0.5952381 , 0.00793651,\n",
       "          0.95634921, 0.45634921, 0.3452381 , 0.00793651, 0.3452381 ,\n",
       "          0.5952381 , 0.20634921, 2.87301587, 0.45634921, 0.0952381 ,\n",
       "          0.5952381 , 0.00793651, 0.0952381 , 0.5952381 , 0.3452381 ,\n",
       "          0.45634921, 0.5952381 , 0.3452381 , 0.00793651, 0.00793651,\n",
       "          0.20634921, 0.95634921, 0.0952381 , 0.5952381 , 0.45634921]),\n",
       "   'Implied Volatility': array([0.136 , 0.1396, 0.1611, 0.1348, 0.191 , 0.1523, 0.1479, 0.1452,\n",
       "          0.191 , 0.1271, 0.136 , 0.1396, 0.1548, 0.1356, 0.1235, 0.1348,\n",
       "          0.1575, 0.1535, 0.1348, 0.1452, 0.1331, 0.136 , 0.1452, 0.1724,\n",
       "          0.1724, 0.1957, 0.1464, 0.159 , 0.1668, 0.1381])}},\n",
       " {'Datetime': Timestamp('2013-04-10 00:00:00'),\n",
       "  'Symbol': 'AAPL',\n",
       "  'Market Features': {'Market Return': 0.0121154433295488,\n",
       "   'Market Volatility': 12.359999656677246,\n",
       "   'Treasury Rate': 0.0649999976158142},\n",
       "  'Surface': {'Log Moneyness': array([-0.21936906,  0.122374  , -0.35632453, -0.45246347,  0.83115232,\n",
       "           0.11364929,  0.8285943 ,  0.95464063, -0.06457837,  0.31294919,\n",
       "           0.15784662,  0.48366153,  0.90550893,  0.62145039,  0.73480469,\n",
       "           0.80677887,  0.50475857,  0.74715608,  0.40322744,  0.27924769,\n",
       "           0.31966663,  0.17367804,  0.26769976,  0.60367835,  0.48853616,\n",
       "           0.76539665,  0.13973515,  0.01534622,  0.6437002 ,  0.33355702]),\n",
       "   'Time to Maturity': array([0.03968254, 0.76190476, 1.12301587, 1.12301587, 0.03968254,\n",
       "          0.40079365, 2.56746032, 0.40079365, 0.28968254, 2.56746032,\n",
       "          2.56746032, 2.56746032, 0.28968254, 0.15079365, 0.28968254,\n",
       "          0.40079365, 0.28968254, 0.28968254, 0.03968254, 0.09126984,\n",
       "          0.03968254, 0.06349206, 0.76190476, 2.56746032, 0.03968254,\n",
       "          0.28968254, 0.03968254, 1.12301587, 0.28968254, 0.28968254]),\n",
       "   'Implied Volatility': array([0.2904, 0.3079, 0.3679, 0.3757, 0.2799, 0.307 , 0.3317, 0.3377,\n",
       "          0.3892, 0.3323, 0.3307, 0.3202, 0.3552, 0.3865, 0.3552, 0.3116,\n",
       "          0.3473, 0.3523, 0.3759, 0.3925, 0.2799, 0.4359, 0.3008, 0.3206,\n",
       "          0.2799, 0.3552, 0.2542, 0.3256, 0.3552, 0.3141])}},\n",
       " {'Datetime': Timestamp('2013-06-17 00:00:00'),\n",
       "  'Symbol': 'TLT',\n",
       "  'Market Features': {'Market Return': 0.0075388755627146,\n",
       "   'Market Volatility': 16.799999237060547,\n",
       "   'Treasury Rate': 0.0399999991059303},\n",
       "  'Surface': {'Log Moneyness': array([ 0.1479486 ,  0.33602401,  0.29570197,  0.23222601,  0.00767665,\n",
       "           0.6027017 , -0.02715997,  0.31360381,  0.54926852,  0.25840493,\n",
       "           0.26908047,  0.62827528,  0.66162469,  0.51845824,  0.57110932,\n",
       "           0.49269598, -0.20915443,  0.77334436,  0.38636749,  0.06349428,\n",
       "           0.12407935,  0.48047022,  0.11682437,  0.36390111,  0.00771018,\n",
       "          -0.16961126,  0.35627591,  0.54926852,  0.15863726,  0.52072254]),\n",
       "   'Time to Maturity': array([0.13095238, 0.38095238, 0.24206349, 0.38095238, 0.8531746 ,\n",
       "          2.29761905, 0.38095238, 0.8531746 , 0.38095238, 0.01984127,\n",
       "          1.1031746 , 0.01984127, 0.38095238, 0.24206349, 0.8531746 ,\n",
       "          0.13095238, 0.38095238, 0.01984127, 0.74206349, 0.38095238,\n",
       "          0.8531746 , 0.01984127, 0.24206349, 0.13095238, 0.74206349,\n",
       "          0.01984127, 2.29761905, 0.38095238, 0.01984127, 0.13095238]),\n",
       "   'Implied Volatility': array([0.2139, 0.1671, 0.1554, 0.1671, 0.2008, 0.16  , 0.1962, 0.1575,\n",
       "          0.1813, 0.2069, 0.1634, 0.2324, 0.1639, 0.1755, 0.1676, 0.188 ,\n",
       "          0.1962, 0.1903, 0.155 , 0.1962, 0.2232, 0.1903, 0.2922, 0.1796,\n",
       "          0.2029, 0.2688, 0.1708, 0.1639, 0.1903, 0.1862])}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def implied_volatility_surfaces(\n",
    "    options_market_data,\n",
    "    toy_sample=False,\n",
    "    max_points=30,\n",
    "    max_surfaces=10,\n",
    "    random_state=0,\n",
    "):\n",
    "    # Group the data by Datetime and Symbol\n",
    "    grouped_data = options_market_data.groupby(level=['Datetime', 'Symbol'])\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    surfaces = []\n",
    "    for (date, symbol), surface in grouped_data:\n",
    "\n",
    "        if toy_sample:\n",
    "            entire_size = len(surface['Implied Volatility'])\n",
    "            sample = rng.choice(range(entire_size), size=min(max_points, entire_size), replace=False)\n",
    "\n",
    "        surface_dict = {\n",
    "            'Datetime': date,\n",
    "            'Symbol': symbol,\n",
    "            'Market Features': {\n",
    "                'Market Return': surface['Market Return'].values[0],\n",
    "                'Market Volatility': surface['Market Volatility'].values[0],\n",
    "                'Treasury Rate': surface['Treasury Rate'].values[0],\n",
    "            },\n",
    "            'Surface': {\n",
    "                'Log Moneyness': surface['Log Moneyness'].values if not toy_sample else surface['Log Moneyness'].values[sample],\n",
    "                'Time to Maturity': surface['Time to Maturity'].values if not toy_sample else surface['Time to Maturity'].values[sample],\n",
    "                'Implied Volatility': surface['Implied Volatility'].values if not toy_sample else surface['Implied Volatility'].values[sample],\n",
    "            }\n",
    "        }\n",
    "        surfaces.append(surface_dict)\n",
    "\n",
    "    if toy_sample:\n",
    "        entire_size = len(surfaces)\n",
    "        sample = rng.choice(range(entire_size), size=min(max_surfaces, entire_size), replace=False)  \n",
    "\n",
    "        return [surfaces[i] for i in sample]\n",
    "\n",
    "    return surfaces\n",
    "\n",
    "surfaces = implied_volatility_surfaces(pre_train_data, toy_sample=True)\n",
    "surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Datetime': Timestamp('2013-01-11 00:00:00'),\n",
       "  'Symbol': 'AAPL',\n",
       "  'Market Features': {'Market Return': -4.751511497752233e-05,\n",
       "   'Market Volatility': 13.359999656677246,\n",
       "   'Treasury Rate': 0.0630000010132789},\n",
       "  'Surface': {'Log Moneyness': array([ 0.0551592 , -0.19245714,  0.63415214,  0.76840733, -0.16988414,\n",
       "           0.72750987,  0.14750983,  0.2445806 , -0.03015713,  0.80283905,\n",
       "           0.63249342,  0.20403206,  0.06347577,  0.16156258,  0.02329139,\n",
       "           0.19554033,  0.73838709,  0.01381309,  0.67444361,  0.24809753,\n",
       "           0.05698059,  0.32625828,  0.30926406,  0.10427799,  0.83463364,\n",
       "          -0.17754798,  0.49295122,  0.47599446,  0.36920461,  0.53795844]),\n",
       "   'Time to Maturity': array([0.64285714, 0.25396825, 1.11507937, 0.39285714, 1.11507937,\n",
       "          0.39285714, 0.14285714, 1.47619048, 0.75396825, 0.39285714,\n",
       "          0.14285714, 0.11111111, 1.11507937, 0.64285714, 1.11507937,\n",
       "          0.05555556, 0.03174603, 0.08333333, 0.64285714, 0.14285714,\n",
       "          0.11111111, 0.        , 0.03174603, 0.75396825, 0.14285714,\n",
       "          0.39285714, 2.92063492, 0.03174603, 2.92063492, 0.39285714]),\n",
       "   'Implied Volatility': array([0.3428, 0.4342, 0.3171, 0.3467, 0.3677, 0.3467, 0.3987, 0.3291,\n",
       "          0.3505, 0.3849, 0.4247, 0.4216, 0.3381, 0.3298, 0.3426, 0.4925,\n",
       "          0.5223, 0.4756, 0.3358, 0.388 , 0.4745, 0.3   , 0.4646, 0.3326,\n",
       "          0.4247, 0.3945, 0.3395, 0.3322, 0.3355, 0.3467])}},\n",
       " {'Datetime': Timestamp('2013-03-28 00:00:00'),\n",
       "  'Symbol': 'GOOGL',\n",
       "  'Market Features': {'Market Return': 0.0040484633961911,\n",
       "   'Market Volatility': 12.699999809265137,\n",
       "   'Treasury Rate': 0.0649999976158142},\n",
       "  'Surface': {'Log Moneyness': array([ 0.00836545, -0.7464662 , -0.10957502, -0.06913673,  0.12529798,\n",
       "           0.00200291, -0.35041939,  0.14265591,  0.19629649,  0.14177638,\n",
       "          -0.18326214,  0.09595395,  0.03269949,  0.33167563,  0.20869038,\n",
       "          -0.1224451 ,  0.15808789, -0.37643854,  0.09633998, -0.09657154,\n",
       "          -0.12655033, -0.05570705, -0.34015201,  0.09113879,  0.02072932,\n",
       "           0.1831764 , -0.02231328, -0.25714036,  0.01936228, -0.67139574]),\n",
       "   'Time to Maturity': array([0.11507937, 0.34126984, 1.17460317, 0.09126984, 0.11507937,\n",
       "          1.17460317, 0.70238095, 1.17460317, 1.17460317, 0.70238095,\n",
       "          0.70238095, 0.34126984, 0.20238095, 0.20238095, 0.20238095,\n",
       "          0.34126984, 0.09126984, 0.34126984, 1.17460317, 0.05952381,\n",
       "          0.70238095, 0.11507937, 1.17460317, 0.20238095, 0.09126984,\n",
       "          0.20238095, 0.70238095, 0.34126984, 1.17460317, 1.17460317]),\n",
       "   'Implied Volatility': array([0.2638, 0.2159, 0.2312, 0.308 , 0.2577, 0.2115, 0.2431, 0.2024,\n",
       "          0.1944, 0.2013, 0.2431, 0.2011, 0.2251, 0.2324, 0.2324, 0.2231,\n",
       "          0.2525, 0.2159, 0.2009, 0.2365, 0.2352, 0.2768, 0.2555, 0.2295,\n",
       "          0.2829, 0.2324, 0.2145, 0.2706, 0.211 , 0.2529])}},\n",
       " {'Datetime': Timestamp('2013-02-08 00:00:00'),\n",
       "  'Symbol': 'AAPL',\n",
       "  'Market Features': {'Market Return': 0.0056419945850431,\n",
       "   'Market Volatility': 13.020000457763672,\n",
       "   'Treasury Rate': 0.0649999976158142},\n",
       "  'Surface': {'Log Moneyness': array([ 0.27293084,  0.78722805,  0.6241932 , -0.17674145,  0.15974295,\n",
       "           0.1319956 ,  0.17375936, -0.05679847, -0.16617853,  0.29830688,\n",
       "          -0.05628978,  0.40152285,  0.04167079,  0.28972857,  0.27168126,\n",
       "           0.10153327, -0.12369024, -0.20400214,  0.53315784, -0.50511289,\n",
       "           0.6808397 ,  0.19109512,  0.64723964,  0.58509004,  0.17319579,\n",
       "           0.49647092,  0.72783828,  0.1527377 ,  0.04101987,  0.13719138]),\n",
       "   'Time to Maturity': array([1.00396825, 0.64285714, 0.14285714, 2.80952381, 0.39285714,\n",
       "          0.        , 0.03174603, 0.        , 1.36507937, 0.08333333,\n",
       "          0.14285714, 0.39285714, 0.14285714, 0.05555556, 0.03174603,\n",
       "          2.80952381, 0.03174603, 0.53174603, 2.80952381, 1.36507937,\n",
       "          0.14285714, 0.28174603, 1.36507937, 1.00396825, 0.08333333,\n",
       "          1.36507937, 0.03174603, 0.05555556, 0.39285714, 2.80952381]),\n",
       "   'Implied Volatility': array([0.2972, 0.3422, 0.3274, 0.2407, 0.2801, 0.3   , 0.2761, 0.3   ,\n",
       "          0.2462, 0.3484, 0.2759, 0.3657, 0.2814, 0.3513, 0.3514, 0.3343,\n",
       "          0.2852, 0.3428, 0.3731, 0.3584, 0.3274, 0.264 , 0.2517, 0.3325,\n",
       "          0.2783, 0.2517, 0.3933, 0.2771, 0.3065, 0.2674])}},\n",
       " {'Datetime': Timestamp('2013-05-29 00:00:00'),\n",
       "  'Symbol': 'AAPL',\n",
       "  'Market Features': {'Market Return': -0.0070729362431658,\n",
       "   'Market Volatility': 14.829999923706056,\n",
       "   'Treasury Rate': 0.0399999991059303},\n",
       "  'Surface': {'Log Moneyness': array([ 0.59874585,  0.61594846,  0.21119557,  0.81626206,  0.75495469,\n",
       "          -0.30521856,  0.60197725,  0.30006163,  0.91219072,  0.52852705,\n",
       "           0.13304315,  0.37170993,  0.25009547,  0.83656648,  0.30152691,\n",
       "           0.01469842,  0.07419761,  0.20936284,  0.49189466,  0.80442231,\n",
       "           0.77543035,  0.78165601,  0.20134113,  0.15444285, -0.36342003,\n",
       "          -0.02434216,  0.78793924, -0.53323297,  0.24073855,  0.86060193]),\n",
       "   'Time to Maturity': array([0.92857143, 0.20634921, 0.0952381 , 0.20634921, 2.37301587,\n",
       "          0.92857143, 0.20634921, 0.20634921, 0.0952381 , 1.28968254,\n",
       "          0.45634921, 0.92857143, 0.31746032, 0.92857143, 2.37301587,\n",
       "          0.20634921, 0.92857143, 0.31746032, 0.92857143, 0.92857143,\n",
       "          2.37301587, 0.20634921, 0.00793651, 0.56746032, 0.56746032,\n",
       "          0.06349206, 0.92857143, 0.0952381 , 1.28968254, 0.20634921]),\n",
       "   'Implied Volatility': array([0.2791, 0.2596, 0.2626, 0.2596, 0.3043, 0.4321, 0.3296, 0.2669,\n",
       "          0.3799, 0.2781, 0.2743, 0.2744, 0.2827, 0.313 , 0.2954, 0.2916,\n",
       "          0.2775, 0.2796, 0.2791, 0.313 , 0.3043, 0.2596, 0.291 , 0.2656,\n",
       "          0.3336, 0.3713, 0.2791, 0.3277, 0.2731, 0.6324])}},\n",
       " {'Datetime': Timestamp('2013-06-18 00:00:00'),\n",
       "  'Symbol': 'AAPL',\n",
       "  'Market Features': {'Market Return': 0.0077609634287832,\n",
       "   'Market Volatility': 16.610000610351562,\n",
       "   'Treasury Rate': 0.0450000017881393},\n",
       "  'Surface': {'Log Moneyness': array([ 0.42598275,  0.55003119,  0.33650807,  0.11826588,  0.35672299,\n",
       "           0.67741321, -0.24603565,  0.443058  ,  0.7402747 ,  0.31140556,\n",
       "           0.87349953,  0.86997923, -0.20949811, -0.00767583,  0.57058134,\n",
       "          -0.30608886, -0.30427594,  0.53199347, -0.16856925,  0.6135692 ,\n",
       "           0.1294655 ,  0.3418996 , -0.115829  ,  0.37707522,  0.59285205,\n",
       "           0.77080836,  0.47533773,  0.9352097 ,  0.71236852,  0.0574552 ]),\n",
       "   'Time to Maturity': array([0.37698413, 0.84920635, 0.37698413, 2.29365079, 0.84920635,\n",
       "          0.48809524, 0.48809524, 1.21031746, 0.12698413, 0.12698413,\n",
       "          2.29365079, 0.84920635, 0.84920635, 0.23809524, 0.01587302,\n",
       "          1.21031746, 0.01587302, 0.01587302, 0.01587302, 0.84920635,\n",
       "          0.23809524, 0.01587302, 1.21031746, 1.21031746, 0.84920635,\n",
       "          0.84920635, 0.01587302, 0.12698413, 2.29365079, 0.06746032]),\n",
       "   'Implied Volatility': array([0.2936, 0.2866, 0.2796, 0.3027, 0.264 , 0.3106, 0.4412, 0.2742,\n",
       "          0.2355, 0.2355, 0.3283, 0.3116, 0.3113, 0.3527, 0.2414, 0.4152,\n",
       "          0.2322, 0.2414, 0.2322, 0.3001, 0.2839, 0.2414, 0.3146, 0.2781,\n",
       "          0.2957, 0.3142, 0.2322, 0.2355, 0.3276, 0.2702])}},\n",
       " {'Datetime': Timestamp('2013-02-28 00:00:00'),\n",
       "  'Symbol': 'AAPL',\n",
       "  'Market Features': {'Market Return': -0.0008644534614572,\n",
       "   'Market Volatility': 15.510000228881836,\n",
       "   'Treasury Rate': 0.1000000014901161},\n",
       "  'Surface': {'Log Moneyness': array([ 0.67256106,  0.50930021,  0.7090175 ,  0.84367124,  0.21474529,\n",
       "          -0.44113284, -0.31581901,  0.56097737, -0.4605769 ,  0.38374198,\n",
       "           0.62782268, -0.28614343,  0.30157245,  0.34450184,  0.33629323,\n",
       "          -0.20607347,  0.30777313,  0.63783755,  0.50592798,  0.67328639,\n",
       "           0.75572618, -0.10845196,  0.00265978,  0.61653622,  0.18350703,\n",
       "           0.00343986,  0.70181353,  0.74936464,  0.08677003,  0.47062725]),\n",
       "   'Time to Maturity': array([0.56349206, 0.31349206, 2.73015873, 0.20238095, 0.56349206,\n",
       "          1.28571429, 1.28571429, 0.31349206, 1.28571429, 0.20238095,\n",
       "          1.28571429, 0.56349206, 0.20238095, 0.03174603, 0.56349206,\n",
       "          1.28571429, 0.31349206, 0.92460317, 0.06349206, 0.92460317,\n",
       "          0.06349206, 1.28571429, 2.73015873, 0.20238095, 0.00396825,\n",
       "          0.03174603, 0.92460317, 0.56349206, 0.56349206, 0.31349206]),\n",
       "   'Implied Volatility': array([0.2687, 0.3489, 0.3118, 0.259 , 0.2743, 0.5199, 0.3539, 0.3738,\n",
       "          0.3715, 0.259 , 0.2883, 0.3731, 0.2605, 0.4785, 0.2721, 0.3333,\n",
       "          0.2914, 0.2673, 0.2354, 0.2673, 0.2354, 0.3191, 0.3196, 0.3735,\n",
       "          0.6048, 0.4164, 0.2673, 0.2687, 0.2937, 0.329 ])}},\n",
       " {'Datetime': Timestamp('2013-01-23 00:00:00'),\n",
       "  'Symbol': 'GOOGL',\n",
       "  'Market Features': {'Market Return': 0.0015063419242931,\n",
       "   'Market Volatility': 12.460000038146973,\n",
       "   'Treasury Rate': 0.0700000002980232},\n",
       "  'Surface': {'Log Moneyness': array([-0.09068206,  0.13226313,  0.14906578,  0.33473205,  0.10358855,\n",
       "          -0.37741972,  0.31558318,  0.17184675,  0.35335495, -0.43242832,\n",
       "          -0.20229491, -0.16743225, -0.43806422,  0.0636009 , -0.04820092,\n",
       "          -0.60588145,  0.12997146,  0.12501218, -0.0800914 , -0.63249869,\n",
       "          -0.44136984,  0.1684041 ,  0.23283925, -0.21049515,  0.1426415 ,\n",
       "           0.13789674,  0.10884791, -0.74785902,  0.25772933,  0.2211996 ]),\n",
       "   'Time to Maturity': array([0.3452381 , 0.5952381 , 0.5952381 , 0.20634921, 0.5952381 ,\n",
       "          0.5952381 , 0.20634921, 1.42857143, 0.5952381 , 1.42857143,\n",
       "          0.5952381 , 0.20634921, 0.20634921, 0.20634921, 0.3452381 ,\n",
       "          1.42857143, 0.20634921, 1.42857143, 0.95634921, 0.20634921,\n",
       "          0.0952381 , 0.3452381 , 0.3452381 , 0.00793651, 0.0952381 ,\n",
       "          0.5952381 , 1.42857143, 0.5952381 , 0.0952381 , 1.42857143]),\n",
       "   'Implied Volatility': array([0.239 , 0.1869, 0.1902, 0.1825, 0.1906, 0.273 , 0.1505, 0.2112,\n",
       "          0.183 , 0.2974, 0.244 , 0.2521, 0.2299, 0.1618, 0.2301, 0.2978,\n",
       "          0.1669, 0.2179, 0.2265, 0.2539, 0.2502, 0.1802, 0.1802, 0.2122,\n",
       "          0.1476, 0.1905, 0.2164, 0.273 , 0.1753, 0.2106])}},\n",
       " {'Datetime': Timestamp('2013-04-10 00:00:00'),\n",
       "  'Symbol': 'GOOGL',\n",
       "  'Market Features': {'Market Return': 0.0121154433295488,\n",
       "   'Market Volatility': 12.359999656677246,\n",
       "   'Treasury Rate': 0.0649999976158142},\n",
       "  'Surface': {'Log Moneyness': array([-0.02428587,  0.0434598 , -0.5781919 , -0.59014662,  0.23235412,\n",
       "           0.32729923,  0.19293324, -0.41917651,  0.03816177,  0.10836427,\n",
       "           0.22712981, -0.57161895,  0.12003796,  0.140976  ,  0.27476522,\n",
       "          -0.47552827,  0.29218107, -0.4731361 , -0.07763147,  0.09896039,\n",
       "           0.35289661, -0.7164482 ,  0.05404717,  0.0849979 ,  0.17757473,\n",
       "           0.08535126,  0.11940966, -0.22400077,  0.01341093, -0.07274782]),\n",
       "   'Time to Maturity': array([0.28968254, 0.28968254, 2.56746032, 0.15079365, 0.28968254,\n",
       "          0.65079365, 0.65079365, 0.65079365, 0.00793651, 0.06349206,\n",
       "          0.65079365, 0.03968254, 0.00793651, 0.15079365, 1.12301587,\n",
       "          0.03968254, 1.12301587, 1.12301587, 0.09126984, 0.65079365,\n",
       "          0.65079365, 2.56746032, 0.65079365, 0.15079365, 1.12301587,\n",
       "          1.12301587, 0.09126984, 0.65079365, 0.09126984, 1.12301587]),\n",
       "   'Implied Volatility': array([0.2237, 0.212 , 0.3046, 0.2667, 0.2005, 0.1967, 0.1932, 0.267 ,\n",
       "          0.2385, 0.3168, 0.1702, 0.4345, 0.2455, 0.2207, 0.1944, 0.4345,\n",
       "          0.2093, 0.289 , 0.3302, 0.203 , 0.1891, 0.3046, 0.2081, 0.2358,\n",
       "          0.1991, 0.2057, 0.2688, 0.2649, 0.2975, 0.2295])}},\n",
       " {'Datetime': Timestamp('2013-05-21 00:00:00'),\n",
       "  'Symbol': 'GOOGL',\n",
       "  'Market Features': {'Market Return': 0.0017209046830023,\n",
       "   'Market Volatility': 13.369999885559082,\n",
       "   'Treasury Rate': 0.0399999991059303},\n",
       "  'Surface': {'Log Moneyness': array([-1.08052339,  0.16328589,  0.19312913, -0.66362131, -0.10231869,\n",
       "          -0.18835817, -0.14960828, -0.0404159 ,  0.08752586,  0.00958413,\n",
       "           0.08829205,  0.02071823, -0.97255038,  0.00441169, -0.55409569,\n",
       "          -0.17538046, -0.61297567, -0.34577748, -0.01214268,  0.13290277,\n",
       "           0.11420245, -0.2506572 , -0.49153914, -0.57489384, -0.76589053,\n",
       "          -0.61437859, -0.55925243, -0.24844337, -0.83980505, -0.13434257]),\n",
       "   'Time to Maturity': array([0.96031746, 0.48809524, 0.12698413, 0.12698413, 0.84920635,\n",
       "          0.96031746, 0.01190476, 0.03968254, 0.34920635, 0.96031746,\n",
       "          0.12698413, 0.12698413, 0.96031746, 0.34920635, 0.48809524,\n",
       "          0.03968254, 0.48809524, 0.12698413, 0.01190476, 0.96031746,\n",
       "          0.96031746, 0.01190476, 0.84920635, 0.84920635, 0.48809524,\n",
       "          0.84920635, 2.4047619 , 0.23809524, 2.4047619 , 0.48809524]),\n",
       "   'Implied Volatility': array([0.288 , 0.2149, 0.2025, 0.3002, 0.2358, 0.247 , 0.3635, 0.2289,\n",
       "          0.2145, 0.2215, 0.2017, 0.1913, 0.279 , 0.2256, 0.2785, 0.3088,\n",
       "          0.2875, 0.2308, 0.2744, 0.214 , 0.2155, 0.3635, 0.2964, 0.2964,\n",
       "          0.2875, 0.2892, 0.2983, 0.2925, 0.2983, 0.2427])}},\n",
       " {'Datetime': Timestamp('2013-04-19 00:00:00'),\n",
       "  'Symbol': 'AAPL',\n",
       "  'Market Features': {'Market Return': 0.0088089890510547,\n",
       "   'Market Volatility': 14.970000267028809,\n",
       "   'Treasury Rate': 0.0450000017881393},\n",
       "  'Surface': {'Log Moneyness': array([ 0.86381564,  0.39021527,  0.52191287,  0.87617451,  0.25816946,\n",
       "           0.85446226,  0.79327765,  0.57336708,  0.07165427,  0.20065239,\n",
       "          -0.06155099,  0.70237328,  0.37781424,  0.89860537,  0.49013405,\n",
       "           0.04360548,  0.75427074, -0.12727674,  0.05777278,  1.06283608,\n",
       "           0.29389325,  0.69653497,  1.05766901,  0.13585677,  0.16377972,\n",
       "           0.7968411 ,  1.05616887,  0.2597658 ,  0.72922767,  0.7807682 ]),\n",
       "   'Time to Maturity': array([2.53174603, 0.00396825, 0.02777778, 1.08730159, 0.25396825,\n",
       "          0.25396825, 0.72619048, 0.36507937, 0.05555556, 0.13888889,\n",
       "          0.05555556, 1.08730159, 0.13888889, 1.08730159, 0.25396825,\n",
       "          0.02777778, 0.72619048, 0.00396825, 0.02777778, 0.00396825,\n",
       "          0.08333333, 0.36507937, 0.00396825, 2.53174603, 0.11507937,\n",
       "          0.25396825, 0.36507937, 1.08730159, 1.08730159, 1.08730159]),\n",
       "   'Implied Volatility': array([0.3464, 0.4939, 0.8129, 0.3452, 0.3561, 0.3685, 0.3301, 0.3526,\n",
       "          0.5521, 0.4107, 0.5643, 0.3347, 0.3673, 0.3452, 0.3696, 0.726 ,\n",
       "          0.3096, 0.4939, 0.7463, 1.3463, 0.4782, 0.3561, 0.4939, 0.3353,\n",
       "          0.4349, 0.3685, 0.3561, 0.3293, 0.3365, 0.3332])}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implied_volatility_surfaces(aapl_googl_data, toy_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.4990990990991"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fine_tune_data) / len(implied_volatility_surfaces(fine_tune_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(implied_volatility_surfaces(fine_tune_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-02-11 00:00:00'),\n",
       "  Timestamp('2013-01-23 00:00:00'),\n",
       "  Timestamp('2013-06-05 00:00:00'),\n",
       "  Timestamp('2013-04-05 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'TLT', 'TLT', 'TLT'],\n",
       " 'Mask Proportion': [0.7, 0.1, 0.3, 0.3],\n",
       " 'Market Features': {'Market Return': tensor([-0.0006,  0.0015, -0.0139, -0.0043]),\n",
       "  'Market Volatility': tensor([12.9400, 12.4600, 17.5000, 13.9200]),\n",
       "  'Treasury Rate': tensor([0.0650, 0.0700, 0.0450, 0.0600]),\n",
       "  'TV Mean': tensor([0.0502, 0.0064, 0.0199, 0.0201]),\n",
       "  'TV Std.': tensor([0.0417, 0.0055, 0.0169, 0.0469])},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-0.1166,  0.3898,  0.8528, -0.0165,  0.4024,  0.3299,  0.2666,  0.3381]),\n",
       "   tensor([0.4634, 0.2972, 0.7231, 0.3096, 0.4714, 0.1731, 0.1744, 0.3576, 0.4301,\n",
       "           0.2747, 0.4230, 0.5337, 0.0786, 0.3694, 0.4766, 0.3418, 0.3339, 0.1761,\n",
       "           0.4656, 0.1548]),\n",
       "   tensor([ 0.0462,  0.3458,  0.4260,  0.2837,  0.3436, -0.2642, -0.1673,  0.3609,\n",
       "            0.1697,  0.4517,  0.0675,  0.3353,  0.6026, -0.0699,  0.0346,  0.6048,\n",
       "            0.3220,  0.4330,  0.5161]),\n",
       "   tensor([ 0.0197, -0.2021,  0.3679,  0.0294,  0.2808,  0.2844,  0.4227,  0.3169,\n",
       "           -0.1321,  0.5663,  0.3360,  0.4248,  0.3276, -0.0230,  0.1528,  0.3314,\n",
       "            0.2740,  0.5045,  0.1922])],\n",
       "  'Time to Maturity': [tensor([0.5198, 0.1310, 0.6310, 0.0198, 0.2698, 0.9921, 0.2698, 1.3532]),\n",
       "   tensor([0.5952, 0.0079, 0.5952, 0.0079, 0.9563, 0.4563, 0.3452, 0.0079, 0.3452,\n",
       "           0.0952, 0.0952, 0.5952, 0.3452, 0.4563, 0.5952, 0.0079, 0.0079, 0.2063,\n",
       "           0.0952, 0.4563]),\n",
       "   tensor([0.0675, 1.1508, 1.1508, 0.4286, 0.7897, 0.9008, 0.9008, 1.1508, 0.0675,\n",
       "           0.9008, 0.1786, 0.0079, 0.0675, 0.0675, 0.4286, 0.9008, 0.0675, 1.1508,\n",
       "           0.4286]),\n",
       "   tensor([0.3095, 0.3095, 0.3095, 0.6706, 0.0000, 0.0278, 0.6706, 1.0317, 1.1429,\n",
       "           0.3095, 0.0278, 0.3095, 0.0595, 0.3095, 0.6706, 1.0317, 0.4206, 0.3095,\n",
       "           1.0317])],\n",
       "  'Total Variance': [tensor([0.0505, 0.0136, 0.0640, 0.0030, 0.0244, 0.0916, 0.0213, 0.1335]),\n",
       "   tensor([0.0110, 0.0002, 0.0108, 0.0003, 0.0222, 0.0100, 0.0073, 0.0003, 0.0056,\n",
       "           0.0015, 0.0022, 0.0108, 0.0073, 0.0081, 0.0110, 0.0002, 0.0002, 0.0079,\n",
       "           0.0024, 0.0087]),\n",
       "   tensor([0.0033, 0.0400, 0.0520, 0.0112, 0.0259, 0.0442, 0.0442, 0.0418, 0.0032,\n",
       "           0.0220, 0.0051, 0.0012, 0.0024, 0.0033, 0.0148, 0.0276, 0.0024, 0.0242,\n",
       "           0.0097]),\n",
       "   tensor([0.0097, 0.0097, 0.0041, 0.0173, 0.0000, 0.0004, 0.0117, 0.0188, 0.2170,\n",
       "           0.0041, 0.0005, 0.0058, 0.0010, 0.0097, 0.0139, 0.0188, 0.0069, 0.0058,\n",
       "           0.0265])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([-0.4047], requires_grad=True),\n",
       "   tensor([0.1560], requires_grad=True),\n",
       "   tensor([0.5376], requires_grad=True),\n",
       "   tensor([0.2970], requires_grad=True)],\n",
       "  'Time to Maturity': [tensor([1.3532], requires_grad=True),\n",
       "   tensor([0.3452], requires_grad=True),\n",
       "   tensor([0.9008], requires_grad=True),\n",
       "   tensor([0.], requires_grad=True)],\n",
       "  'Total Variance': [tensor([0.0753]),\n",
       "   tensor([0.0073]),\n",
       "   tensor([0.0436]),\n",
       "   tensor([0.])]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "class IVSurfaceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data, \n",
    "        mask_proportions, \n",
    "        random_state=0,\n",
    "        n_query_points=None\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.mask_proportions = mask_proportions\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        self.n_query_points = n_query_points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        surface_data = self.data[idx]\n",
    "        \n",
    "        # Extract the surface coordinates and volatilities\n",
    "        points_coordinates = np.stack([\n",
    "            surface_data['Surface']['Log Moneyness'], \n",
    "            surface_data['Surface']['Time to Maturity']\n",
    "        ], axis=1)\n",
    "        points_volatilities = surface_data['Surface']['Implied Volatility']\n",
    "\n",
    "        # Select a random mask proportion\n",
    "        proportion = self.rng.choice(self.mask_proportions)\n",
    "\n",
    "        # Perform clustering\n",
    "        n_clusters = int(np.ceil(1 / proportion))\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('kmeans', KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init='auto'))\n",
    "        ])\n",
    "        labels = pipeline.fit_predict(points_coordinates)\n",
    "        masked_indices = []\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_indices = np.where(labels == cluster)[0]\n",
    "            num_to_mask = int(np.ceil(len(cluster_indices) * proportion))\n",
    "            masked_indices.extend(self.rng.choice(cluster_indices, size=num_to_mask, replace=False))\n",
    "        \n",
    "        masked_indices = np.array(masked_indices)\n",
    "        unmasked_indices = np.setdiff1d(range(len(labels)), masked_indices)\n",
    "\n",
    "        # Calculate Total Variance mean and std for unmasked points\n",
    "        time_to_maturity_unmasked = points_coordinates[unmasked_indices, 1]\n",
    "        total_variance_unmasked = time_to_maturity_unmasked * np.square(points_volatilities[unmasked_indices])\n",
    "\n",
    "        tv_mean = np.mean(total_variance_unmasked)\n",
    "        tv_std = np.std(total_variance_unmasked)\n",
    "\n",
    "        # Define query indices based on n_query_points\n",
    "        if self.n_query_points is None:\n",
    "            query_indices = masked_indices\n",
    "        else:\n",
    "            query_indices = self.rng.choice(masked_indices, size=self.n_query_points, replace=False)\n",
    "\n",
    "        time_to_maturity_query = points_coordinates[query_indices, 1]\n",
    "        total_variance_query = time_to_maturity_query * np.square(points_volatilities[query_indices])    \n",
    "            \n",
    "        data_item = {\n",
    "            'Datetime': surface_data['Datetime'],\n",
    "            'Symbol': surface_data['Symbol'],\n",
    "            'Mask Proportion': proportion,\n",
    "            'Market Features': {\n",
    "                'Market Return': torch.tensor(surface_data['Market Features']['Market Return'], dtype=torch.float32),\n",
    "                'Market Volatility': torch.tensor(surface_data['Market Features']['Market Volatility'], dtype=torch.float32),\n",
    "                'Treasury Rate': torch.tensor(surface_data['Market Features']['Treasury Rate'], dtype=torch.float32),\n",
    "                'TV Mean': torch.tensor(tv_mean, dtype=torch.float32),  \n",
    "                'TV Std.': torch.tensor(tv_std, dtype=torch.float32),  \n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[unmasked_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(time_to_maturity_unmasked, dtype=torch.float32),\n",
    "                'Total Variance': torch.tensor(total_variance_unmasked, dtype=torch.float32)\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[query_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(time_to_maturity_query, dtype=torch.float32),\n",
    "                'Total Variance': torch.tensor(total_variance_query, dtype=torch.float32)  \n",
    "            }\n",
    "        }\n",
    "\n",
    "        return data_item\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batched_data = {\n",
    "            'Datetime': [item['Datetime'] for item in batch],\n",
    "            'Symbol': [item['Symbol'] for item in batch],\n",
    "            'Mask Proportion': [item['Mask Proportion'] for item in batch],\n",
    "            'Market Features': {\n",
    "                'Market Return': default_collate([item['Market Features']['Market Return'] for item in batch]),\n",
    "                'Market Volatility': default_collate([item['Market Features']['Market Volatility'] for item in batch]),\n",
    "                'Treasury Rate': default_collate([item['Market Features']['Treasury Rate'] for item in batch]),\n",
    "                'TV Mean': default_collate([item['Market Features']['TV Mean'] for item in batch]),\n",
    "                'TV Std.': default_collate([item['Market Features']['TV Std.'] for item in batch]),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': [item['Input Surface']['Log Moneyness'] for item in batch],\n",
    "                'Time to Maturity': [item['Input Surface']['Time to Maturity'] for item in batch],\n",
    "                'Total Variance': [item['Input Surface']['Total Variance'] for item in batch],\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': [item['Query Points']['Log Moneyness'].requires_grad_(True) for item in batch],\n",
    "                'Time to Maturity': [item['Query Points']['Time to Maturity'].requires_grad_(True) for item in batch],\n",
    "                'Total Variance': [item['Query Points']['Total Variance'] for item in batch],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "# Assuming surfaces is the output from the implied_volatility_surfaces function\n",
    "mask_proportions = HYPERPARAMETERS['Input Preprocessing']['Mask Proportions']  \n",
    "n_query_points = HYPERPARAMETERS['Input Preprocessing']['Number of Query Points']  \n",
    "dataset = IVSurfaceDataset(surfaces, mask_proportions, RANDOM_STATE, n_query_points)\n",
    "data_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=HYPERPARAMETERS['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Fetch one batch from the DataLoader\n",
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "class SurfaceBatchNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features=1, \n",
    "        momentum=0.1\n",
    "    ):\n",
    "        super(SurfaceBatchNorm, self).__init__()\n",
    "        self.log_moneyness_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.time_to_maturity_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_return_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_volatility_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.treasury_rate_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.tv_mean_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.tv_std_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Concatenate all tensors from the Input Surface into one tensor for each feature\n",
    "        input_surface_log_moneyness = torch.cat([x for x in batch['Input Surface']['Log Moneyness']])\n",
    "        input_surface_time_to_maturity = torch.cat([x for x in batch['Input Surface']['Time to Maturity']])\n",
    "\n",
    "        # Concatenate Input Surface tensors with Query Points tensors\n",
    "        total_log_moneyness = torch.cat([input_surface_log_moneyness] + [x for x in batch['Query Points']['Log Moneyness']])\n",
    "        total_time_to_maturity = torch.cat([input_surface_time_to_maturity] + [x for x in batch['Query Points']['Time to Maturity']])\n",
    "\n",
    "        # Normalize Log Moneyness and Time to Maturity\n",
    "        norm_log_moneyness = self.log_moneyness_bn(total_log_moneyness.unsqueeze(1)).squeeze(1)\n",
    "        norm_time_to_maturity = self.time_to_maturity_bn(total_time_to_maturity.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Split the normalized results back to corresponding structures\n",
    "        input_surface_sizes = [len(x) for x in batch['Input Surface']['Log Moneyness']]\n",
    "        query_points_sizes = [len(x) for x in batch['Query Points']['Log Moneyness']]\n",
    "        total_input_size = sum(input_surface_sizes)\n",
    "\n",
    "        # Normalizing Market Features\n",
    "        market_features = batch['Market Features']\n",
    "        norm_market_return = self.market_return_bn(market_features['Market Return'].unsqueeze(1)).squeeze(1)\n",
    "        norm_market_volatility = self.market_volatility_bn(market_features['Market Volatility'].unsqueeze(1)).squeeze(1)\n",
    "        norm_treasury_rate = self.treasury_rate_bn(market_features['Treasury Rate'].unsqueeze(1)).squeeze(1)\n",
    "        norm_tv_mean = self.tv_mean_bn(market_features['TV Mean'].unsqueeze(1)).squeeze(1)\n",
    "        norm_tv_std = self.tv_std_bn(market_features['TV Std.'].unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Reconstructing the batch with normalized data\n",
    "        output = {\n",
    "            'Datetime': batch['Datetime'],\n",
    "            'Symbol': batch['Symbol'],\n",
    "            'Mask Proportion': batch['Mask Proportion'],\n",
    "            'Market Features': {\n",
    "                'Market Return': norm_market_return,\n",
    "                'Market Volatility': norm_market_volatility,\n",
    "                'Treasury Rate': norm_treasury_rate,\n",
    "                'TV Mean': norm_tv_mean,\n",
    "                'TV Std.': norm_tv_std\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[:total_input_size], input_surface_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[:total_input_size], input_surface_sizes)),\n",
    "                'Total Variance': batch['Input Surface']['Total Variance']\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[total_input_size:], query_points_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[total_input_size:], query_points_sizes)),\n",
    "                'Total Variance': batch['Query Points']['Total Variance']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return output\n",
    "\n",
    "# Usage\n",
    "# surfacebatchnorm = SurfaceBatchNorm()\n",
    "# processed_batch = surfacebatchnorm(batch)\n",
    "# processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class EllipticalRBFKernel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        bandwidth, \n",
    "        remove_kernel=False,\n",
    "        epsilon=1e-10\n",
    "    ):\n",
    "        super(EllipticalRBFKernel, self).__init__()\n",
    "        if remove_kernel:\n",
    "            self.bandwidth = epsilon\n",
    "        else:\n",
    "            self.bandwidth = bandwidth\n",
    "        # Initialize the log of the scale vector to zero, which corresponds to scale factors of one\n",
    "        self.log_scale = nn.Parameter(torch.zeros(input_dim))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, distances):\n",
    "        # Convert log scale to actual scale values\n",
    "        scale = torch.exp(self.log_scale)\n",
    "        \n",
    "        # Calculate the scaled distances\n",
    "        scaled_distances = (distances ** 2) * scale  # Element-wise multiplication by scale\n",
    "\n",
    "        # Normalize by the trace of the scale matrix\n",
    "        trace_scale_matrix = torch.sum(scale)\n",
    "        normalized_distances = torch.sum(scaled_distances, dim=-1) / trace_scale_matrix \n",
    "\n",
    "        # Compute the RBF kernel output using the normalized distances\n",
    "        kernel_values = torch.exp(-normalized_distances / (2 * self.bandwidth ** 2)) + self.epsilon\n",
    "\n",
    "        return kernel_values\n",
    "\n",
    "class SurfaceContinuousKernelPositionalEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceContinuousKernelPositionalEmbedding, self).__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "        self.remove_positional_embedding = remove_positional_embedding\n",
    "\n",
    "        # Initialize multiple RBF kernels, each with a different fixed bandwidth\n",
    "        self.kernels = nn.ModuleList()\n",
    "        for i in range(1, d_embedding + 1):\n",
    "            bandwidth_value = torch.erfinv(torch.tensor(i / (d_embedding + 1))) * np.sqrt(2)\n",
    "            self.kernels.append(\n",
    "                EllipticalRBFKernel(\n",
    "                    bandwidth=bandwidth_value, \n",
    "                    input_dim=2, \n",
    "                    remove_kernel=remove_kernel\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.input_surface_layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.query_points_layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "        # Initialize learnable scaling parameter (the base for positional embedding)\n",
    "        self.log_scale = nn.Parameter(torch.log(torch.tensor(10000.0)))\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_surface_batch, \n",
    "        query_points_batch\n",
    "    ):\n",
    "        batch_size = len(input_surface_batch['Log Moneyness'])\n",
    "\n",
    "        input_surface_embeddings = []\n",
    "        query_points_embeddings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Extract the coordinates and implied volatilities for each surface in the batch\n",
    "            surface_coords = torch.stack([\n",
    "                input_surface_batch['Log Moneyness'][i], \n",
    "                input_surface_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "            surface_tvs = input_surface_batch['Total Variance'][i]\n",
    "\n",
    "            query_coords = torch.stack([\n",
    "                query_points_batch['Log Moneyness'][i], \n",
    "                query_points_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "\n",
    "            all_coords = torch.cat((surface_coords, query_coords), dim=0)\n",
    "\n",
    "            # Compute the pairwise differences between all points and the input surface points\n",
    "            point_differences = all_coords.unsqueeze(1) - surface_coords.unsqueeze(0)  # (n+m, n, 2)\n",
    "\n",
    "            # Initialize the output embeddings for the current surface with d_embedding channels\n",
    "            all_embedded = torch.zeros((all_coords.shape[0], self.d_embedding), dtype=torch.float32, device=surface_coords.device)\n",
    "\n",
    "            for kernel_idx, kernel in enumerate(self.kernels):\n",
    "                # Apply the RBF kernel to each distance vector \n",
    "                kernel_outputs = kernel(point_differences)\n",
    "\n",
    "                # Compute the weighted sum of TVs based on the kernel outputs\n",
    "                weighted_sum = (kernel_outputs * surface_tvs.unsqueeze(0)).sum(dim=1)\n",
    "                normalization_factor = kernel_outputs.sum(dim=1)\n",
    "\n",
    "                all_embedded[:, kernel_idx] = weighted_sum / normalization_factor   \n",
    "                # if torch.isnan(all_embedded[:, kernel_idx]).any():\n",
    "                #     print('Kernel: ', kernel.bandwidth)\n",
    "                #     print('NAN Embedding: ', all_embedded[:, kernel_idx], weighted_sum, normalization_factor)\n",
    "                #     print('NAN Embedding Source: ', kernel_outputs, surface_tvs)\n",
    "\n",
    "            # Split the embeddings into input surface and query points embeddings\n",
    "            input_surface_embedded = all_embedded[:surface_coords.shape[0], :]\n",
    "            query_points_embedded = all_embedded[surface_coords.shape[0]:, :]\n",
    "\n",
    "            # Normalize the embedded surfaces\n",
    "            input_surface_embedded = self.input_surface_layer_norm(input_surface_embedded)\n",
    "            query_points_embedded = self.query_points_layer_norm(query_points_embedded)\n",
    "\n",
    "            # Positional embedding for input surface points\n",
    "            input_surface_pe = self._compute_positional_embedding(surface_coords)\n",
    "\n",
    "            # Positional embedding for query points\n",
    "            query_points_pe = self._compute_positional_embedding(query_coords)\n",
    "\n",
    "            # Add positional embeddings with a factor of sqrt(2)\n",
    "            input_surface_final = input_surface_embedded + input_surface_pe * np.sqrt(2)\n",
    "            query_points_final = query_points_embedded + query_points_pe * np.sqrt(2)\n",
    "\n",
    "            # Append the encoded surface for this input surface to the batch list\n",
    "            input_surface_embeddings.append(input_surface_final)\n",
    "            query_points_embeddings.append(query_points_final)\n",
    "\n",
    "        # Keep all encoded surfaces as lists to handle variable lengths\n",
    "        return {\n",
    "            'Input Surface': input_surface_embeddings,\n",
    "            'Query Points': query_points_embeddings\n",
    "        }\n",
    "\n",
    "    def _compute_positional_embedding(\n",
    "        self, \n",
    "        coords, \n",
    "    ):\n",
    "        positional_embedding = torch.zeros(coords.size(0), self.d_embedding, device=coords.device)\n",
    "\n",
    "        if not self.remove_positional_embedding:\n",
    "            for i in range(self.d_embedding // 4):\n",
    "                div_factor = torch.exp(self.log_scale) ** (4 * i / self.d_embedding)\n",
    "                positional_embedding[:, 4 * i] = torch.sin(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 1] = torch.cos(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 2] = torch.sin(coords[:, 1] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 3] = torch.cos(coords[:, 1] / div_factor)\n",
    "\n",
    "        return positional_embedding\n",
    "\n",
    "# Example of initializing and using this module\n",
    "# d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "\n",
    "# continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "# kernel_positional_embedded_batch = continuous_kernel_positional_embedding(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "# kernel_positional_embedded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SurfaceEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        momentum=0.1,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceEmbedding, self).__init__()\n",
    "        self.batch_norm = SurfaceBatchNorm(num_features=1, momentum=momentum)\n",
    "        self.kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding, remove_kernel, remove_positional_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.mask_token = nn.Parameter(torch.randn(d_embedding))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Apply batch normalization\n",
    "        norm_batch = self.batch_norm(batch)\n",
    "\n",
    "        # Extract market features from processed batch and create external_features_batch tensor\n",
    "        market_features = norm_batch['Market Features']\n",
    "        external_features_batch = torch.stack([\n",
    "            market_features['Market Return'],\n",
    "            market_features['Market Volatility'],\n",
    "            market_features['Treasury Rate'],\n",
    "            market_features['TV Mean'],\n",
    "            market_features['TV Std.']\n",
    "        ], dim=-1)  # (batch, features)\n",
    "\n",
    "        # Compute kernel and positional embeddings\n",
    "        embeddings = self.kernel_positional_embedding(norm_batch['Input Surface'], norm_batch['Query Points'])\n",
    "        # print('internal embedding: ', embeddings)\n",
    "\n",
    "        input_surface_embeddings = embeddings['Input Surface']\n",
    "        query_points_embeddings = embeddings['Query Points']\n",
    "\n",
    "        embedded_sequences = []\n",
    "\n",
    "        for input_surface_embedding, query_points_embedding in zip(input_surface_embeddings, query_points_embeddings):\n",
    "            # Add mask token to the query point embeddings\n",
    "            masked_query_points_embedding = query_points_embedding + self.mask_token\n",
    "\n",
    "            # Combine input surface embeddings and masked query points embeddings\n",
    "            combined_sequence = torch.cat((input_surface_embedding, masked_query_points_embedding), dim=0)\n",
    "\n",
    "            # Apply layer normalization\n",
    "            combined_sequence = self.layer_norm(combined_sequence)\n",
    "\n",
    "            embedded_sequences.append(combined_sequence)\n",
    "\n",
    "        return embedded_sequences, external_features_batch\n",
    "\n",
    "\n",
    "# # Example of initializing and using this module\n",
    "# d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "# surface_embedding = SurfaceEmbedding(d_embedding=d_embedding)\n",
    "# embedded_sequences_batch, external_features_batch = surface_embedding(batch)\n",
    "# embedded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualNorm(nn.Module):\n",
    "    def __init__(self, d_embedding):\n",
    "        super(ResidualNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x, \n",
    "        sublayer_output\n",
    "    ):\n",
    "        return self.norm(x + sublayer_output)\n",
    "    \n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        gate_dropout,\n",
    "        weight_initializer_std=0.02,\n",
    "        bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(GatedAttentionFusion, self).__init__()\n",
    "        self.gate_layer = nn.Sequential(\n",
    "            nn.Linear(d_embedding * 2, d_embedding),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(gate_dropout)\n",
    "        )\n",
    "        self.remove_external_attention = remove_external_attention\n",
    "        self.remove_gate = remove_gate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for module in self.gate_layer:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        self_attention_output, \n",
    "        external_attention_output\n",
    "    ):\n",
    "        if self.remove_external_attention:\n",
    "\n",
    "            return self_attention_output\n",
    "\n",
    "        if self.remove_gate:  \n",
    "\n",
    "            return self_attention_output + external_attention_output\n",
    "        # Concatenate self-attention and external attention outputs\n",
    "        concatenated_output = torch.cat((self_attention_output, external_attention_output), dim=-1)\n",
    "        # Compute gate values\n",
    "        gate_values = self.gate_layer(concatenated_output)\n",
    "        # Calculate gated embedding\n",
    "        gated_embedding = gate_values * self_attention_output + (1 - gate_values) * external_attention_output\n",
    "\n",
    "        return gated_embedding\n",
    "    \n",
    "    \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        ffn_hidden_dim, \n",
    "        ffn_dropout, \n",
    "        layer_depth, \n",
    "        weight_initializer_std=0.02, \n",
    "        bias_initializer_value=0,\n",
    "    ):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_embedding, ffn_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ffn_dropout),\n",
    "            nn.Linear(ffn_hidden_dim, d_embedding),\n",
    "            nn.Dropout(ffn_dropout)\n",
    "        )\n",
    "\n",
    "        self.layer_depth = layer_depth\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feedforward(x)\n",
    "    \n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for i, module in enumerate(self.feedforward):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "                \n",
    "                # Rescale the output matrices of the last linear projection\n",
    "                if i == len(self.feedforward) - 2:\n",
    "                    scale_factor = 1 / (2 * self.layer_depth) ** 0.5\n",
    "                    module.weight.data *= scale_factor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        n_heads, \n",
    "        ffn_hidden_dim, \n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        layer_depth,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.residual_norm_self_attention = ResidualNorm(d_embedding)\n",
    "        self.external_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            kdim=external_dim, \n",
    "            vdim=external_dim, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.residual_norm_external_attention = ResidualNorm(d_embedding)\n",
    "        self.gated_attention_fusion = GatedAttentionFusion(\n",
    "            d_embedding, \n",
    "            gate_dropout,\n",
    "            weight_initializer_std,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention, \n",
    "            remove_gate,\n",
    "        )\n",
    "        self.residual_norm_fusion = ResidualNorm(d_embedding)\n",
    "        self.feed_forward = FeedForwardNetwork(\n",
    "            d_embedding, \n",
    "            ffn_hidden_dim, \n",
    "            ffn_dropout, \n",
    "            layer_depth, \n",
    "            weight_initializer_std, \n",
    "            linear_bias_initializer_value\n",
    "        )\n",
    "        self.residual_norm_ffn = ResidualNorm(d_embedding)\n",
    "        # Initialize self-attention\n",
    "        self._initialize_attention_weights(self.self_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "        # Initialize external-attention\n",
    "        self._initialize_attention_weights(self.external_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "\n",
    "    def _initialize_attention_weights(\n",
    "        self, \n",
    "        attention_module, \n",
    "        weight_initializer_std, \n",
    "        linear_bias_initializer_value, \n",
    "        layer_depth\n",
    "    ):\n",
    "        if attention_module._qkv_same_embed_dim:\n",
    "            nn.init.normal_(attention_module.in_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "        else:\n",
    "            nn.init.normal_(attention_module.q_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.k_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.v_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "\n",
    "        if attention_module.in_proj_bias is not None:\n",
    "            nn.init.constant_(attention_module.in_proj_bias, linear_bias_initializer_value)\n",
    "            nn.init.constant_(attention_module.out_proj.bias, linear_bias_initializer_value)\n",
    "        \n",
    "        if attention_module.bias_k is not None:\n",
    "            nn.init.constant_(attention_module.bias_k, linear_bias_initializer_value)\n",
    "        if attention_module.bias_v is not None:\n",
    "            nn.init.constant_(attention_module.bias_v, linear_bias_initializer_value)\n",
    "        \n",
    "        # Transformer layer rescaling for output weights\n",
    "        scale_factor = 1 / (2 * layer_depth) ** 0.5\n",
    "        nn.init.normal_(attention_module.out_proj.weight, mean=0.0, std=weight_initializer_std * scale_factor)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        surface_embeddings, \n",
    "        external_features,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Self-Attention\n",
    "        self_attention_output, self_attention_weights = self.self_attention(surface_embeddings, surface_embeddings, surface_embeddings)\n",
    "        self_attention_output = self.residual_norm_self_attention(surface_embeddings, self_attention_output)\n",
    "        # External Attention\n",
    "        external_attention_output, external_attention_weights = self.external_attention(surface_embeddings, external_features, external_features) \n",
    "        external_attention_output = self.residual_norm_external_attention(surface_embeddings, external_attention_output)\n",
    "        # Gated Attention Fusion\n",
    "        gated_embedding = self.gated_attention_fusion(self_attention_output, external_attention_output)\n",
    "        gated_embedding = self.residual_norm_fusion(surface_embeddings, gated_embedding)\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.feed_forward(gated_embedding)\n",
    "        # Final Residual Connection and Layer Normalization\n",
    "        surface_embeddings = self.residual_norm_ffn(gated_embedding, ffn_output)\n",
    "\n",
    "        if output_attention_map:\n",
    "            # Remove the batch dimension for attention weights\n",
    "            return surface_embeddings, self_attention_weights.squeeze(0), external_attention_weights.squeeze(0)\n",
    "        \n",
    "        return surface_embeddings, None, None\n",
    "\n",
    "class SurfaceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(SurfaceEncoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            Encoder(\n",
    "                d_embedding, \n",
    "                n_heads, \n",
    "                ffn_hidden_dim, \n",
    "                attention_dropout, \n",
    "                gate_dropout,\n",
    "                ffn_dropout,\n",
    "                external_dim,\n",
    "                (i + 1),\n",
    "                weight_initializer_std,\n",
    "                linear_bias_initializer_value,\n",
    "                gate_bias_initializer_value,\n",
    "                remove_external_attention,\n",
    "                remove_gate\n",
    "            )\n",
    "            for i in range(num_encoder_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        embedded_sequences_batch, \n",
    "        external_features_batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        batch_size = len(embedded_sequences_batch)\n",
    "        encoded_sequences_batch = []\n",
    "        self_attention_maps = []\n",
    "        external_attention_maps = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            surface_embeddings = embedded_sequences_batch[i].unsqueeze(1) \n",
    "            external_features = external_features_batch[i].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            for j, encoder in enumerate(self.encoders):\n",
    "                if j == len(self.encoders) - 1 and output_attention_map:\n",
    "                    surface_embeddings, self_attention_map, external_attention_map = encoder(surface_embeddings, external_features, output_attention_map)\n",
    "                    \n",
    "                else:\n",
    "                    surface_embeddings, _, _ = encoder(surface_embeddings, external_features)\n",
    "                \n",
    "            encoded_sequences_batch.append(surface_embeddings.squeeze(1))\n",
    "            if output_attention_map:\n",
    "                self_attention_maps.append(self_attention_map)\n",
    "                external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return encoded_sequences_batch, self_attention_maps, external_attention_maps\n",
    "        \n",
    "        return encoded_sequences_batch, None, None    \n",
    "\n",
    "# Example of initializing and using these modules\n",
    "# torch.manual_seed(RANDOM_STATE)\n",
    "# n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "# ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "# attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "# gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "# ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "# num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "# external_dim = 5\n",
    "\n",
    "# surface_encoder = SurfaceEncoder(\n",
    "#     d_embedding, \n",
    "#     num_encoder_blocks,\n",
    "#     n_heads, \n",
    "#     ffn_hidden_dim, \n",
    "#     attention_dropout, \n",
    "#     gate_dropout, \n",
    "#     ffn_dropout, \n",
    "#     external_dim, \n",
    "# )\n",
    "\n",
    "# Assume embedded_sequences_batch is the output of the SurfaceEmbedding module and\n",
    "# external_features is the formatted external market features batch\n",
    "# encoded_sequences_batch, self_attention_map_batch, external_attention_map_batch = surface_encoder(embedded_sequences_batch, external_features_batch)\n",
    "# encoded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IvySPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class IvySPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(IvySPT, self).__init__()\n",
    "        self.surface_embedding = SurfaceEmbedding(\n",
    "            d_embedding, \n",
    "            remove_kernel, \n",
    "            remove_positional_embedding\n",
    "        )\n",
    "        self.surface_encoder = SurfaceEncoder(\n",
    "            d_embedding, \n",
    "            num_encoder_blocks,\n",
    "            n_heads, \n",
    "            ffn_hidden_dim,\n",
    "            attention_dropout, \n",
    "            gate_dropout,\n",
    "            ffn_dropout,\n",
    "            external_dim,\n",
    "            weight_initializer_std,\n",
    "            linear_bias_initializer_value,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention,\n",
    "            remove_gate\n",
    "        )\n",
    "        self.final_layer = nn.Linear(d_embedding, 1)\n",
    "        nn.init.normal_(self.final_layer.weight, mean=0.0, std=weight_initializer_std * (1 / (2 * (num_encoder_blocks + 1)) ** 0.5))\n",
    "        nn.init.constant_(self.final_layer.bias, linear_bias_initializer_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Obtain the embedded sequences and external features from the SurfaceEmbedding module\n",
    "        embedded_sequences_batch, external_features_batch = self.surface_embedding(batch)\n",
    "        # print('embedded: ', embedded_sequences_batch, external_features_batch)\n",
    "        # print('external: ', external_features_batch)\n",
    "\n",
    "        # Encode the sequences using the SurfaceEncoder module\n",
    "        encoded_sequences_batch, self_attention_maps, external_attention_maps = self.surface_encoder(\n",
    "            embedded_sequences_batch, \n",
    "            external_features_batch, \n",
    "            output_attention_map\n",
    "        )\n",
    "\n",
    "        # List to hold the implied volatility estimates for each query point in the batch\n",
    "        tv_estimates_batch = []\n",
    "\n",
    "        query_self_attention_maps = []\n",
    "        query_external_attention_maps = []\n",
    "\n",
    "        for i in range(len(encoded_sequences_batch)):\n",
    "            # Extract the encoded sequence\n",
    "            encoded_sequence = encoded_sequences_batch[i]\n",
    "\n",
    "            # Determine the number of query points for this sequence\n",
    "            num_query_points = len(batch['Query Points']['Log Moneyness'][i])\n",
    "\n",
    "            # Extract the encoded query points (last num_query_points elements in the sequence)\n",
    "            encoded_query_points = encoded_sequence[-num_query_points:]\n",
    "\n",
    "            # Estimate the implied volatility for each query point using the fully connected layer\n",
    "            tv_estimates = self.final_layer(encoded_query_points).squeeze(-1)\n",
    "\n",
    "            # Append the estimates to the batch list\n",
    "            tv_estimates_batch.append(tv_estimates)\n",
    "\n",
    "            if output_attention_map:\n",
    "                # Extract the attention maps for the query points\n",
    "                self_attention_map = self_attention_maps[i][-num_query_points:]\n",
    "                external_attention_map = external_attention_maps[i][-num_query_points:]\n",
    "\n",
    "                query_self_attention_maps.append(self_attention_map)\n",
    "                query_external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return tv_estimates_batch, query_self_attention_maps, query_external_attention_maps\n",
    "        \n",
    "        return tv_estimates_batch, None, None\n",
    "\n",
    "# Example of initializing and using this module\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "external_dim = 5\n",
    "\n",
    "ivy_spt = IvySPT(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim,\n",
    "    attention_dropout, \n",
    "    gate_dropout,\n",
    "    ffn_dropout,\n",
    "    external_dim\n",
    ")\n",
    "\n",
    "# # Pass the batch through the IvySPT model to get implied volatility estimates\n",
    "# tv_estimates_batch, self_attention_maps, external_attention_maps = ivy_spt(batch, output_attention_map=False)\n",
    "# gc.collect()\n",
    "# tv_estimates_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['Query Points']['Total Variance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SurfaceArbitrageFreeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SurfaceArbitrageFreeLoss, self).__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        tv_estimates_batch, \n",
    "        batch,\n",
    "        testing_mode=False,\n",
    "    ):\n",
    "        mse_loss_sum = 0.0\n",
    "        calendar_arbitrage_loss_sum = 0.0\n",
    "        butterfly_arbitrage_loss_sum = 0.0\n",
    "        total_elements = 0\n",
    "        loss_records = []\n",
    "\n",
    "        for total_implied_variance, target_variance, time_to_maturity, log_moneyness in zip(\n",
    "            tv_estimates_batch, \n",
    "            batch['Query Points']['Total Variance'], \n",
    "            batch['Query Points']['Time to Maturity'], \n",
    "            batch['Query Points']['Log Moneyness']\n",
    "        ):\n",
    "            sequence_length = total_implied_variance.size(0)\n",
    "            total_elements += sequence_length\n",
    "\n",
    "            # Calculate mean squared error between model estimates and target variances\n",
    "            mse_loss = torch.sum((total_implied_variance - target_variance) ** 2)\n",
    "            mse_loss_sum += mse_loss\n",
    "\n",
    "            unit_vectors = torch.eye(sequence_length, device=total_implied_variance.device)\n",
    "\n",
    "            # Compute gradients needed for arbitrage conditions\n",
    "            w_t = torch.stack([\n",
    "                torch.autograd.grad(\n",
    "                    outputs=total_implied_variance, \n",
    "                    inputs=time_to_maturity,\n",
    "                    grad_outputs=vec, \n",
    "                    create_graph=True   \n",
    "                )[0]\n",
    "                for vec in unit_vectors\n",
    "            ]).diag()\n",
    "\n",
    "            w_x = torch.stack([\n",
    "                torch.autograd.grad(\n",
    "                    outputs=total_implied_variance, \n",
    "                    inputs=log_moneyness,\n",
    "                    grad_outputs=vec, \n",
    "                    create_graph=True   \n",
    "                )[0]\n",
    "                for vec in unit_vectors\n",
    "            ]).diag()\n",
    "\n",
    "            w_xx = torch.stack([\n",
    "                torch.autograd.grad(\n",
    "                    outputs=w_x, \n",
    "                    inputs=log_moneyness, \n",
    "                    grad_outputs=vec,\n",
    "                    create_graph=True   \n",
    "                )[0]\n",
    "                for vec in unit_vectors\n",
    "            ]).diag()\n",
    "\n",
    "            # Calculate Calendar Arbitrage Loss\n",
    "            calendar_arbitrage_loss = torch.clamp(-w_t, min=0) ** 2\n",
    "            calendar_arbitrage_loss_sum += calendar_arbitrage_loss.sum()\n",
    "\n",
    "            # Calculate Butterfly Arbitrage Loss\n",
    "            w = total_implied_variance\n",
    "            g = (1 - log_moneyness * w_x / (2 * w)) ** 2 - w_x / 4 * (1 / w + 1 / 4) + w_xx / 2\n",
    "            butterfly_arbitrage_loss = torch.clamp(-g, min=0) ** 2\n",
    "            butterfly_arbitrage_loss_sum += butterfly_arbitrage_loss.sum()\n",
    "            if testing_mode:\n",
    "                record = {\n",
    "                    'MSE Loss': mse_loss.mean().item(),\n",
    "                    'Calendar Arbitrage Loss': calendar_arbitrage_loss.mean().item(),\n",
    "                    'Butterfly Arbitrage Loss': butterfly_arbitrage_loss.mean().item()\n",
    "                }\n",
    "                loss_records.append(record)\n",
    "\n",
    "        # Calculate mean losses\n",
    "        mse_loss = mse_loss_sum / total_elements\n",
    "        calendar_arbitrage_loss = calendar_arbitrage_loss_sum / total_elements\n",
    "        butterfly_arbitrage_loss = butterfly_arbitrage_loss_sum / total_elements\n",
    "\n",
    "        # Stack losses into a single tensor\n",
    "        total_losses = torch.stack([mse_loss, calendar_arbitrage_loss, butterfly_arbitrage_loss])\n",
    "\n",
    "        if testing_mode:\n",
    "            loss_records = pd.DataFrame(loss_records)\n",
    "            loss_records['Datetime'] = batch['Datetime']\n",
    "            loss_records['Mask Proportion'] = batch['Mask Proportion']\n",
    "            loss_records.set_index(['Datetime', 'Mask Proportion'], inplace=True)\n",
    "\n",
    "            return total_losses, loss_records\n",
    "\n",
    "        return total_losses, None\n",
    "\n",
    "# surface_arbitrage_free_loss = SurfaceArbitrageFreeLoss()  \n",
    "# all_losses, loss_records = surface_arbitrage_free_loss(tv_estimates_batch, batch)\n",
    "# all_losses, loss_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLossCoefficients(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        initial_losses, \n",
    "        alpha=1.0, \n",
    "        learning_rate=0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the adaptive loss weights module.\n",
    "\n",
    "        Args:\n",
    "            initial_losses (torch.Tensor): Initial loss values for each task to set the initial loss ratios.\n",
    "            alpha (float): The strength of the restoring force in balancing training rates.\n",
    "            learning_rate (float): Learning rate for updating the weights.\n",
    "        \"\"\"\n",
    "        super(AdaptiveLossCoefficients, self).__init__()\n",
    "        self.initial_losses = initial_losses\n",
    "        self.alpha = alpha\n",
    "        self.weights = torch.nn.Parameter(torch.ones_like(self.initial_losses))\n",
    "        self.optimizer = torch.optim.Adam([self.weights], lr=learning_rate)\n",
    "        self.total_weights = self.weights.sum().item()  # Total of weights to maintain normalization\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        current_losses, \n",
    "        final_layer\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Adjusts and normalizes the weights based on current losses using the GradNorm approach.\n",
    "\n",
    "        Args:\n",
    "            current_losses (torch.Tensor): Current computed losses from the main model.\n",
    "            final_layer (torch.nn.Moduler): The final layer of the model whose parameters are used for \n",
    "            gradient norm calculation. \n",
    "\n",
    "        Returns:\n",
    "            None: The updated weights are detached and stored within the module.\n",
    "        \"\"\"\n",
    "        loss_ratios = current_losses / self.initial_losses\n",
    "        relative_inverse_rates = loss_ratios / loss_ratios.mean()\n",
    "\n",
    "        # Compute gradient norms for each weighted loss\n",
    "        gradient_norms = torch.stack([\n",
    "            torch.norm(torch.autograd.grad(self.weights[i] * loss, final_layer.parameters(), create_graph=True)[0])\n",
    "            for i, loss in enumerate(current_losses)\n",
    "        ])\n",
    "\n",
    "        target_gradient_norms = (gradient_norms.mean() * (relative_inverse_rates ** self.alpha)).detach()\n",
    "        gradnorm_loss = torch.sum(torch.abs(gradient_norms - target_gradient_norms))\n",
    "\n",
    "        # Update the weights using the GradNorm loss\n",
    "        self.optimizer.zero_grad()\n",
    "        gradnorm_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Normalize to sum to total_weights, detach, and ensure gradient tracking\n",
    "        with torch.no_grad():\n",
    "            normalized_weights = self.weights / self.weights.sum() * self.total_weights\n",
    "            self.weights.data = normalized_weights.detach()  # Explicitly detach from the graph\n",
    "\n",
    "        # Re-enable gradient tracking on the updated weights\n",
    "        self.weights.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_batch_to_device(batched_data, device):\n",
    "    def move_to_device(data, device):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return data.to(device)\n",
    "        elif isinstance(data, dict):\n",
    "            return {key: move_to_device(value, device) for key, value in data.items()}\n",
    "        elif isinstance(data, list):\n",
    "            return [move_to_device(item, device) for item in data]\n",
    "        else:\n",
    "            return data  # For non-tensor data (e.g., strings), return as is\n",
    "\n",
    "    return move_to_device(batched_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        train_data_loader, \n",
    "        validate_data_loader, \n",
    "        test_data_loader, \n",
    "        n_epochs, \n",
    "        warmup_ratio, \n",
    "        peak_learning_rate, \n",
    "        min_learning_rate, \n",
    "        gradient_clip, \n",
    "        adamw_betas, \n",
    "        adamw_epsilon, \n",
    "        adamw_weight_decay, \n",
    "        layer_wise_decay,\n",
    "        loss_asymmetry_alpha, \n",
    "        device\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.validate_data_loader = validate_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.n_epochs = n_epochs\n",
    "        self.warmup_epochs = int(warmup_ratio * n_epochs)\n",
    "        self.loss_asymmetry_alpha = loss_asymmetry_alpha\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.peak_learning_rate = peak_learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.device = device\n",
    "\n",
    "        # AdamW Optimizer with Layer-wise decay\n",
    "        self.optimizer = AdamW(\n",
    "            self._layer_wise_learning_rate_decay(layer_wise_decay, peak_learning_rate), \n",
    "            betas=adamw_betas, \n",
    "            eps=adamw_epsilon, \n",
    "            weight_decay=adamw_weight_decay\n",
    "        )\n",
    "\n",
    "        # Learning Rate Scheduler\n",
    "        warmup_scheduler = LambdaLR(\n",
    "            self.optimizer,\n",
    "            lr_lambda=lambda step: min(1.0, step / self.warmup_epochs)\n",
    "        )\n",
    "        \n",
    "        cosine_scheduler = CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=self.n_epochs - self.warmup_epochs,\n",
    "            eta_min=self.min_learning_rate\n",
    "        )\n",
    "\n",
    "        self.scheduler = SequentialLR(\n",
    "            self.optimizer,\n",
    "            schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "            milestones=[self.warmup_epochs]\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        adaptive_loss_weights = None\n",
    "        loss_coefficients_history = []\n",
    "        train_loss_components_history = []\n",
    "        validate_loss_components_history = []\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            train_loss_components_sums = torch.zeros(3, device=self.device)  \n",
    "            total_batches = 0\n",
    "\n",
    "            for batch in self.train_data_loader:\n",
    "                batch = send_batch_to_device(batch, self.device)\n",
    "                tv_estimates_batch, _, _ = self.model(batch)\n",
    "                train_loss_components, _ = SurfaceArbitrageFreeLoss()(tv_estimates_batch, batch)\n",
    "                # print('loss: ', train_loss_components.detach().cpu().numpy())\n",
    "                \n",
    "                if adaptive_loss_weights is None: \n",
    "                    adaptive_loss_weights = AdaptiveLossCoefficients(\n",
    "                        initial_losses=train_loss_components.detach().clone(),\n",
    "                        alpha=self.loss_asymmetry_alpha,\n",
    "                        learning_rate=np.sqrt(self.peak_learning_rate * self.min_learning_rate)\n",
    "                    )\n",
    "\n",
    "                # Obtain the current loss coefficients\n",
    "                loss_coefficients = adaptive_loss_weights.weights.detach().clone()\n",
    "                train_loss = train_loss_components @ loss_coefficients\n",
    "\n",
    "                # Record the current loss coefficients\n",
    "                loss_coefficients_history.append(loss_coefficients.cpu().numpy())\n",
    "\n",
    "                # Accumulate the loss components\n",
    "                train_loss_components_sums += train_loss_components.detach().clone()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                train_loss.backward(retain_graph=True)\n",
    "\n",
    "                adaptive_loss_weights(train_loss_components, self.model.final_layer)\n",
    "\n",
    "                clip_grad_norm_(self.model.parameters(), self.gradient_clip)\n",
    "                self.optimizer.step()\n",
    "                total_batches += 1\n",
    "\n",
    "                # Free up memory\n",
    "                del batch, tv_estimates_batch, train_loss_components, loss_coefficients, train_loss\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            # Calculate the average loss components for this epoch\n",
    "            avg_train_loss_components = train_loss_components_sums / total_batches\n",
    "            train_loss_components_history.append(avg_train_loss_components.cpu().numpy())    \n",
    "            \n",
    "            # Validate after each epoch\n",
    "            avg_validate_loss_components = self.validate()\n",
    "            validate_loss_components_history.append(avg_validate_loss_components.cpu().numpy())\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.n_epochs} - Training Loss: {avg_train_loss_components.cpu().numpy()}, Validation Loss: {avg_validate_loss_components.cpu().numpy()}\")\n",
    "\n",
    "            # Adjust learning rate\n",
    "            self.scheduler.step()\n",
    "\n",
    "        return loss_coefficients_history, train_loss_components_history, validate_loss_components_history\n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        validate_loss_components_sums = torch.zeros(3, device=self.device)  \n",
    "        total_batches = 0\n",
    "\n",
    "        for batch in self.validate_data_loader:\n",
    "            batch = send_batch_to_device(batch, self.device)\n",
    "            tv_estimates_batch, _, _ = self.model(batch)\n",
    "\n",
    "            validate_loss_components, _ = SurfaceArbitrageFreeLoss()(tv_estimates_batch, batch)\n",
    "            validate_loss_components_sums += validate_loss_components.detach().clone()\n",
    "            total_batches += 1\n",
    "\n",
    "            # Free up memory\n",
    "            del batch, tv_estimates_batch, validate_loss_components\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Calculate the average loss components for this epoch\n",
    "        avg_validate_loss_components = validate_loss_components_sums / total_batches  \n",
    "\n",
    "        return avg_validate_loss_components    \n",
    "    \n",
    "    \n",
    "    def test(\n",
    "        self,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        self.model.eval()\n",
    "\n",
    "        if output_attention_map:\n",
    "            with torch.no_grad():\n",
    "                batch = next(iter(self.test_data_loader))\n",
    "                batch = send_batch_to_device(batch, self.device)\n",
    "                tv_estimates_batch, self_attention_maps, external_attention_maps = ivy_spt(batch, output_attention_map=output_attention_map)\n",
    "\n",
    "                # Free up memory\n",
    "                del batch, tv_estimates_batch\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                return self_attention_maps, external_attention_maps\n",
    "\n",
    "        test_loss_components_sums = torch.zeros(3, device=self.device)  \n",
    "        total_batches = 0\n",
    "        test_loss_records = []\n",
    "\n",
    "        for batch in self.test_data_loader:\n",
    "            batch = send_batch_to_device(batch, self.device)\n",
    "            tv_estimates_batch, _, _ = self.model(batch)\n",
    "\n",
    "            test_loss_components, loss_records = SurfaceArbitrageFreeLoss()(tv_estimates_batch, batch, testing_mode=True)\n",
    "            test_loss_components_sums += test_loss_components.detach().clone()\n",
    "            total_batches += 1\n",
    "            test_loss_records.append(loss_records)\n",
    "\n",
    "            # Free up memory\n",
    "            del batch, tv_estimates_batch, test_loss_components, loss_records\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Calculate the average loss components for this epoch\n",
    "        avg_test_loss_components = test_loss_components_sums / total_batches  \n",
    "        test_loss_records = pd.concat(test_loss_records) \n",
    "\n",
    "        return avg_test_loss_components, test_loss_records   \n",
    "    \n",
    "    def _layer_wise_learning_rate_decay(\n",
    "        self, \n",
    "        layer_wise_decay, \n",
    "        base_lr\n",
    "    ):\n",
    "        params = []\n",
    "\n",
    "        # Final layer (depth 0)\n",
    "        params.append({\n",
    "            'params': self.model.final_layer.parameters(),\n",
    "            'lr': base_lr\n",
    "        })\n",
    "\n",
    "        # Surface Encoder layers (depth from 1 to num_encoder_blocks)\n",
    "        if layer_wise_decay is not None:\n",
    "            for i, encoder in enumerate(self.model.surface_encoder.encoders):\n",
    "                lr = base_lr * (layer_wise_decay ** (i + 1))\n",
    "                params.append({\n",
    "                    'params': encoder.parameters(),\n",
    "                    'lr': lr\n",
    "                })\n",
    "\n",
    "            # Surface Embedding layers (highest depth)\n",
    "            params.append({\n",
    "                'params': self.model.surface_embedding.parameters(),\n",
    "                'lr': base_lr * (layer_wise_decay ** (len(self.model.surface_encoder.encoders) + 1))\n",
    "            })\n",
    "        else:\n",
    "            # No decay: All layers use the base learning rate\n",
    "            params.extend([\n",
    "                {'params': self.model.surface_encoder.parameters(), 'lr': base_lr},\n",
    "                {'params': self.model.surface_embedding.parameters(), 'lr': base_lr},\n",
    "            ])\n",
    "\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Number of Query Points' : 1,\n",
    "        'Batch Size' : 10\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 64,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 256,\n",
    "        'Attention Dropout' : 0.,\n",
    "        'Gate Dropout' : 0.,\n",
    "        'FFN Dropout' : 0.,\n",
    "        'Number of Blocks' : 4,\n",
    "        'External Feature Dimension' : 5,\n",
    "        'Weight Initializer Std.' : 0.02,\n",
    "        'Linear Bias Initializer' : 0.0,\n",
    "        'Gate Bias Inititalizer' : 10.0\n",
    "    },\n",
    "    'Adaptive Loss Weights' : {\n",
    "        'Asymmetry' : 1.5,\n",
    "    },\n",
    "    'Trainer' : {\n",
    "        'Pre-Train' : {\n",
    "            'Number of Epochs' : 20,\n",
    "            'Warmup Ratio' : 0.15,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-5,\n",
    "            'Gradient Clipping' : 1,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : None,\n",
    "        },\n",
    "        'Fine-Tune' : {\n",
    "            'Number of Epochs' : 10,\n",
    "            'Warmup Ratio' : 0.1,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-6,\n",
    "            'Gradient Clipping' : 0,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : 0.9,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IVSurfaceDataset(\n",
    "    surfaces, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training Loss: [0.00130012 0.0054825  0.0339609 ], Validation Loss: [0.00686512 0.04283484 0.        ]\n",
      "Epoch 2/20 - Training Loss: [0.00173029 0.02270596 0.15620717], Validation Loss: [2.0192596e-03 3.3820022e-04 1.0254211e+00]\n",
      "Epoch 3/20 - Training Loss: [0.00170538 0.00050908 0.02406102], Validation Loss: [9.4385948e-03 6.4705745e-03 5.1633833e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hermes/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training Loss: [0.0027229  0.00187001 0.35919875], Validation Loss: [0.01151543 0.0178588  0.        ]\n",
      "Epoch 5/20 - Training Loss: [0.01310629 0.00053318 0.        ], Validation Loss: [0.00982715 0.05687633 0.        ]\n",
      "Epoch 6/20 - Training Loss: [1.5239663e-02 9.5566345e-04 1.0299943e-06], Validation Loss: [0.01377342 0.05075788 0.        ]\n",
      "Epoch 7/20 - Training Loss: [0.01189741 0.07418746 0.        ], Validation Loss: [1.39072435e-02 2.10756250e-03 5.10082531e+00]\n",
      "Epoch 8/20 - Training Loss: [0.01108345 0.02201041 0.5607176 ], Validation Loss: [1.2900060e-02 3.1720973e-03 6.2026157e+00]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 51\u001b[0m\n\u001b[1;32m     17\u001b[0m pre_trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m     model_pre_train,\n\u001b[1;32m     19\u001b[0m     data_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     device\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m fine_tuner \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     35\u001b[0m     model_pre_train,\n\u001b[1;32m     36\u001b[0m     data_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     device\n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 51\u001b[0m \u001b[43mpre_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 100\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     98\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 100\u001b[0m \u001b[43madaptive_loss_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 53\u001b[0m, in \u001b[0;36mAdaptiveLossCoefficients.forward\u001b[0;34m(self, current_losses, final_layer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Update the weights using the GradNorm loss\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 53\u001b[0m \u001b[43mgradnorm_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Normalize to sum to total_weights, detach, and ensure gradient tracking\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "model_pre_train = IvySPT(\n",
    "    hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "    hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "    hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "    hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "    hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "    hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "    hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "    hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "    hyperparameters['Surface Encoding']['Gate Bias Inititalizer']\n",
    ")\n",
    "pre_trainer = Trainer(\n",
    "    model_pre_train,\n",
    "    data_loader,\n",
    "    data_loader,\n",
    "    data_loader,\n",
    "    hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "fine_tuner = Trainer(\n",
    "    model_pre_train,\n",
    "    data_loader,\n",
    "    data_loader,\n",
    "    data_loader,\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "pre_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
