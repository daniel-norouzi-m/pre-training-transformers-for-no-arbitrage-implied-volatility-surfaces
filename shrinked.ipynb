{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Batch Size' : 4\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 8,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 16,\n",
    "        'Attention Dropout' : 0.1,\n",
    "        'Gate Dropout' : 0.1,\n",
    "        'FFN Dropout' : 0.1,\n",
    "        'Number of Blocks' : 2,\n",
    "        'External Feature Dimension' : 3,\n",
    "    },\n",
    "    'No-Arbitrage' : {\n",
    "        'Butterfly' : 1,\n",
    "        'Calendar' : 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Log Moneyness</th>\n",
       "      <th>Time to Maturity</th>\n",
       "      <th>Implied Volatility</th>\n",
       "      <th>Market Return</th>\n",
       "      <th>Market Volatility</th>\n",
       "      <th>Treasury Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.316688</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.316688</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.304266</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.304266</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.291996</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-06-28</th>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.427518</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.434898</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.434898</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.442224</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.442224</td>\n",
       "      <td>2.253968</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574326 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Log Moneyness  Time to Maturity  Implied Volatility  \\\n",
       "Datetime   Symbol                                                        \n",
       "2013-01-02 AAPL        -0.316688          0.007937              0.3726   \n",
       "           AAPL        -0.316688          0.007937              0.6095   \n",
       "           AAPL        -0.304266          0.007937              0.3726   \n",
       "           AAPL        -0.304266          0.007937              0.6095   \n",
       "           AAPL        -0.291996          0.007937              0.3726   \n",
       "...                          ...               ...                 ...   \n",
       "2013-06-28 GOOGL        0.427518          2.253968              0.2430   \n",
       "           GOOGL        0.434898          2.253968              0.2383   \n",
       "           GOOGL        0.434898          2.253968              0.2426   \n",
       "           GOOGL        0.442224          2.253968              0.2402   \n",
       "           GOOGL        0.442224          2.253968              0.2433   \n",
       "\n",
       "                   Market Return  Market Volatility  Treasury Rate  \n",
       "Datetime   Symbol                                                   \n",
       "2013-01-02 AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "           AAPL         0.025086          14.680000          0.055  \n",
       "...                          ...                ...            ...  \n",
       "2013-06-28 GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "           GOOGL       -0.004299          16.860001          0.030  \n",
       "\n",
       "[574326 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_googl_data = pd.read_csv('volatility_surface_AAPL_GOOGL_2013_01_2013_06.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "aapl_googl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': Timestamp('2013-01-02 00:00:00'),\n",
       " 'Symbol': 'AAPL',\n",
       " 'Market Features': {'Market Return': 0.0250861159586972,\n",
       "  'Market Volatility': 14.68000030517578,\n",
       "  'Treasury Rate': 0.0549999997019767},\n",
       " 'Surface': {'Log Moneyness': array([-0.31668849, -0.31668849, -0.30426597, ...,  0.63882295,\n",
       "          0.6483924 ,  0.6483924 ]),\n",
       "  'Time to Maturity': array([0.00793651, 0.00793651, 0.00793651, ..., 2.95634921, 2.95634921,\n",
       "         2.95634921]),\n",
       "  'Implied Volatility': array([0.3726, 0.6095, 0.3726, ..., 0.3387, 0.3342, 0.3389])}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def implied_volatility_surfaces(options_market_data):\n",
    "    # Group the data by Datetime and Symbol\n",
    "    grouped_data = options_market_data.groupby(level=['Datetime', 'Symbol'])\n",
    "\n",
    "    surfaces = []\n",
    "    for (date, symbol), surface in grouped_data:\n",
    "        surface_dict = {\n",
    "            'Datetime': date,\n",
    "            'Symbol': symbol,\n",
    "            'Market Features': {\n",
    "                'Market Return': surface['Market Return'].values[0],\n",
    "                'Market Volatility': surface['Market Volatility'].values[0],\n",
    "                'Treasury Rate': surface['Treasury Rate'].values[0],\n",
    "            },\n",
    "            'Surface': {\n",
    "                'Log Moneyness': surface['Log Moneyness'].values,\n",
    "                'Time to Maturity': surface['Time to Maturity'].values,\n",
    "                'Implied Volatility': surface['Implied Volatility'].values,\n",
    "            }\n",
    "        }\n",
    "        surfaces.append(surface_dict)\n",
    "\n",
    "    return surfaces\n",
    "\n",
    "surfaces = implied_volatility_surfaces(aapl_googl_data)\n",
    "surfaces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00'),\n",
       "  Timestamp('2013-03-07 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL', 'AAPL'],\n",
       " 'Market Features': {'Market Return': tensor([-0.0003, -0.0019, -0.0007,  0.0018]),\n",
       "  'Market Volatility': tensor([15.4400, 13.5700, 13.0200, 13.0600]),\n",
       "  'Treasury Rate': tensor([0.0400, 0.0600, 0.0350, 0.0900])},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-0.3159, -0.2852, -0.2553,  ...,  0.8627,  0.8723,  0.8723]),\n",
       "   tensor([-0.2509, -0.2509, -0.2368,  ...,  0.8381,  0.8477,  0.8477]),\n",
       "   tensor([-0.3741, -0.3661, -0.3582,  ...,  0.3886,  0.4034,  0.4107]),\n",
       "   tensor([-0.1790, -0.1790, -0.1652,  ...,  0.8818,  0.8914,  0.8914])],\n",
       "  'Time to Maturity': [tensor([0.0159, 0.0159, 0.0159,  ..., 2.3254, 2.3254, 2.3254]),\n",
       "   tensor([0.0159, 0.0159, 0.0159,  ..., 2.8532, 2.8532, 2.8532]),\n",
       "   tensor([0.0159, 0.0159, 0.0159,  ..., 2.4087, 2.4087, 2.4087]),\n",
       "   tensor([0.0040, 0.0040, 0.0040,  ..., 2.7024, 2.7024, 2.7024])],\n",
       "  'Implied Volatility': [tensor([0.4187, 0.3716, 0.3716,  ..., 0.3194, 0.3320, 0.3194]),\n",
       "   tensor([0.4101, 0.4484, 0.4101,  ..., 0.3084, 0.2916, 0.3084]),\n",
       "   tensor([0.3347, 0.3347, 0.3347,  ..., 0.2339, 0.2302, 0.2355]),\n",
       "   tensor([0.5199, 0.7914, 0.5199,  ..., 0.3147, 0.2958, 0.3147])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([ 0.7563,  0.2614,  0.4080,  0.2614,  0.7347,  0.4305,  0.7879,  0.4668,\n",
       "            0.3927,  0.1980,  0.7013,  0.3374,  0.1599,  0.7455,  0.1599,  0.1102,\n",
       "            0.5556,  0.2073,  0.4230,  0.1102,  0.4004,  0.1403,  0.1886,  0.3210,\n",
       "            0.1403,  0.0360,  0.3127,  0.0470,  0.1203,  0.6189,  0.1886,  0.0470,\n",
       "            0.2165,  0.0791,  0.8335,  0.7509,  0.4230,  0.7669,  0.4155,  0.3292,\n",
       "            0.6251,  0.1599,  0.5750,  0.1304,  0.1102,  0.1599,  0.3850,  0.0360,\n",
       "            0.0791,  0.2959,  0.7347,  0.7013,  0.0685,  0.2701,  0.0360,  0.6432,\n",
       "            0.3850,  0.1203,  0.1696,  0.0360,  0.2701,  0.6669,  0.2526,  0.0999,\n",
       "            0.2165,  0.4080,  0.3535,  0.3772,  0.2347,  0.1791,  0.0470,  0.2788,\n",
       "            0.6669,  0.2165,  0.5425,  0.2165,  0.1886,  0.2257,  0.0578,  0.4452,\n",
       "            0.0360,  0.2437,  0.3374,  0.2959,  0.2257,  0.0250,  0.5224,  0.5156,\n",
       "            0.7775,  0.2073,  0.0791,  0.6492,  0.2257,  0.2257,  0.1502,  0.6842,\n",
       "            0.0791,  0.6432,  0.1886,  0.4452,  0.0895,  0.1403,  0.5621,  0.5814,\n",
       "            0.1980,  0.6372,  0.2165,  0.7827,  0.3927,  0.0999,  0.1403,  0.6189,\n",
       "            0.6066,  0.6311,  0.1886,  0.2526,  0.4668,  0.2073,  0.6842,  0.7827,\n",
       "            0.0250,  0.7722,  0.0685,  0.3927,  0.7292,  0.3127,  0.1886,  0.2701,\n",
       "            0.0360,  0.0895,  0.1304,  0.2347,  0.1203,  0.2701,  0.3455,  0.4080,\n",
       "            0.0895,  0.1980,  0.5291,  0.2347,  0.2614,  0.0895,  0.8135,  0.1791,\n",
       "            0.6785,  0.4739,  0.2073,  0.1403,  0.3043,  0.1599,  0.2437,  0.2874,\n",
       "            0.3694,  0.7879,  0.0791,  0.2701,  0.2347,  0.6372,  0.2073,  0.6669,\n",
       "            0.0360,  0.3127,  0.2701,  0.2526,  0.1403,  0.1696,  0.2257,  0.3374,\n",
       "           -0.1054, -0.6462, -0.1181, -0.3805, -0.6036, -0.2263, -0.1310, -0.3477,\n",
       "            0.0250, -0.0322, -0.7136, -0.1310, -0.3317, -0.7612, -0.1707, -0.3159,\n",
       "           -0.0560,  0.0360, -0.3805, -0.5430, -0.2121, -0.5045, -0.6247, -0.3973,\n",
       "           -0.4858, -0.1981, -0.6681, -0.6906, -0.1844, -0.0560,  0.0138, -0.2553,\n",
       "           -0.0681, -0.1310, -0.2121, -0.0681, -0.0205, -0.1441, -0.1844, -0.3004,\n",
       "           -0.0681, -0.3973, -0.2852, -0.1441, -0.4495, -0.3317, -0.5236, -0.4318,\n",
       "           -0.1844, -0.1181, -0.3639, -0.1310, -0.3477, -0.7371, -0.7371, -0.3004,\n",
       "           -0.1707, -0.0440, -0.6906, -0.2701, -0.1981, -0.1844, -0.0440, -0.1844,\n",
       "           -0.4675, -0.2263, -0.6036, -0.0205, -0.3159, -0.1181, -0.0440, -0.3159,\n",
       "           -0.6036, -0.3004, -0.4144, -0.4675, -0.1441, -0.7136, -0.4144, -0.4318,\n",
       "           -0.6906,  0.0025, -0.3805, -0.3317, -0.4144, -0.7612, -0.6681, -0.6462,\n",
       "           -0.1441,  0.0138, -0.4858, -0.6036,  0.0025, -0.1573, -0.1441, -0.3639,\n",
       "           -0.6906,  0.0025,  0.0360, -0.2121, -0.5045, -0.6036, -0.0804, -0.3317,\n",
       "           -0.0928, -0.0089, -0.0205, -0.2553, -0.3004, -0.1573, -0.5628, -0.2407,\n",
       "           -0.1573, -0.5830, -0.1707, -0.7859, -0.1054, -0.4858, -0.1981, -0.1981,\n",
       "           -0.0560, -0.1310, -0.2553, -0.1181, -0.0440, -0.8112, -0.6036, -0.2701,\n",
       "           -0.1844, -0.1573, -0.1844,  0.0250, -0.3004,  0.0250, -0.6036, -0.2852,\n",
       "           -0.0560, -0.3477, -0.0440,  0.0025, -0.0440, -0.2263, -0.0440,  0.0025,\n",
       "           -0.0440, -0.5430, -0.6462, -0.2263, -0.3159, -0.5236, -0.0928,  0.0138,\n",
       "           -0.7136, -0.0681, -0.1310, -0.5236,  0.0025, -0.1844, -0.3639, -0.1707,\n",
       "           -0.0560, -0.3159, -0.0804, -0.7859, -0.0928, -0.4675, -0.3973, -0.0681,\n",
       "           -0.1310, -0.0322, -0.3805, -0.1181, -0.6462, -0.0804, -0.3805, -0.1707,\n",
       "           -0.4318, -0.5628, -0.6906,  0.5621,  0.7722,  0.7509,  0.5358,  0.8433,\n",
       "            0.8235,  0.1403,  0.2437,  0.0025,  0.1599,  0.4230,  0.7181,  0.1599,\n",
       "            0.4080,  0.4950,  0.8627,  0.0250,  0.3927,  0.0250,  0.0470,  0.5878,\n",
       "            0.7616,  0.7070,  0.1791,  0.3615, -0.0440,  0.7292,  0.0895,  0.1886,\n",
       "            0.4378,  0.8531,  0.5358,  0.1102,  0.4880,  0.6066,  0.2165,  0.6551,\n",
       "            0.2788,  0.1696,  0.4524,  0.1502,  0.2959,  0.5224,  0.0470,  0.4004,\n",
       "            0.3615,  0.3127,  0.5941,  0.1304,  0.6785,  0.3043,  0.7879,  0.6727,\n",
       "            0.5556,  0.1203,  0.0791,  0.6492,  0.7669,  0.7347,  0.7616,  0.1980,\n",
       "            0.7292,  0.5491,  0.2257,  0.8627,  0.3043,  0.4880,  0.4950,  0.2788,\n",
       "            0.3615,  0.2701,  0.2347,  0.2165,  0.1502,  0.3210,  0.6610,  0.7879,\n",
       "            0.6372,  0.5088,  0.3694,  0.4452,  0.8135,  0.7563,  0.0578,  0.6432,\n",
       "            0.5156,  0.4950,  0.6251,  0.8627,  0.2437,  0.2701,  0.1502,  0.4230,\n",
       "            0.2959,  0.3927,  0.1980,  0.4080,  0.5686,  0.7930,  0.1791,  0.4524,\n",
       "            0.8531,  0.7775,  0.3210,  0.3772,  0.7181,  0.3772,  0.5491,  0.6492,\n",
       "            0.4230,  0.6189,  0.5621,  0.7181], requires_grad=True),\n",
       "   tensor([ 1.4496e-01,  1.3530e-01, -1.0498e-01,  2.2357e-02,  1.1428e-02,\n",
       "           -5.6781e-02,  3.2888e-01,  2.1907e-01,  3.2085e-01,  1.1569e-01,\n",
       "            2.7125e-01,  2.1907e-01,  6.4916e-02,  3.1275e-01, -5.6781e-02,\n",
       "           -1.0795e-02,  2.5416e-01, -6.8615e-02,  3.7784e-04, -2.2095e-02,\n",
       "            7.5279e-02,  5.4445e-02,  9.5688e-02,  1.5453e-01,  5.4445e-02,\n",
       "           -2.2095e-02,  9.5688e-02, -4.5085e-02,  1.1428e-02,  1.7340e-01,\n",
       "           -2.2277e-01,  2.8806e-01,  1.2554e-01,  2.7125e-01,  2.7125e-01,\n",
       "            3.4476e-01, -5.6781e-02,  3.1275e-01,  1.0574e-01,  1.0574e-01,\n",
       "           -3.3524e-02,  1.1569e-01,  1.9192e-01,  2.1907e-01,  3.2888e-01,\n",
       "            2.2357e-02,  3.3168e-02,  1.9192e-01,  8.5536e-02,  2.9636e-01,\n",
       "            1.7340e-01,  1.8270e-01,  2.6274e-01, -1.0498e-01,  1.1569e-01,\n",
       "           -1.9537e-01, -1.1741e-01,  1.8270e-01,  3.3685e-01, -1.0498e-01,\n",
       "            1.1428e-02, -8.0591e-02,  2.8806e-01,  1.8270e-01,  7.5279e-02,\n",
       "            3.3168e-02,  2.8806e-01,  2.1010e-01,  3.3685e-01, -5.6781e-02,\n",
       "            1.2554e-01,  2.2357e-02, -1.0498e-01,  1.1428e-02,  2.5416e-01,\n",
       "            1.6401e-01, -2.0897e-01,  1.9192e-01,  7.5279e-02,  2.6274e-01,\n",
       "            3.2888e-01, -3.3524e-02,  9.5688e-02,  1.1428e-02,  1.1569e-01,\n",
       "           -1.0795e-02,  2.9636e-01, -4.5085e-02,  2.4550e-01,  1.9192e-01,\n",
       "            6.4916e-02,  4.3863e-02,  1.4496e-01,  3.7784e-04,  1.5453e-01,\n",
       "            2.7969e-01, -2.2095e-02,  1.3530e-01,  2.0105e-01,  3.0459e-01,\n",
       "            1.8270e-01, -8.0591e-02, -6.8615e-02,  2.5416e-01,  2.0105e-01,\n",
       "            2.0105e-01,  3.0459e-01,  2.3677e-01,  9.5688e-02, -2.2095e-02,\n",
       "           -1.6870e-01,  1.9192e-01,  7.5279e-02,  6.4916e-02,  3.7784e-04,\n",
       "            1.9192e-01, -1.6870e-01, -2.2095e-02,  1.1428e-02,  4.3863e-02,\n",
       "           -1.5563e-01,  1.4496e-01,  2.3677e-01, -3.3524e-02, -9.2713e-02,\n",
       "           -2.2095e-02,  1.5453e-01,  3.1275e-01,  3.0459e-01,  3.2085e-01,\n",
       "            2.9636e-01,  2.7969e-01,  1.4496e-01,  2.2796e-01, -5.6781e-02,\n",
       "            2.2357e-02,  9.5688e-02, -4.5085e-02,  2.9636e-01,  2.4550e-01,\n",
       "            1.8270e-01, -9.2713e-02,  2.1907e-01,  2.4550e-01,  1.7340e-01,\n",
       "           -2.2095e-02,  3.0459e-01,  1.0574e-01, -1.6870e-01,  5.4445e-02,\n",
       "            8.5536e-02, -1.2998e-01,  1.8270e-01,  1.6401e-01, -1.1741e-01,\n",
       "            2.0105e-01, -3.3524e-02, -9.2713e-02,  2.7969e-01,  2.7125e-01,\n",
       "            2.1010e-01,  1.1569e-01,  1.0574e-01, -1.8194e-01, -5.6781e-02,\n",
       "            9.5688e-02,  2.3677e-01,  1.3530e-01,  2.7125e-01, -1.1741e-01,\n",
       "           -1.4272e-01,  2.6274e-01,  2.5416e-01, -2.3675e-01, -1.6870e-01,\n",
       "            1.3530e-01,  7.0457e-01,  7.3701e-01,  3.5260e-01,  1.5453e-01,\n",
       "            4.0584e-01,  7.4759e-01,  7.4232e-01,  2.0105e-01,  5.5042e-01,\n",
       "            3.7784e-04,  4.6341e-01,  1.4496e-01,  1.2554e-01,  6.3637e-01,\n",
       "            1.1569e-01,  7.7357e-01,  2.1010e-01,  6.6535e-01,  3.6038e-01,\n",
       "            2.9636e-01,  5.0451e-01,  1.7340e-01,  5.1120e-01,  3.7577e-01,\n",
       "            3.0459e-01,  4.0584e-01,  4.4933e-01,  5.8816e-01,  5.3101e-01,\n",
       "            6.9907e-01,  6.5962e-01,  3.6038e-01,  2.7125e-01,  4.2782e-01,\n",
       "            8.3811e-01,  5.8197e-01,  4.9778e-01,  3.5260e-01,  1.6401e-01,\n",
       "            5.9432e-01,  4.8417e-01,  7.4232e-01,  2.3677e-01,  4.2782e-01,\n",
       "            1.6401e-01,  2.3677e-01,  8.5536e-02,  5.8816e-01,  5.3101e-01,\n",
       "            5.6947e-01,  4.2782e-01,  4.9778e-01,  6.3637e-01,  1.2554e-01,\n",
       "            3.5260e-01,  7.1550e-01,  2.6274e-01,  1.8270e-01,  2.2796e-01,\n",
       "            6.9353e-01,  2.8806e-01,  1.0574e-01,  5.4445e-02,  7.6326e-01,\n",
       "            9.5688e-02,  5.8197e-01,  4.7730e-01,  7.5279e-02,  1.8270e-01,\n",
       "            3.5260e-01,  4.7038e-01,  1.2554e-01,  2.1907e-01,  3.6810e-01,\n",
       "            4.9100e-01,  6.5386e-01,  7.2092e-01,  1.9192e-01,  3.3685e-01,\n",
       "            5.4399e-01,  5.5681e-01,  7.4759e-01,  7.8377e-01,  7.6326e-01,\n",
       "            5.3101e-01,  7.2631e-01,  5.7574e-01,  7.2092e-01,  4.7730e-01,\n",
       "            5.3101e-01,  6.9907e-01,  5.0451e-01,  6.4223e-01,  7.9889e-01,\n",
       "            6.5386e-01,  3.9092e-01,  5.5681e-01,  4.5640e-01,  6.7672e-01,\n",
       "            7.5284e-01,  8.2844e-01,  6.6535e-01,  5.2445e-01,  4.5640e-01,\n",
       "            6.2453e-01,  6.8235e-01,  4.4933e-01,  6.3047e-01,  4.2055e-01,\n",
       "            4.3504e-01,  7.2092e-01,  7.7868e-01,  7.2631e-01,  4.7038e-01,\n",
       "            5.2445e-01,  4.4221e-01,  8.2844e-01,  5.3752e-01,  6.0043e-01,\n",
       "            3.2888e-01,  6.1856e-01,  4.4933e-01,  4.4221e-01,  7.0457e-01,\n",
       "            7.8884e-01,  4.3504e-01,  4.5640e-01,  7.4759e-01,  3.7577e-01,\n",
       "            7.5284e-01,  8.0884e-01,  7.7357e-01,  3.9092e-01,  4.9778e-01,\n",
       "            7.4232e-01,  4.0584e-01,  6.5962e-01,  7.3701e-01,  3.6810e-01,\n",
       "            4.4221e-01,  3.7577e-01,  6.3047e-01,  3.7577e-01,  7.5284e-01,\n",
       "            4.1322e-01,  7.7868e-01,  8.4768e-01,  5.3101e-01,  4.1322e-01,\n",
       "            4.1322e-01,  6.8795e-01,  7.3168e-01,  5.2445e-01,  5.8816e-01,\n",
       "            5.1120e-01,  3.9841e-01,  3.6038e-01,  5.3752e-01,  6.8235e-01,\n",
       "            4.7038e-01,  7.1005e-01,  6.5962e-01,  4.1322e-01,  7.5284e-01,\n",
       "            5.7574e-01,  6.7672e-01,  4.4933e-01,  6.6535e-01,  5.6316e-01,\n",
       "            4.7038e-01,  4.3504e-01,  6.0043e-01,  3.4476e-01,  4.1322e-01,\n",
       "            6.8235e-01,  6.3637e-01,  5.8197e-01,  4.0584e-01,  3.9092e-01,\n",
       "            6.7672e-01,  6.1256e-01,  3.8337e-01,  7.7868e-01,  3.8337e-01,\n",
       "            6.0043e-01,  7.2092e-01,  4.2782e-01,  5.0451e-01,  4.0584e-01,\n",
       "            8.4768e-01,  5.3752e-01,  6.3047e-01,  6.4223e-01,  3.6810e-01,\n",
       "            3.4476e-01,  4.2055e-01,  7.6326e-01,  3.7577e-01,  3.6810e-01,\n",
       "            7.6326e-01,  6.4806e-01,  5.3752e-01,  6.7105e-01,  4.9100e-01,\n",
       "            5.0451e-01,  3.5260e-01,  7.6326e-01,  3.8337e-01,  4.4221e-01,\n",
       "            7.1550e-01,  7.3701e-01,  5.3101e-01,  6.4223e-01,  6.6535e-01,\n",
       "            5.3752e-01,  7.6326e-01,  6.0651e-01,  5.5681e-01,  7.0457e-01,\n",
       "            3.4476e-01,  5.7574e-01,  3.8337e-01,  3.5260e-01,  3.9092e-01,\n",
       "            7.8884e-01,  4.4933e-01,  7.6843e-01,  6.6535e-01,  3.1275e-01,\n",
       "            6.7672e-01,  3.3685e-01,  5.5681e-01,  4.8417e-01,  3.2085e-01,\n",
       "            7.1550e-01,  6.8795e-01,  7.0457e-01,  6.7105e-01,  4.0584e-01,\n",
       "            7.9387e-01,  4.8417e-01,  6.1256e-01,  7.3701e-01, -4.3899e-01,\n",
       "           -9.2713e-02,  3.3685e-01, -2.5094e-01, -3.4055e-01,  6.2453e-01,\n",
       "            7.6843e-01,  1.2554e-01,  2.1907e-01,  6.4916e-02,  5.5042e-01,\n",
       "            3.0459e-01, -1.9537e-01, -4.0509e-01,  5.1120e-01,  6.7105e-01,\n",
       "            2.2357e-02, -6.8615e-02,  4.8417e-01,  2.2357e-02,  4.8417e-01,\n",
       "            3.8337e-01, -3.7230e-01, -1.4272e-01, -2.2277e-01,  5.7574e-01,\n",
       "            5.1120e-01, -5.8741e-01,  6.0043e-01,  7.7868e-01,  2.8806e-01,\n",
       "            4.3863e-02, -3.8856e-01, -4.0509e-01, -4.9210e-01, -2.0897e-01,\n",
       "           -3.4055e-01, -3.2504e-01, -4.7408e-01, -2.0897e-01, -4.3899e-01,\n",
       "           -3.2504e-01, -1.0498e-01, -4.7408e-01, -4.2189e-01, -2.3675e-01,\n",
       "           -2.0897e-01, -4.5638e-01, -2.9474e-01, -5.6781e-02, -3.4055e-01,\n",
       "           -4.3899e-01, -4.5638e-01, -1.5563e-01, -3.5630e-01, -5.1045e-01,\n",
       "           -1.1741e-01, -8.0591e-02, -2.5094e-01, -4.9210e-01, -3.0978e-01,\n",
       "           -2.0897e-01, -2.6533e-01, -2.6533e-01, -4.3899e-01, -1.9537e-01,\n",
       "           -4.5638e-01, -5.6761e-01, -4.9210e-01, -5.1045e-01, -3.5630e-01,\n",
       "           -2.6533e-01, -4.7408e-01, -1.5563e-01, -4.9210e-01, -2.9474e-01,\n",
       "           -3.7230e-01, -1.8194e-01, -9.2713e-02, -2.0897e-01, -1.9537e-01,\n",
       "           -5.1045e-01, -4.7408e-01, -9.2713e-02, -6.8615e-02, -5.4819e-01,\n",
       "           -2.3675e-01, -1.2998e-01, -3.7230e-01, -4.3899e-01, -5.2914e-01,\n",
       "           -2.3675e-01,  2.2357e-02, -6.2823e-01, -2.2277e-01, -2.9474e-01,\n",
       "           -5.2914e-01, -5.4819e-01, -1.6870e-01, -2.5094e-01, -2.2277e-01,\n",
       "           -1.4272e-01, -1.5563e-01, -3.4055e-01, -4.0509e-01, -1.8194e-01,\n",
       "           -1.5563e-01, -1.4272e-01, -2.7992e-01, -1.9537e-01, -3.4055e-01,\n",
       "           -1.1741e-01, -3.5630e-01, -3.2504e-01, -2.3675e-01, -3.5630e-01,\n",
       "           -1.8194e-01, -2.0897e-01, -1.6870e-01, -3.4055e-01, -3.2504e-01,\n",
       "           -2.2277e-01, -9.2713e-02, -2.2277e-01, -3.8856e-01, -3.0978e-01,\n",
       "           -3.7230e-01, -3.3524e-02, -9.2713e-02, -4.9210e-01, -4.0509e-01,\n",
       "           -2.0897e-01, -3.5630e-01, -2.5094e-01], requires_grad=True),\n",
       "   tensor([-0.0206, -0.0376, -0.2971, -0.2466, -0.2536,  0.1009, -0.1654, -0.2607,\n",
       "           -0.1785,  0.0180, -0.2120,  0.0234, -0.0666,  0.0808,  0.0234, -0.1086,\n",
       "            0.0446, -0.0904, -0.1025, -0.1148, -0.1851,  0.2699, -0.1720, -0.2971,\n",
       "           -0.0206, -0.1148, -0.1984,  0.0287, -0.2536, -0.0039, -0.3661,  0.0655,\n",
       "           -0.3196, -0.0784, -0.2188, -0.1398, -0.1461,  0.0808, -0.1654, -0.0549,\n",
       "            0.2003, -0.3272,  0.0071,  0.2270,  0.0909, -0.2188,  0.1399, -0.2824,\n",
       "           -0.3504, -0.2188, -0.2971,  0.2093, -0.0607, -0.1590, -0.0376, -0.3504,\n",
       "            0.1351, -0.1398,  0.0959, -0.1590,  0.1303, -0.1461, -0.2052, -0.0784,\n",
       "           -0.3741, -0.0844, -0.2326,  0.2226, -0.1525, -0.1335, -0.1525, -0.0904,\n",
       "           -0.1720,  0.2357,  0.0016, -0.0206, -0.2257, -0.2679, -0.0607,  0.0499,\n",
       "           -0.1720, -0.1335, -0.0607, -0.2257, -0.0376, -0.1086, -0.2188,  0.1912,\n",
       "           -0.2257,  0.2137,  0.1542, -0.2326, -0.0319,  0.0859, -0.3349,  0.0603,\n",
       "           -0.2751, -0.3272,  0.0016,  0.0757, -0.0784, -0.2824,  0.0959,  0.0341,\n",
       "           -0.1272,  0.0125, -0.2971,  0.2357, -0.3426,  0.2093,  0.0125, -0.1335,\n",
       "           -0.3741, -0.0784, -0.2326, -0.1335, -0.3349,  0.0394, -0.0964, -0.2052,\n",
       "            0.1059, -0.3046, -0.2679, -0.1210, -0.1525, -0.0666, -0.1590, -0.1851,\n",
       "           -0.2395, -0.0319,  0.0706, -0.2466, -0.1398,  0.0180, -0.2679,  0.2443,\n",
       "           -0.0491,  0.0655,  0.0706,  0.0394,  0.0016, -0.1525, -0.0319, -0.0725,\n",
       "           -0.3196, -0.2751, -0.2395, -0.1918,  0.0125,  0.1495,  0.0016, -0.3120,\n",
       "           -0.1148, -0.0094, -0.1148, -0.0549, -0.2824, -0.1525, -0.2120,  0.0551,\n",
       "           -0.1210, -0.1851, -0.3349, -0.1984, -0.1335, -0.0491, -0.0094, -0.3120,\n",
       "           -0.2120,  0.0341, -0.3196, -0.0844,  0.0446, -0.3741, -0.2052, -0.1461,\n",
       "           -0.3196, -0.7716, -0.5019, -0.4149, -0.8717, -0.5774, -0.2897, -0.8983,\n",
       "           -0.5390, -0.3349, -0.4839, -0.3196, -0.4149, -0.5019, -0.7716, -0.2466,\n",
       "           -0.4317, -0.4402, -0.7365, -0.9977, -0.3582, -0.7026, -0.3349, -0.5296,\n",
       "           -0.4839, -0.8849, -0.5677, -0.3902, -0.5390, -0.5296, -0.7597, -0.6073,\n",
       "           -0.7716, -0.5019, -0.3349, -0.8849, -0.4929, -0.4149, -0.3984, -0.7957,\n",
       "           -0.7716, -0.6806, -0.4750, -0.5580, -0.8586, -0.7835, -0.5580, -0.4488,\n",
       "           -0.7026, -0.6073, -0.5390, -0.7957, -0.7365, -0.3661, -0.5019, -0.8329,\n",
       "           -0.4149, -0.8204, -0.4233, -0.4750, -0.5110, -0.4662, -0.8204, -0.7480,\n",
       "           -0.4317, -0.5296, -0.3741, -0.4839, -0.8586, -0.8079, -0.7138, -0.9977,\n",
       "           -0.9977, -0.4402, -0.7716, -0.9257, -0.4402, -0.5484, -0.4488, -0.8983,\n",
       "           -0.5484, -0.5110, -0.3661, -0.4149, -0.4317, -0.6698, -0.5296, -0.4066,\n",
       "           -0.9119, -0.5110, -0.3349, -0.6380, -0.7138, -0.6174, -0.7957, -0.4066,\n",
       "           -0.3426, -0.4662, -0.5774, -0.4750, -0.5873, -0.8204, -0.5203, -0.6174,\n",
       "           -0.5484, -0.5019, -0.4839, -0.7597, -0.4402, -0.4750, -0.4750, -0.4317,\n",
       "           -0.7597, -0.9119, -0.4066, -0.3272, -0.6174, -0.5296, -0.4149, -0.6277,\n",
       "           -0.7480, -0.7138, -0.1148, -0.0206, -0.1785,  0.2948, -0.1272,  0.0125,\n",
       "            0.2614, -0.1272,  0.3886,  0.1636,  0.3029,  0.3960,  0.4034, -0.0319,\n",
       "            0.1059,  0.0016,  0.1542,  0.3110,  0.2357,  0.4107,  0.2003,  0.3029,\n",
       "            0.1542,  0.1912,  0.3960,  0.1821, -0.2751,  0.0287, -0.1785,  0.1447,\n",
       "            0.0655,  0.1447, -0.3046,  0.0551, -0.2120, -0.0491, -0.1461, -0.2188,\n",
       "           -0.0150, -0.2326,  0.1157, -0.0206, -0.2824, -0.1851,  0.1108, -0.1720,\n",
       "            0.0287,  0.0341,  0.1729,  0.1059, -0.2897,  0.1447,  0.2003, -0.2257,\n",
       "            0.1821,  0.1912, -0.1785, -0.0964,  0.1059,  0.0909,  0.0446,  0.0655,\n",
       "           -0.1984,  0.0287,  0.1636,  0.0499, -0.0206,  0.1059,  0.1729, -0.0433,\n",
       "            0.0551,  0.2270, -0.3046,  0.0341,  0.2093, -0.2326,  0.0757, -0.2607,\n",
       "           -0.2607, -0.0319,  0.1059,  0.2270,  0.0394,  0.0706,  0.2270, -0.2536,\n",
       "           -0.0433,  0.0603,  0.1351,  0.1636,  0.2357, -0.1398, -0.1086,  0.2357,\n",
       "           -0.2052,  0.0234, -0.0844, -0.2257,  0.1351,  0.0016,  0.1912, -0.0319,\n",
       "            0.1157,  0.2181,  0.0808,  0.2529, -0.1335,  0.2357,  0.0125, -0.0491,\n",
       "            0.2443, -0.0904, -0.2897,  0.1821, -0.0150,  0.0125,  0.2093,  0.0287],\n",
       "          requires_grad=True),\n",
       "   tensor([ 0.8275,  0.7373,  0.8173,  0.4859,  0.6976,  0.7807,  0.7861,  0.7966,\n",
       "            0.5347,  0.8624,  0.7966,  0.8275,  0.5279,  0.4788,  0.6442,  0.6860,\n",
       "            0.7647,  0.6195,  0.7754,  0.8070,  0.6381,  0.4570,  0.4496,  0.8326,\n",
       "            0.5616,  0.6623,  0.7034,  0.6381,  0.7807,  0.5001,  0.5347,  0.7148,\n",
       "            0.5813,  0.4421,  0.6006,  0.8070,  0.6918,  0.4195,  0.6918,  0.5549,\n",
       "            0.5616,  0.7205,  0.8818,  0.8722,  0.6918,  0.6976,  0.2171,  0.6683,\n",
       "            0.7538,  0.4347,  0.5071,  0.5747,  0.5141,  0.5483,  0.4788,  0.6801,\n",
       "            0.3885,  0.8722,  0.6860,  0.4421,  0.6442,  0.6195,  0.8070,  0.6860,\n",
       "            0.7701,  0.5483,  0.6132,  0.5141,  0.6918,  0.4195,  0.7913,  0.7205,\n",
       "            0.5747,  0.3565,  0.6918,  0.5141,  0.4195,  0.6976,  0.4931,  0.8018,\n",
       "            0.4041,  0.5141,  0.5210,  0.5747,  0.7091,  0.5616,  0.4041,  0.7807,\n",
       "            0.4859,  0.7966,  0.4195,  0.7966,  0.4859,  0.7701,  0.4347,  0.5001,\n",
       "            0.4643,  0.5682,  0.6742,  0.6683,  0.7373,  0.5813,  0.6683,  0.6381,\n",
       "            0.5877,  0.6503,  0.8173,  0.7861,  0.5813,  0.3150,  0.8070,  0.2979,\n",
       "            0.2538,  0.4347,  0.4716,  0.6563,  0.6069,  0.2717,  0.6257,  0.7483,\n",
       "            0.5942,  0.7701,  0.6976,  0.5071,  0.5549,  0.3646,  0.5747,  0.5279,\n",
       "            0.5549,  0.4859,  0.8070,  0.3483,  0.4195,  0.7701,  0.3318,  0.5001,\n",
       "            0.6563,  0.5210,  0.7754,  0.5616,  0.5210,  0.4421,  0.3401,  0.6801,\n",
       "            0.5347,  0.8722,  0.1790,  0.6563,  0.7807,  0.4716,  0.6976,  0.6503,\n",
       "            0.5747,  0.7483,  0.8624,  0.3885,  0.6069,  0.2448,  0.3065,  0.4347,\n",
       "            0.6563,  0.2171,  0.6918,  0.8275,  0.7373,  0.3726,  0.5347,  0.4570,\n",
       "            0.4931,  0.8122,  0.6623,  0.4347,  0.7861,  0.4931,  0.3565,  0.7538,\n",
       "            0.2628,  0.4859,  0.6503,  0.5279,  0.6006,  0.5071,  0.4788,  0.6801,\n",
       "            0.5616,  0.8624,  0.5347,  0.7754,  0.5141, -0.3953, -0.3126, -0.2072,\n",
       "           -0.2813, -0.1652, -0.1930, -0.5437, -0.0990,  0.0102, -0.4854, -0.4484,\n",
       "           -0.0862, -0.5437, -0.1119, -0.5437, -0.2510, -0.5639, -0.2216, -0.2362,\n",
       "           -0.2968, -0.6271, -0.5845, -0.2968, -0.4667, -0.5045, -0.2968, -0.5845,\n",
       "           -0.0862, -0.3448, -0.2660, -0.0737, -0.0862, -0.4854,  0.0661, -0.2216,\n",
       "           -0.3953, -0.4484, -0.4126, -0.1930, -0.2510, -0.4303, -0.4303, -0.5639,\n",
       "           -0.5045, -0.6055,  0.1495, -0.3614, -0.1382,  0.0661, -0.5845,  0.1293,\n",
       "           -0.5845, -0.3782, -0.0369, -0.3448, -0.5045, -0.2362, -0.2813, -0.0013,\n",
       "           -0.3448, -0.3126, -0.4126, -0.3782, -0.3448, -0.4667, -0.2072, -0.0612,\n",
       "           -0.4126, -0.4484, -0.4303, -0.0130, -0.2510, -0.5639,  0.0552, -0.4854,\n",
       "           -0.1516, -0.0612, -0.3614, -0.3614, -0.2813, -0.3782, -0.0862, -0.5239,\n",
       "           -0.2510, -0.2660, -0.4126,  0.1087,  0.0329, -0.1930, -0.0990, -0.4303,\n",
       "           -0.1790, -0.6271, -0.4667, -0.3126, -0.4126, -0.1382, -0.2968, -0.2813,\n",
       "           -0.1516, -0.2362, -0.3614, -0.3286,  0.3806,  0.0441,  0.1790,  0.1790,\n",
       "            0.0102, -0.0490,  0.1693,  0.1293,  0.1693,  0.2538, -0.1250, -0.0249,\n",
       "            0.1693,  0.1983,  0.1790, -0.0862, -0.0490, -0.0130,  0.1790,  0.3065,\n",
       "            0.2357,  0.0876,  0.0661,  0.1293,  0.2628,  0.1495,  0.3318,  0.2979,\n",
       "           -0.0130,  0.4041,  0.0982,  0.1594,  0.2805,  0.3318,  0.3483,  0.0769,\n",
       "            0.2357, -0.0737,  0.1495,  0.0102,  0.1983,  0.0876, -0.0990, -0.0990,\n",
       "            0.0102, -0.0249,  0.1495,  0.0216,  0.2077,  0.3234,  0.0876,  0.1394,\n",
       "            0.2448, -0.1382,  0.0661,  0.4271,  0.1495,  0.4041,  0.3646, -0.1119,\n",
       "            0.0441, -0.0490,  0.1887,  0.1394,  0.4118,  0.1594,  0.0982, -0.1250,\n",
       "            0.2628,  0.1087,  0.3565, -0.0862,  0.2357,  0.4195,  0.3401,  0.2357,\n",
       "           -0.1516,  0.2264,  0.2628,  0.2717,  0.3065,  0.1594,  0.2628,  0.0982,\n",
       "            0.2892,  0.3150,  0.0329, -0.1516,  0.0102, -0.0490,  0.0661,  0.3318,\n",
       "            0.0769,  0.1594, -0.0490, -0.0013, -0.0130,  0.0769,  0.1790,  0.1190,\n",
       "            0.0329,  0.3234, -0.0612,  0.0441,  0.0982,  0.1495,  0.0769,  0.2077,\n",
       "            0.2357, -0.1516,  0.2717, -0.0130,  0.2077, -0.1790,  0.1394, -0.0130,\n",
       "           -0.0130, -0.0737,  0.2171, -0.0249,  0.3565,  0.0661,  0.0552,  0.2171,\n",
       "            0.2357,  0.1594,  0.0441,  0.1983,  0.0769, -0.0013,  0.3483,  0.1293,\n",
       "            0.2077, -0.0249,  0.1790,  0.1983,  0.2357,  0.1790,  0.2979,  0.1190,\n",
       "            0.2448,  0.1594,  0.0552,  0.1087,  0.3318, -0.0013,  0.1293,  0.1087,\n",
       "            0.3234,  0.1983,  0.2805, -0.0612, -0.0249, -0.0490, -0.0249,  0.3150,\n",
       "           -0.0612, -0.0249,  0.3150,  0.3318,  0.0329,  0.0329,  0.3234, -0.0369,\n",
       "            0.3726, -0.1250, -0.0130,  0.2892,  0.7701,  0.8722,  0.8426,  0.3150,\n",
       "            0.3483,  0.8526,  0.8526,  0.6442,  0.2805,  0.6918,  0.5942,  0.8624,\n",
       "            0.2077,  0.7373,  0.2979,  0.3318,  0.3318,  0.6069,  0.2077,  0.7592,\n",
       "            0.6563,  0.5141, -0.0249, -0.1250, -0.0737, -0.1516, -0.4303,  0.1495,\n",
       "            0.1293,  0.0441, -0.2362, -0.2362,  0.0661, -0.1516, -0.3953],\n",
       "          requires_grad=True)],\n",
       "  'Time to Maturity': [tensor([0.1587, 0.1587, 0.2698, 0.2698, 0.1587, 0.5198, 0.1587, 0.1587, 0.0476,\n",
       "           0.1587, 0.0476, 0.2698, 0.5198, 0.1587, 0.0159, 0.0476, 0.1587, 0.2698,\n",
       "           0.2698, 0.0714, 0.5198, 0.2698, 0.4087, 0.4087, 0.0476, 0.2698, 0.0476,\n",
       "           0.4087, 0.4087, 0.0476, 0.4087, 0.0159, 0.2698, 0.1587, 0.1587, 0.0476,\n",
       "           0.5198, 0.1587, 0.1587, 0.1587, 0.1587, 0.0714, 0.1587, 0.0159, 0.0992,\n",
       "           0.0476, 0.1587, 0.0992, 0.0159, 0.4087, 0.1587, 0.1587, 0.5198, 0.0714,\n",
       "           0.0714, 0.0476, 0.0476, 0.0476, 0.0714, 0.0159, 0.2698, 0.1587, 0.0476,\n",
       "           0.5198, 0.1587, 0.5198, 0.2698, 0.5198, 0.0992, 0.2698, 0.1587, 0.4087,\n",
       "           0.0476, 0.0714, 0.0476, 0.4087, 0.1587, 0.2698, 0.0476, 0.1587, 0.1587,\n",
       "           0.0714, 0.5198, 0.2698, 0.5198, 0.0159, 0.1587, 0.1587, 0.1587, 0.5198,\n",
       "           0.0714, 0.1587, 0.0714, 0.0992, 0.0476, 0.1587, 0.0992, 0.1587, 0.5198,\n",
       "           0.1587, 0.0476, 0.0159, 0.1587, 0.0476, 0.0476, 0.1587, 0.1587, 0.1587,\n",
       "           0.1587, 0.0159, 0.0714, 0.0476, 0.0476, 0.1587, 0.0714, 0.2698, 0.1587,\n",
       "           0.0159, 0.0476, 0.0476, 0.0476, 0.0476, 0.2698, 0.1587, 0.1587, 0.4087,\n",
       "           0.2698, 0.5198, 0.0476, 0.0159, 0.4087, 0.5198, 0.2698, 0.0476, 0.2698,\n",
       "           0.0476, 0.4087, 0.0992, 0.0476, 0.0714, 0.0714, 0.1587, 0.1587, 0.0714,\n",
       "           0.1587, 0.1587, 0.0992, 0.4087, 0.4087, 0.1587, 0.0992, 0.4087, 0.5198,\n",
       "           0.0476, 0.5198, 0.0992, 0.4087, 0.0476, 0.1587, 0.1587, 0.0476, 0.1587,\n",
       "           0.2698, 0.5198, 0.0992, 0.1587, 0.1587, 0.1587, 1.2421, 2.3254, 2.3254,\n",
       "           1.2421, 1.2421, 2.3254, 2.3254, 1.2421, 1.2421, 1.2421, 2.3254, 2.3254,\n",
       "           1.2421, 2.3254, 1.2421, 2.3254, 1.2421, 1.2421, 2.3254, 1.2421, 2.3254,\n",
       "           1.2421, 2.3254, 1.2421, 2.3254, 2.3254, 2.3254, 1.2421, 2.3254, 1.2421,\n",
       "           1.2421, 2.3254, 1.2421, 0.0992, 0.0476, 0.0992, 0.8810, 0.0992, 0.8810,\n",
       "           0.2698, 0.0476, 0.0476, 0.8810, 0.1587, 0.8810, 0.0714, 0.8810, 0.1587,\n",
       "           0.1587, 0.5198, 0.2698, 0.0714, 0.8810, 0.2698, 0.4087, 0.4087, 0.0714,\n",
       "           0.8810, 0.0476, 0.5198, 0.1587, 0.2698, 0.1587, 0.8810, 0.5198, 0.0476,\n",
       "           0.0476, 0.0159, 0.5198, 0.8810, 0.4087, 0.0159, 0.5198, 0.5198, 0.5198,\n",
       "           0.4087, 0.8810, 0.4087, 0.0476, 0.8810, 0.0476, 0.0992, 0.5198, 0.8810,\n",
       "           0.0476, 0.0476, 0.8810, 0.5198, 0.1587, 0.0159, 0.2698, 0.2698, 0.1587,\n",
       "           0.0476, 0.0714, 0.5198, 0.5198, 0.5198, 0.5198, 0.4087, 0.5198, 0.4087,\n",
       "           0.2698, 0.5198, 0.4087, 0.0159, 0.0714, 0.4087, 0.4087, 0.0159, 0.4087,\n",
       "           0.2698, 0.1587, 0.4087, 0.5198, 0.0476, 0.2698, 0.0476, 0.0159, 0.0476,\n",
       "           0.8810, 0.2698, 0.0476, 0.0476, 0.1587, 0.8810, 0.5198, 0.4087, 0.1587,\n",
       "           0.1587, 0.0714, 0.4087, 0.8810, 0.2698, 0.1587, 0.0159, 0.4087, 0.2698,\n",
       "           0.0714, 0.2698, 0.0476, 0.2698, 0.5198, 0.5198, 0.0992, 0.5198, 0.0476,\n",
       "           0.0714, 0.0714, 0.5198, 0.5198, 0.0476, 0.2698, 0.0159, 0.1587, 0.1587,\n",
       "           0.0714, 0.0992, 0.5198, 0.8810, 0.2698, 0.8810, 0.8810, 0.2698, 0.0159,\n",
       "           0.1587, 0.8810, 0.8810, 0.0714, 0.0992, 0.5198, 0.0992, 0.2698, 0.0476,\n",
       "           0.4087, 0.4087, 0.0476, 0.2698, 0.8810, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254, 2.3254,\n",
       "           2.3254, 2.3254, 0.8810, 0.8810, 1.2421, 0.8810, 0.8810, 0.8810, 0.8810,\n",
       "           1.2421, 0.8810, 0.8810, 1.2421, 1.2421, 1.2421, 1.2421, 0.5198, 1.2421,\n",
       "           0.5198, 1.2421, 0.8810, 0.8810, 0.8810, 0.8810, 1.2421, 0.8810, 0.8810,\n",
       "           0.8810, 0.8810, 1.2421, 0.8810, 0.8810, 1.2421, 0.8810, 0.8810, 0.5198,\n",
       "           0.8810, 1.2421, 0.8810, 1.2421, 0.8810, 0.8810, 1.2421, 0.8810, 0.5198,\n",
       "           0.8810, 0.8810, 0.8810, 1.2421, 0.5198, 0.8810, 0.8810, 1.2421, 0.8810,\n",
       "           0.8810, 0.5198, 0.5198, 0.8810, 1.2421, 0.8810, 0.8810, 1.2421, 1.2421,\n",
       "           1.2421, 1.2421, 0.8810, 0.5198, 0.8810, 1.2421, 0.5198, 0.8810, 0.8810,\n",
       "           1.2421, 0.8810, 0.8810, 1.2421, 0.5198, 0.5198, 1.2421, 0.5198, 0.5198,\n",
       "           0.8810], requires_grad=True),\n",
       "   tensor([0.4365, 0.0754, 0.0159, 0.6865, 0.6865, 0.5754, 0.0754, 0.3254, 0.3254,\n",
       "           0.0992, 0.1270, 0.4365, 0.4365, 0.4365, 0.0437, 0.1865, 0.4365, 0.0992,\n",
       "           0.4365, 0.1865, 0.0159, 0.3254, 0.0159, 0.1270, 0.0992, 0.4365, 0.6865,\n",
       "           0.0437, 0.6865, 0.1865, 0.0159, 0.1865, 0.1270, 0.6865, 0.3254, 0.0754,\n",
       "           0.0754, 0.1865, 0.0159, 0.0754, 0.3254, 0.1270, 0.1865, 0.5754, 0.1865,\n",
       "           0.0159, 0.5754, 0.3254, 0.1270, 0.0754, 0.3254, 0.5754, 0.0159, 0.4365,\n",
       "           0.1865, 0.0159, 0.3254, 0.6865, 0.1865, 0.1865, 0.5754, 0.4365, 0.3254,\n",
       "           0.0992, 0.3254, 0.1865, 0.0754, 0.5754, 0.0754, 0.1865, 0.0159, 0.1865,\n",
       "           0.0754, 0.3254, 0.0992, 0.0754, 0.0754, 0.1270, 0.4365, 0.1865, 0.3254,\n",
       "           0.0754, 0.0754, 0.0159, 0.1865, 0.4365, 0.0754, 0.3254, 0.1270, 0.5754,\n",
       "           0.5754, 0.6865, 0.0754, 0.1270, 0.0437, 0.4365, 0.6865, 0.3254, 0.0754,\n",
       "           0.4365, 0.0992, 0.0159, 0.5754, 0.0754, 0.6865, 0.4365, 0.5754, 0.5754,\n",
       "           0.1270, 0.3254, 0.1270, 0.6865, 0.3254, 0.0754, 0.0159, 0.6865, 0.0159,\n",
       "           0.0437, 0.1865, 0.0159, 0.3254, 0.1270, 0.0437, 0.4365, 0.1865, 0.0992,\n",
       "           0.6865, 0.3254, 0.0437, 0.0159, 0.1865, 0.0754, 0.0159, 0.0754, 0.0159,\n",
       "           0.3254, 0.1865, 0.1270, 0.1865, 0.4365, 0.4365, 0.4365, 0.0992, 0.0159,\n",
       "           0.0437, 0.6865, 0.0437, 0.6865, 0.1270, 0.5754, 0.0992, 0.0159, 0.0159,\n",
       "           0.1865, 0.0992, 0.0992, 0.6865, 0.1865, 0.3254, 0.1865, 0.6865, 0.6865,\n",
       "           0.1270, 0.1865, 0.4365, 0.4365, 0.0992, 0.1270, 0.1270, 0.1270, 0.0159,\n",
       "           0.6865, 0.1865, 0.0159, 0.0754, 0.0159, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.0476, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.0476, 1.0476, 1.4087, 1.4087, 1.4087, 1.0476,\n",
       "           1.4087, 1.4087, 1.0476, 1.0476, 1.4087, 1.0476, 1.0476, 1.4087, 1.4087,\n",
       "           1.0476, 1.4087, 1.4087, 1.4087, 1.0476, 1.0476, 1.0476, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.0476, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.0476, 1.0476, 1.0476, 1.0476, 1.0476, 1.4087, 1.0476, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.0476, 1.0476, 1.4087, 1.4087, 1.4087, 1.4087,\n",
       "           1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.4087, 1.0476, 1.4087, 1.0476,\n",
       "           1.4087, 1.0476, 1.4087, 1.0476, 1.0476, 0.6865, 0.3254, 0.0754, 0.1865,\n",
       "           0.0754, 0.0754, 0.3254, 0.1865, 0.3254, 0.5754, 0.3254, 0.0754, 0.0754,\n",
       "           0.0754, 0.1865, 0.6865, 0.6865, 0.1865, 0.0754, 0.0754, 0.1865, 0.1865,\n",
       "           0.0754, 0.1865, 0.6865, 0.3254, 0.0754, 0.5754, 0.4365, 0.3254, 0.0754,\n",
       "           0.6865, 0.1865, 0.4365, 0.4365, 0.6865, 0.0754, 0.3254, 0.5754, 0.3254,\n",
       "           0.1865, 0.5754, 0.6865, 0.3254, 0.6865, 0.4365, 0.0754, 0.6865, 0.3254,\n",
       "           0.3254, 0.0754, 0.3254, 0.1865, 0.3254, 0.6865, 0.5754, 0.5754, 0.6865,\n",
       "           0.3254, 0.4365, 0.5754, 0.0754, 0.6865, 0.0754, 0.6865, 0.3254, 0.0754,\n",
       "           0.6865, 0.5754, 0.3254, 0.5754, 0.6865, 0.5754, 0.1865, 0.0754, 0.4365,\n",
       "           0.1865, 0.5754, 0.1865, 0.5754, 0.0754, 0.1865, 0.6865, 0.5754, 0.5754,\n",
       "           0.5754, 0.0754, 0.3254, 0.4365, 0.4365, 0.5754, 0.3254, 0.5754, 0.1865,\n",
       "           0.6865, 0.3254, 0.4365, 0.3254, 0.5754, 0.1865, 0.3254, 0.3254, 0.5754,\n",
       "           0.0754, 0.0754, 0.5754, 0.6865, 0.3254, 0.0754, 0.3254, 0.6865, 0.6865,\n",
       "           0.0159, 0.1865, 0.6865, 0.0754, 0.5754, 0.3254, 0.6865, 0.1865, 0.5754,\n",
       "           0.6865, 0.5754, 0.5754, 0.5754, 0.6865, 0.0754, 0.6865, 0.1865, 0.5754,\n",
       "           0.5754, 0.5754, 0.3254, 0.1865, 0.5754, 0.6865, 0.1865, 0.1865, 0.4365,\n",
       "           0.1865, 0.4365, 0.0754, 0.5754, 0.6865, 0.0754, 0.6865, 0.6865, 0.6865,\n",
       "           0.6865, 0.3254, 0.5754, 0.5754, 0.5754, 0.6865, 0.1865, 0.3254, 0.5754,\n",
       "           0.4365, 0.0754, 0.1865, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532, 2.8532,\n",
       "           2.8532, 0.0754, 0.6865, 1.0476, 0.5754, 0.0754, 0.1865, 0.5754, 0.1865,\n",
       "           0.6865, 0.4365, 1.4087, 0.1865, 0.6865, 1.4087, 0.1865, 0.4365, 1.4087,\n",
       "           1.4087, 0.5754, 0.5754, 1.0476, 1.0476, 0.4365, 1.4087, 0.6865, 0.6865,\n",
       "           1.4087, 0.4365, 0.6865, 0.4365, 0.1865, 0.1865, 1.0476, 1.0476, 0.4365,\n",
       "           1.0476, 0.5754, 1.0476, 0.6865, 0.4365, 0.4365, 0.5754, 0.6865, 0.0754,\n",
       "           1.0476, 0.3254, 1.0476, 0.4365, 0.4365, 0.6865, 0.6865, 0.6865, 1.0476,\n",
       "           0.3254, 0.3254, 0.6865, 0.6865, 0.4365, 0.6865, 0.1865, 1.0476, 1.4087,\n",
       "           0.5754, 1.0476, 1.4087, 1.4087, 0.4365, 0.5754, 0.6865, 0.5754, 0.6865,\n",
       "           0.0754, 0.5754, 0.5754, 1.4087, 1.4087, 1.4087, 1.0476, 0.4365, 1.4087,\n",
       "           0.0754, 0.6865, 0.6865, 0.1865, 1.4087, 0.6865, 0.5754, 0.6865, 1.0476,\n",
       "           1.0476, 1.4087, 0.1865, 0.1865, 0.0754, 0.0754, 1.4087, 1.4087, 0.3254,\n",
       "           0.3254, 0.5754, 0.3254, 1.0476], requires_grad=True),\n",
       "   tensor([0.2421, 0.0437, 0.0159, 0.4921, 0.4921, 0.2421, 0.0714, 0.2421, 0.2421,\n",
       "           0.0714, 0.1310, 0.2421, 0.2421, 0.2421, 0.0159, 0.1310, 0.2421, 0.0714,\n",
       "           0.2421, 0.1310, 0.0159, 0.1310, 0.0159, 0.1310, 0.0714, 0.2421, 0.4921,\n",
       "           0.0159, 0.4921, 0.1310, 0.0159, 0.1310, 0.1310, 0.4921, 0.2421, 0.0714,\n",
       "           0.0437, 0.1310, 0.0159, 0.0437, 0.1310, 0.1310, 0.1310, 0.2421, 0.1310,\n",
       "           0.0159, 0.2421, 0.2421, 0.1310, 0.0714, 0.2421, 0.2421, 0.0159, 0.2421,\n",
       "           0.1310, 0.0159, 0.1310, 0.4921, 0.1310, 0.1310, 0.2421, 0.2421, 0.2421,\n",
       "           0.0992, 0.2421, 0.1310, 0.0714, 0.2421, 0.0714, 0.1310, 0.0159, 0.1310,\n",
       "           0.0437, 0.1310, 0.0992, 0.0437, 0.0437, 0.1310, 0.2421, 0.1310, 0.2421,\n",
       "           0.0437, 0.0437, 0.0159, 0.1310, 0.2421, 0.0714, 0.1310, 0.1310, 0.2421,\n",
       "           0.2421, 0.4921, 0.0437, 0.0992, 0.0437, 0.2421, 0.4921, 0.2421, 0.0437,\n",
       "           0.2421, 0.0992, 0.0159, 0.2421, 0.0437, 0.4921, 0.2421, 0.4921, 0.2421,\n",
       "           0.1310, 0.1310, 0.0992, 0.4921, 0.2421, 0.0437, 0.0159, 0.4921, 0.0159,\n",
       "           0.0159, 0.1310, 0.0159, 0.1310, 0.1310, 0.0437, 0.2421, 0.1310, 0.0714,\n",
       "           0.4921, 0.2421, 0.0437, 0.0159, 0.1310, 0.0714, 0.0159, 0.0437, 0.0159,\n",
       "           0.1310, 0.1310, 0.0992, 0.1310, 0.2421, 0.2421, 0.2421, 0.0992, 0.0159,\n",
       "           0.0437, 0.4921, 0.0437, 0.4921, 0.0992, 0.2421, 0.0714, 0.0159, 0.0159,\n",
       "           0.1310, 0.0714, 0.0992, 0.4921, 0.1310, 0.2421, 0.1310, 0.4921, 0.4921,\n",
       "           0.1310, 0.1310, 0.2421, 0.2421, 0.0992, 0.1310, 0.1310, 0.0992, 0.0159,\n",
       "           0.4921, 0.1310, 0.0159, 0.0437, 0.0159, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 0.4921, 0.4921, 0.9643, 0.9643, 0.8532,\n",
       "           0.9643, 0.2421, 0.2421, 0.1310, 0.4921, 0.9643, 0.1310, 0.1310, 0.1310,\n",
       "           0.2421, 0.4921, 0.8532, 0.9643, 0.9643, 0.9643, 0.4921, 0.2421, 0.1310,\n",
       "           0.1310, 0.8532, 0.1310, 0.8532, 0.9643, 0.1310, 0.9643, 0.8532, 0.8532,\n",
       "           0.8532, 0.4921, 0.9643, 0.9643, 0.4921, 0.8532, 0.9643, 0.1310, 0.4921,\n",
       "           0.4921, 0.4921, 0.9643, 0.2421, 0.9643, 0.8532, 0.8532, 0.2421, 0.8532,\n",
       "           0.8532, 0.1310, 0.4921, 0.9643, 0.9643, 0.1310, 0.1310, 0.9643, 0.1310,\n",
       "           0.4921, 0.4921, 0.1310, 0.9643, 0.2421, 0.1310, 0.4921, 0.4921, 0.9643,\n",
       "           0.1310, 0.8532, 0.2421, 0.9643, 0.2421, 0.8532, 0.1310, 0.9643, 0.1310,\n",
       "           0.1310, 0.4921, 0.9643, 0.9643, 0.4921, 0.9643, 0.8532, 0.1310, 0.4921,\n",
       "           0.8532, 0.4921, 0.2421, 0.8532, 0.9643, 0.2421, 0.8532, 0.4921, 0.9643,\n",
       "           0.8532, 0.1310, 0.9643, 0.8532, 0.9643, 0.8532, 0.8532, 0.1310, 0.1310,\n",
       "           0.1310, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087, 2.4087,\n",
       "           0.8532, 0.9643, 0.9643, 0.4921, 0.8532, 0.8532, 0.8532, 0.9643, 0.9643,\n",
       "           0.9643, 0.9643, 0.8532, 0.9643, 0.9643, 0.9643, 0.4921, 0.9643, 0.8532,\n",
       "           0.8532, 0.8532, 0.9643, 0.4921, 0.8532, 0.9643, 0.8532, 0.9643, 0.8532,\n",
       "           0.8532, 0.8532, 0.4921, 0.8532, 0.9643, 0.9643, 0.4921, 0.8532, 0.9643,\n",
       "           0.8532, 0.4921, 0.8532, 0.8532, 0.9643, 0.8532, 0.4921, 0.4921, 0.9643,\n",
       "           0.8532, 0.9643, 0.9643, 0.8532, 0.8532, 0.9643, 0.8532, 0.9643, 0.4921,\n",
       "           0.4921, 0.8532, 0.9643, 0.8532, 0.9643, 0.9643, 0.9643, 0.8532, 0.4921,\n",
       "           0.8532, 0.4921, 0.8532, 0.9643, 0.9643, 0.9643, 0.4921, 0.8532, 0.8532,\n",
       "           0.4921, 0.8532, 0.4921, 0.4921, 0.9643, 0.8532, 0.4921, 0.9643, 0.8532,\n",
       "           0.4921, 0.9643, 0.8532, 0.4921, 0.8532, 0.9643, 0.4921, 0.9643, 0.8532,\n",
       "           0.4921, 0.4921], requires_grad=True),\n",
       "   tensor([1.2579, 0.5357, 0.5357, 0.0357, 0.0357, 1.2579, 0.4246, 0.5357, 0.4246,\n",
       "           1.2579, 0.1746, 0.1746, 0.5357, 0.2857, 0.4246, 1.2579, 1.2579, 0.8968,\n",
       "           0.5357, 0.4246, 0.0357, 0.8968, 0.1746, 1.2579, 0.4246, 1.2579, 0.1746,\n",
       "           1.2579, 0.1746, 0.4246, 0.8968, 0.0357, 0.5357, 0.4246, 0.8968, 0.1746,\n",
       "           0.0357, 1.2579, 0.8968, 1.2579, 0.8968, 0.0357, 0.1746, 1.2579, 0.4246,\n",
       "           0.5357, 1.2579, 0.0357, 1.2579, 0.5357, 0.2857, 0.8968, 0.8968, 0.5357,\n",
       "           0.0357, 0.5357, 0.8968, 0.5357, 0.1746, 0.5357, 1.2579, 0.0357, 0.4246,\n",
       "           0.4246, 0.5357, 0.0357, 0.0357, 0.5357, 0.8968, 0.8968, 0.4246, 0.4246,\n",
       "           0.0357, 1.2579, 1.2579, 0.0357, 0.8968, 0.0357, 0.1746, 0.1746, 1.2579,\n",
       "           0.2857, 0.0357, 0.5357, 0.4246, 1.2579, 0.4246, 0.1746, 0.2857, 1.2579,\n",
       "           0.5357, 0.4246, 0.8968, 1.2579, 0.4246, 0.1746, 1.2579, 0.1746, 0.0357,\n",
       "           1.2579, 0.5357, 0.4246, 0.8968, 1.2579, 0.5357, 0.4246, 1.2579, 0.1746,\n",
       "           0.1746, 0.8968, 0.5357, 1.2579, 1.2579, 0.1746, 0.4246, 0.8968, 0.8968,\n",
       "           0.8968, 0.5357, 0.1746, 0.4246, 0.5357, 0.1746, 1.2579, 0.1746, 1.2579,\n",
       "           1.2579, 0.0357, 0.8968, 0.4246, 1.2579, 0.5357, 1.2579, 0.1746, 1.2579,\n",
       "           0.8968, 0.4246, 0.4246, 0.5357, 0.2857, 0.8968, 0.4246, 0.8968, 0.4246,\n",
       "           0.8968, 0.5357, 1.2579, 0.5357, 1.2579, 0.5357, 0.4246, 0.5357, 0.5357,\n",
       "           1.2579, 0.1746, 0.5357, 0.0357, 0.8968, 0.8968, 1.2579, 0.0357, 1.2579,\n",
       "           0.4246, 1.2579, 0.4246, 0.4246, 0.2857, 0.2857, 0.0357, 1.2579, 1.2579,\n",
       "           0.4246, 0.5357, 1.2579, 0.8968, 0.5357, 0.8968, 0.4246, 1.2579, 1.2579,\n",
       "           0.8968, 0.5357, 0.1746, 0.5357, 0.0357, 0.5357, 0.1746, 1.2579, 0.0357,\n",
       "           0.0357, 1.2579, 0.0357, 0.0357, 0.2857, 0.0357, 1.2579, 0.4246, 1.2579,\n",
       "           0.5357, 1.2579, 0.5357, 0.2857, 1.2579, 0.5357, 0.0357, 0.8968, 0.2857,\n",
       "           0.0357, 0.4246, 0.5357, 1.2579, 0.2857, 1.2579, 0.0357, 1.2579, 0.8968,\n",
       "           0.4246, 0.5357, 0.8968, 0.5357, 0.8968, 0.2857, 0.8968, 0.0357, 0.5357,\n",
       "           0.5357, 1.2579, 1.2579, 0.5357, 0.2857, 0.4246, 0.8968, 0.1746, 0.4246,\n",
       "           1.2579, 0.5357, 0.5357, 1.2579, 0.4246, 1.2579, 0.5357, 1.2579, 1.2579,\n",
       "           0.5357, 1.2579, 0.1151, 0.2857, 1.2579, 1.2579, 0.8968, 0.8968, 1.2579,\n",
       "           0.8968, 0.8968, 0.8968, 0.8968, 0.0357, 0.1746, 0.1746, 1.2579, 0.1746,\n",
       "           0.5357, 0.8968, 1.2579, 0.5357, 0.5357, 0.0357, 0.4246, 0.1746, 0.5357,\n",
       "           0.5357, 0.8968, 0.0357, 0.2857, 0.8968, 1.2579, 1.2579, 0.2857, 1.2579,\n",
       "           0.1746, 1.2579, 0.5357, 0.5357, 0.2857, 0.4246, 0.8968, 0.1746, 0.1746,\n",
       "           0.1746, 0.8968, 0.8968, 0.2857, 0.0357, 0.0357, 0.0040, 0.0595, 0.0595,\n",
       "           0.1151, 0.0595, 0.0357, 0.8968, 0.4246, 0.0040, 0.0833, 0.1151, 0.1746,\n",
       "           0.4246, 0.0040, 0.1746, 0.1746, 0.5357, 0.2857, 0.0595, 0.1746, 0.0357,\n",
       "           0.1151, 0.4246, 0.4246, 0.1746, 0.0595, 0.0595, 0.0357, 0.8968, 0.1746,\n",
       "           0.0357, 0.1746, 0.0357, 0.4246, 0.1746, 0.2857, 0.0357, 0.2857, 0.8968,\n",
       "           0.0357, 0.1746, 0.0833, 0.4246, 0.1746, 0.8968, 0.0595, 0.1151, 0.4246,\n",
       "           0.2857, 0.0595, 0.0357, 0.1746, 0.2857, 0.0357, 0.0040, 0.0357, 0.0357,\n",
       "           0.0357, 0.1151, 0.2857, 0.2857, 0.1746, 0.1746, 0.0040, 0.1746, 0.1151,\n",
       "           0.1746, 0.1746, 0.0357, 0.0357, 0.0833, 0.1746, 0.0357, 0.0357, 0.0595,\n",
       "           0.0595, 0.0040, 0.0040, 0.0357, 0.0833, 0.1746, 0.1151, 0.4246, 0.0040,\n",
       "           0.0595, 0.0833, 0.0357, 0.0595, 0.2857, 0.0040, 0.0833, 0.0595, 0.2857,\n",
       "           0.0833, 0.0040, 0.8968, 0.0357, 0.4246, 0.2857, 0.5357, 0.0040, 0.2857,\n",
       "           0.1151, 0.1746, 0.0357, 0.4246, 0.4246, 0.0595, 0.2857, 0.0595, 0.0595,\n",
       "           0.0357, 0.1746, 0.0357, 0.0040, 0.0595, 0.1151, 0.1151, 0.1746, 0.0833,\n",
       "           0.4246, 0.5357, 0.1151, 0.0357, 0.4246, 0.1746, 0.2857, 0.5357, 0.4246,\n",
       "           0.0357, 0.2857, 0.0595, 0.5357, 0.2857, 0.5357, 0.1746, 0.1746, 0.0833,\n",
       "           0.5357, 0.5357, 0.0357, 0.8968, 0.4246, 0.1746, 0.0040, 0.0040, 0.4246,\n",
       "           0.0833, 0.0040, 0.1746, 0.5357, 0.0357, 0.2857, 0.2857, 0.1746, 0.0595,\n",
       "           0.1746, 0.4246, 0.1151, 0.5357, 0.2857, 0.2857, 0.2857, 0.0595, 0.0357,\n",
       "           0.2857, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024,\n",
       "           2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024,\n",
       "           2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024,\n",
       "           2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024, 2.7024],\n",
       "          requires_grad=True)],\n",
       "  'Implied Volatility': [tensor([0.2695, 0.2695, 0.3415, 0.3091, 0.3302, 0.2849, 0.2695, 0.3302, 0.2561,\n",
       "           0.2912, 0.3295, 0.3461, 0.2658, 0.3302, 0.3133, 0.3065, 0.3302, 0.2950,\n",
       "           0.3461, 0.2847, 0.3088, 0.2847, 0.2711, 0.2940, 0.2561, 0.2887, 0.2561,\n",
       "           0.2683, 0.2662, 0.3295, 0.2817, 0.3133, 0.3271, 0.2548, 0.3302, 0.3295,\n",
       "           0.2849, 0.3302, 0.3302, 0.2695, 0.3302, 0.2702, 0.3302, 0.3716, 0.2707,\n",
       "           0.3295, 0.3302, 0.2604, 0.3133, 0.3104, 0.2695, 0.3302, 0.2631, 0.2702,\n",
       "           0.2654, 0.3295, 0.2561, 0.3138, 0.2702, 0.3153, 0.3128, 0.2695, 0.2561,\n",
       "           0.2665, 0.3024, 0.2849, 0.3461, 0.2849, 0.2661, 0.3066, 0.2564, 0.2844,\n",
       "           0.2561, 0.2702, 0.3295, 0.2745, 0.2695, 0.3000, 0.2636, 0.3302, 0.2585,\n",
       "           0.3356, 0.2849, 0.3216, 0.2785, 0.3190, 0.3302, 0.2695, 0.3302, 0.2676,\n",
       "           0.2737, 0.2695, 0.3356, 0.2661, 0.3295, 0.3302, 0.2601, 0.2695, 0.2717,\n",
       "           0.2695, 0.2859, 0.3133, 0.2695, 0.2561, 0.3295, 0.3302, 0.2695, 0.2695,\n",
       "           0.3302, 0.3133, 0.3047, 0.2561, 0.2561, 0.2695, 0.3356, 0.3413, 0.2695,\n",
       "           0.3133, 0.2561, 0.3295, 0.2616, 0.3295, 0.2888, 0.2695, 0.3302, 0.3104,\n",
       "           0.3096, 0.2740, 0.2607, 0.3133, 0.2666, 0.2775, 0.2929, 0.3295, 0.3461,\n",
       "           0.2561, 0.2710, 0.2661, 0.2561, 0.3356, 0.2702, 0.2581, 0.3302, 0.2702,\n",
       "           0.3302, 0.3302, 0.3215, 0.2669, 0.2877, 0.2759, 0.2661, 0.2854, 0.2985,\n",
       "           0.3295, 0.2631, 0.2661, 0.2770, 0.2561, 0.2609, 0.3302, 0.2628, 0.2695,\n",
       "           0.3461, 0.2716, 0.2871, 0.2804, 0.3116, 0.3302, 0.2894, 0.4638, 0.3104,\n",
       "           0.3651, 0.3439, 0.3153, 0.3111, 0.3495, 0.2803, 0.2844, 0.3451, 0.3081,\n",
       "           0.3100, 0.3491, 0.2940, 0.3186, 0.2867, 0.2829, 0.3227, 0.4616, 0.3150,\n",
       "           0.4355, 0.4535, 0.3170, 0.3788, 0.3126, 0.4782, 0.3439, 0.3120, 0.2870,\n",
       "           0.2817, 0.3202, 0.2878, 0.3081, 0.3473, 0.2819, 0.2757, 0.3187, 0.2901,\n",
       "           0.4840, 0.2996, 0.3473, 0.3051, 0.3044, 0.4344, 0.3507, 0.4801, 0.3369,\n",
       "           0.3235, 0.2824, 0.3695, 0.3381, 0.3175, 0.3695, 0.3459, 0.3335, 0.3648,\n",
       "           0.2771, 0.3258, 0.3508, 0.3295, 0.3267, 0.2716, 0.2983, 0.3472, 0.3258,\n",
       "           0.3258, 0.3338, 0.3280, 0.2834, 0.2778, 0.3716, 0.3472, 0.3235, 0.3472,\n",
       "           0.3459, 0.2899, 0.3459, 0.3473, 0.3347, 0.3473, 0.2607, 0.3822, 0.3535,\n",
       "           0.3258, 0.3258, 0.3469, 0.3472, 0.3114, 0.3147, 0.3695, 0.3695, 0.2622,\n",
       "           0.3473, 0.3632, 0.3422, 0.3472, 0.2690, 0.2665, 0.3398, 0.3472, 0.6848,\n",
       "           0.3001, 0.3320, 0.2909, 0.3227, 0.2803, 0.3700, 0.4059, 0.4187, 0.3459,\n",
       "           0.3469, 0.3103, 0.3459, 0.2912, 0.3258, 0.3273, 0.3258, 0.4187, 0.3473,\n",
       "           0.2778, 0.3379, 0.3473, 0.3473, 0.2693, 0.6979, 0.3822, 0.3242, 0.3397,\n",
       "           0.3146, 0.3507, 0.2703, 0.3390, 0.2887, 0.3510, 0.4187, 0.2792, 0.5274,\n",
       "           0.2886, 0.2902, 0.2826, 0.3419, 0.2725, 0.2686, 0.2735, 0.3472, 0.3473,\n",
       "           0.3507, 0.3381, 0.3822, 0.2809, 0.2643, 0.3695, 0.3716, 0.2985, 0.3510,\n",
       "           0.2723, 0.3187, 0.3822, 0.2952, 0.2957, 0.3109, 0.2810, 0.5483, 0.4187,\n",
       "           0.3369, 0.3277, 0.2782, 0.3401, 0.2721, 0.3472, 0.3002, 0.3695, 0.3209,\n",
       "           0.3459, 0.2989, 0.3473, 0.5483, 0.3469, 0.3216, 0.3192, 0.3167, 0.3136,\n",
       "           0.3194, 0.3264, 0.3043, 0.3047, 0.3011, 0.3043, 0.2967, 0.3194, 0.2968,\n",
       "           0.2973, 0.3001, 0.3310, 0.3061, 0.2967, 0.3011, 0.3056, 0.3047, 0.3182,\n",
       "           0.3194, 0.3037, 0.2964, 0.3076, 0.3194, 0.2982, 0.3043, 0.2974, 0.3292,\n",
       "           0.3020, 0.3053, 0.3042, 0.3089, 0.2767, 0.3089, 0.2854, 0.2671, 0.2943,\n",
       "           0.2808, 0.2738, 0.3089, 0.2829, 0.2853, 0.2823, 0.2788, 0.3124, 0.2811,\n",
       "           0.2849, 0.2871, 0.3116, 0.3116, 0.3089, 0.2682, 0.2819, 0.3089, 0.3089,\n",
       "           0.3089, 0.3116, 0.2819, 0.3089, 0.3116, 0.2767, 0.3116, 0.2879, 0.2849,\n",
       "           0.3089, 0.2858, 0.2794, 0.2777, 0.2785, 0.2685, 0.2765, 0.2760, 0.3124,\n",
       "           0.3089, 0.3116, 0.3089, 0.2829, 0.2849, 0.3116, 0.3116, 0.2792, 0.3116,\n",
       "           0.3096, 0.3069, 0.3124, 0.3089, 0.2824, 0.2717, 0.2672, 0.2872, 0.2779,\n",
       "           0.3000, 0.2763, 0.3089, 0.3124, 0.3089, 0.2814, 0.3229, 0.3089, 0.3116,\n",
       "           0.2791, 0.2818, 0.3116, 0.2978, 0.2849, 0.2849, 0.2999, 0.2849, 0.2849,\n",
       "           0.3089]),\n",
       "   tensor([0.2906, 0.3365, 0.5096, 0.2726, 0.2813, 0.2881, 0.4041, 0.2970, 0.3017,\n",
       "           0.3056, 0.3919, 0.3156, 0.2899, 0.3284, 0.4003, 0.2914, 0.3032, 0.3120,\n",
       "           0.2953, 0.2862, 0.4731, 0.2803, 0.4101, 0.3146, 0.3002, 0.2974, 0.2711,\n",
       "           0.3724, 0.2737, 0.3042, 0.4484, 0.3578, 0.3025, 0.2807, 0.3123, 0.4041,\n",
       "           0.3174, 0.3151, 0.4731, 0.3529, 0.2829, 0.2985, 0.3123, 0.2841, 0.3578,\n",
       "           0.3676, 0.2858, 0.2881, 0.2924, 0.5174, 0.2851, 0.2802, 0.4101, 0.3086,\n",
       "           0.3042, 0.4101, 0.2949, 0.2717, 0.3578, 0.3110, 0.2854, 0.3037, 0.3211,\n",
       "           0.3597, 0.2819, 0.2819, 0.4041, 0.2829, 0.5174, 0.2948, 0.4731, 0.2908,\n",
       "           0.3375, 0.2759, 0.4201, 0.4041, 0.6361, 0.2893, 0.3017, 0.3151, 0.3017,\n",
       "           0.3124, 0.3546, 0.3681, 0.2894, 0.3011, 0.4041, 0.2805, 0.3755, 0.2891,\n",
       "           0.2829, 0.2718, 0.3510, 0.2962, 0.3968, 0.3091, 0.2810, 0.2859, 0.4136,\n",
       "           0.3163, 0.3556, 0.4101, 0.2838, 0.4041, 0.2839, 0.2957, 0.3016, 0.2990,\n",
       "           0.2940, 0.2829, 0.3308, 0.2726, 0.2756, 0.3271, 0.3696, 0.2801, 0.6375,\n",
       "           0.3452, 0.2908, 0.3983, 0.3010, 0.3096, 0.4799, 0.3030, 0.3020, 0.3011,\n",
       "           0.2828, 0.3276, 0.3968, 0.4731, 0.3151, 0.4041, 0.4731, 0.4456, 0.4101,\n",
       "           0.2811, 0.2869, 0.3084, 0.3578, 0.3271, 0.2935, 0.3068, 0.3597, 0.4101,\n",
       "           0.4562, 0.2751, 0.4799, 0.2696, 0.4300, 0.2833, 0.2944, 0.4101, 0.4101,\n",
       "           0.3013, 0.3262, 0.3597, 0.2774, 0.3007, 0.3017, 0.3151, 0.2728, 0.2706,\n",
       "           0.2875, 0.3255, 0.3013, 0.3018, 0.3597, 0.2893, 0.2893, 0.3695, 0.4101,\n",
       "           0.2791, 0.3151, 0.4484, 0.3801, 0.4731, 0.2950, 0.2950, 0.2879, 0.2686,\n",
       "           0.2922, 0.2675, 0.2950, 0.2749, 0.2950, 0.2827, 0.2675, 0.2659, 0.2781,\n",
       "           0.2950, 0.2811, 0.2675, 0.2753, 0.2768, 0.2864, 0.2832, 0.2675, 0.2768,\n",
       "           0.2675, 0.2718, 0.2800, 0.2751, 0.2675, 0.2768, 0.2768, 0.2950, 0.2675,\n",
       "           0.2703, 0.2664, 0.2675, 0.2950, 0.2938, 0.2938, 0.2668, 0.2675, 0.2675,\n",
       "           0.2675, 0.2675, 0.2775, 0.2929, 0.2671, 0.2646, 0.2672, 0.2675, 0.2950,\n",
       "           0.2675, 0.2768, 0.2768, 0.2768, 0.2636, 0.2903, 0.2675, 0.2788, 0.2781,\n",
       "           0.2671, 0.2675, 0.2680, 0.2664, 0.2693, 0.2675, 0.2708, 0.2675, 0.2950,\n",
       "           0.2804, 0.2651, 0.2708, 0.2950, 0.2660, 0.2781, 0.2938, 0.2950, 0.2768,\n",
       "           0.2675, 0.2740, 0.2700, 0.2938, 0.2938, 0.3270, 0.3017, 0.5174, 0.3578,\n",
       "           0.4041, 0.5174, 0.3017, 0.3151, 0.3017, 0.3016, 0.3784, 0.5174, 0.5174,\n",
       "           0.4041, 0.3578, 0.3048, 0.3207, 0.3578, 0.4041, 0.4041, 0.3578, 0.3578,\n",
       "           0.4041, 0.3578, 0.3270, 0.3017, 0.4041, 0.3315, 0.3619, 0.3784, 0.5174,\n",
       "           0.3048, 0.3578, 0.3678, 0.3284, 0.3048, 0.4041, 0.3784, 0.3016, 0.3784,\n",
       "           0.3151, 0.3378, 0.3270, 0.3017, 0.3151, 0.3689, 0.4041, 0.3048, 0.3784,\n",
       "           0.3784, 0.5174, 0.3017, 0.3151, 0.3784, 0.3048, 0.3701, 0.3016, 0.2963,\n",
       "           0.3784, 0.3284, 0.3701, 0.5174, 0.3048, 0.4041, 0.3048, 0.3784, 0.5174,\n",
       "           0.3048, 0.3016, 0.3017, 0.3016, 0.3270, 0.3016, 0.3151, 0.5174, 0.3284,\n",
       "           0.3578, 0.3016, 0.3151, 0.3016, 0.4041, 0.3151, 0.3270, 0.3701, 0.3701,\n",
       "           0.3411, 0.5174, 0.3784, 0.3678, 0.3284, 0.3701, 0.3434, 0.3277, 0.3578,\n",
       "           0.3270, 0.3017, 0.3284, 0.3662, 0.3016, 0.3578, 0.3017, 0.3784, 0.3181,\n",
       "           0.5174, 0.5174, 0.3336, 0.3270, 0.3703, 0.4041, 0.3784, 0.3048, 0.3270,\n",
       "           0.4101, 0.3151, 0.3048, 0.4041, 0.3016, 0.3017, 0.3048, 0.3578, 0.3016,\n",
       "           0.3048, 0.3543, 0.3016, 0.3093, 0.3270, 0.5174, 0.3163, 0.3151, 0.3701,\n",
       "           0.3669, 0.3701, 0.3784, 0.3151, 0.3701, 0.3270, 0.3578, 0.3151, 0.3284,\n",
       "           0.3151, 0.3413, 0.5174, 0.3205, 0.3270, 0.4041, 0.3270, 0.3048, 0.3048,\n",
       "           0.3270, 0.3017, 0.3016, 0.3016, 0.3016, 0.3270, 0.3578, 0.3784, 0.3701,\n",
       "           0.3511, 0.4041, 0.3151, 0.3046, 0.2916, 0.3240, 0.3065, 0.3002, 0.3105,\n",
       "           0.3118, 0.3075, 0.3084, 0.2999, 0.2997, 0.2924, 0.3028, 0.2899, 0.3026,\n",
       "           0.3173, 0.3020, 0.3065, 0.3036, 0.3047, 0.2881, 0.2925, 0.3054, 0.3008,\n",
       "           0.3165, 0.2992, 0.3081, 0.2915, 0.2908, 0.3932, 0.3090, 0.2916, 0.2999,\n",
       "           0.2935, 0.7247, 0.3792, 0.4230, 0.3071, 0.3801, 0.3962, 0.3439, 0.3909,\n",
       "           0.3370, 0.3464, 0.2896, 0.3962, 0.3792, 0.2993, 0.3374, 0.4610, 0.3030,\n",
       "           0.2798, 0.3346, 0.4089, 0.3296, 0.2859, 0.4352, 0.3372, 0.2892, 0.2870,\n",
       "           0.2988, 0.3484, 0.3353, 0.3237, 0.3962, 0.3481, 0.3264, 0.2927, 0.3484,\n",
       "           0.4797, 0.3532, 0.3400, 0.3274, 0.3358, 0.3484, 0.3030, 0.3370, 0.3801,\n",
       "           0.3088, 0.3068, 0.2844, 0.3413, 0.3221, 0.3370, 0.3792, 0.2834, 0.2795,\n",
       "           0.3632, 0.3194, 0.2912, 0.3690, 0.4610, 0.3792, 0.3962, 0.2711, 0.3372,\n",
       "           0.3226, 0.3030, 0.3372, 0.3372, 0.3161, 0.3260, 0.3081, 0.3005, 0.2949,\n",
       "           0.7247, 0.4089, 0.3063, 0.2854, 0.2914, 0.3025, 0.2935, 0.4214, 0.2888,\n",
       "           0.7247, 0.3232, 0.3071, 0.3962, 0.2934, 0.3024, 0.3044, 0.3534, 0.3227,\n",
       "           0.2948, 0.2795, 0.3962, 0.3481, 0.7247, 0.3801, 0.2745, 0.2870, 0.3355,\n",
       "           0.3632, 0.3162, 0.3632, 0.3043]),\n",
       "   tensor([0.2497, 0.2415, 0.3682, 0.2836, 0.2791, 0.2361, 0.3716, 0.3175, 0.3010,\n",
       "           0.2096, 0.2970, 0.2365, 0.2639, 0.2309, 0.2737, 0.2746, 0.2380, 0.2812,\n",
       "           0.2721, 0.2829, 0.3682, 0.2055, 0.3347, 0.2970, 0.2167, 0.2839, 0.2673,\n",
       "           0.2757, 0.2891, 0.2076, 0.3682, 0.2042, 0.2970, 0.2364, 0.3224, 0.3352,\n",
       "           0.3506, 0.2034, 0.3682, 0.2625, 0.2055, 0.2970, 0.2046, 0.2519, 0.2141,\n",
       "           0.3347, 0.2343, 0.3246, 0.2970, 0.3190, 0.3246, 0.2519, 0.3347, 0.2921,\n",
       "           0.2207, 0.3347, 0.2055, 0.2545, 0.2164, 0.2970, 0.2335, 0.2929, 0.3170,\n",
       "           0.2582, 0.3175, 0.2611, 0.3352, 0.2519, 0.3190, 0.2891, 0.3682, 0.2569,\n",
       "           0.3506, 0.2359, 0.2098, 0.2248, 0.2507, 0.3306, 0.2564, 0.1938, 0.2974,\n",
       "           0.3506, 0.2708, 0.3347, 0.2268, 0.2726, 0.4354, 0.2359, 0.2970, 0.2191,\n",
       "           0.2314, 0.2767, 0.2455, 0.2285, 0.3506, 0.2363, 0.2837, 0.3175, 0.2169,\n",
       "           0.2369, 0.2640, 0.3347, 0.2370, 0.2151, 0.2457, 0.2417, 0.2887, 0.2191,\n",
       "           0.2970, 0.2055, 0.2012, 0.2506, 0.3246, 0.2977, 0.3347, 0.2469, 0.3682,\n",
       "           0.2896, 0.2631, 0.3682, 0.2055, 0.2970, 0.2507, 0.2769, 0.3066, 0.2538,\n",
       "           0.2526, 0.3090, 0.3506, 0.2965, 0.2030, 0.3352, 0.3682, 0.2166, 0.3347,\n",
       "           0.2055, 0.2293, 0.2176, 0.2039, 0.2346, 0.2443, 0.2908, 0.2201, 0.3347,\n",
       "           0.2507, 0.2917, 0.2507, 0.2656, 0.2061, 0.2348, 0.2121, 0.3347, 0.3347,\n",
       "           0.2099, 0.3091, 0.2373, 0.2969, 0.2970, 0.3079, 0.1956, 0.2478, 0.2642,\n",
       "           0.3306, 0.3306, 0.2859, 0.2535, 0.2090, 0.3306, 0.3306, 0.2080, 0.3347,\n",
       "           0.2390, 0.1946, 0.3682, 0.3506, 0.3682, 0.2698, 0.3032, 0.2884, 0.2801,\n",
       "           0.3305, 0.2929, 0.2670, 0.2960, 0.2907, 0.2720, 0.2879, 0.2707, 0.2789,\n",
       "           0.2910, 0.3190, 0.2626, 0.2814, 0.2919, 0.2952, 0.2949, 0.2642, 0.3009,\n",
       "           0.2827, 0.5224, 0.4978, 0.3306, 0.2919, 0.2642, 0.2970, 0.2970, 0.3306,\n",
       "           0.5668, 0.2919, 0.3009, 0.2642, 0.2949, 0.2949, 0.2952, 0.4176, 0.2970,\n",
       "           0.3306, 0.3009, 0.2970, 0.3009, 0.2642, 0.3306, 0.2949, 0.3009, 0.2944,\n",
       "           0.2944, 0.2952, 0.2642, 0.2949, 0.2919, 0.2944, 0.2642, 0.2970, 0.2919,\n",
       "           0.2919, 0.2919, 0.2949, 0.4812, 0.2642, 0.2944, 0.2944, 0.3246, 0.2932,\n",
       "           0.3009, 0.2970, 0.2919, 0.2949, 0.2642, 0.2970, 0.2970, 0.2642, 0.2970,\n",
       "           0.2952, 0.2919, 0.2970, 0.2949, 0.5395, 0.2970, 0.2952, 0.2919, 0.2642,\n",
       "           0.3306, 0.2944, 0.4376, 0.2642, 0.3246, 0.2881, 0.3306, 0.2642, 0.2970,\n",
       "           0.3306, 0.2952, 0.2858, 0.2949, 0.2919, 0.2949, 0.2944, 0.2970, 0.2952,\n",
       "           0.3009, 0.2952, 0.3246, 0.2944, 0.2949, 0.3246, 0.3009, 0.2952, 0.2949,\n",
       "           0.2944, 0.3306, 0.2642, 0.2877, 0.2949, 0.3009, 0.2944, 0.2970, 0.2970,\n",
       "           0.2970, 0.2528, 0.2449, 0.2582, 0.2317, 0.2532, 0.2433, 0.2319, 0.2529,\n",
       "           0.2308, 0.2347, 0.2322, 0.2351, 0.2350, 0.2460, 0.2372, 0.2441, 0.2352,\n",
       "           0.2316, 0.2324, 0.2302, 0.2334, 0.2316, 0.2359, 0.2337, 0.2310, 0.2343,\n",
       "           0.2686, 0.2222, 0.2513, 0.2195, 0.2228, 0.2217, 0.2813, 0.2181, 0.2524,\n",
       "           0.2297, 0.2451, 0.2631, 0.2256, 0.2597, 0.2192, 0.2250, 0.2642, 0.2570,\n",
       "           0.2184, 0.2489, 0.2198, 0.2217, 0.2205, 0.2190, 0.2780, 0.2151, 0.2229,\n",
       "           0.2575, 0.2248, 0.2241, 0.2541, 0.2335, 0.2164, 0.2190, 0.2235, 0.2172,\n",
       "           0.2597, 0.2196, 0.2196, 0.2217, 0.2274, 0.2184, 0.2123, 0.2317, 0.2214,\n",
       "           0.2239, 0.2642, 0.2220, 0.2236, 0.2596, 0.2203, 0.2631, 0.2607, 0.2294,\n",
       "           0.2143, 0.2268, 0.2214, 0.2199, 0.2206, 0.2646, 0.2306, 0.2228, 0.2171,\n",
       "           0.2215, 0.2299, 0.2441, 0.2352, 0.2217, 0.2553, 0.2212, 0.2363, 0.2626,\n",
       "           0.2191, 0.2252, 0.2156, 0.2262, 0.2146, 0.2230, 0.2186, 0.2220, 0.2426,\n",
       "           0.2137, 0.2213, 0.2346, 0.2137, 0.2405, 0.2731, 0.2231, 0.2238, 0.2262,\n",
       "           0.2139, 0.2221]),\n",
       "   tensor([0.3007, 0.2970, 0.2970, 0.5094, 0.5094, 0.3007, 0.3291, 0.3249, 0.3291,\n",
       "           0.3091, 0.3091, 0.3091, 0.2970, 0.3641, 0.3321, 0.3007, 0.3007, 0.2970,\n",
       "           0.3249, 0.3291, 0.5094, 0.2969, 0.3091, 0.3007, 0.3321, 0.3007, 0.3776,\n",
       "           0.3091, 0.3091, 0.3291, 0.2970, 0.3822, 0.2970, 0.3321, 0.2970, 0.3776,\n",
       "           0.5094, 0.3007, 0.2970, 0.3007, 0.2970, 0.3822, 0.3091, 0.3091, 0.3291,\n",
       "           0.3249, 0.2954, 0.5094, 0.3091, 0.3249, 0.3361, 0.2970, 0.2980, 0.2970,\n",
       "           0.5094, 0.2970, 0.2899, 0.3249, 0.3091, 0.3051, 0.3091, 0.3822, 0.3321,\n",
       "           0.3321, 0.3249, 0.5094, 0.3822, 0.3249, 0.2980, 0.2938, 0.3321, 0.3291,\n",
       "           0.3822, 0.2848, 0.3007, 0.5094, 0.2980, 0.3822, 0.3776, 0.3776, 0.2879,\n",
       "           0.3641, 0.5094, 0.2970, 0.3321, 0.3007, 0.3321, 0.3776, 0.3361, 0.3091,\n",
       "           0.3249, 0.3291, 0.2980, 0.3007, 0.3291, 0.3776, 0.2901, 0.3091, 0.3822,\n",
       "           0.3007, 0.3249, 0.3291, 0.2980, 0.3007, 0.2970, 0.3291, 0.3007, 0.3776,\n",
       "           0.3091, 0.2871, 0.2970, 0.2962, 0.2849, 0.3776, 0.3291, 0.2970, 0.2970,\n",
       "           0.2940, 0.3249, 0.3776, 0.3291, 0.2970, 0.3776, 0.3007, 0.3091, 0.3012,\n",
       "           0.3007, 0.3822, 0.2980, 0.3291, 0.3007, 0.3042, 0.2889, 0.3776, 0.3011,\n",
       "           0.2980, 0.3291, 0.3321, 0.2970, 0.3641, 0.2970, 0.3291, 0.2870, 0.3291,\n",
       "           0.2980, 0.2970, 0.2949, 0.2970, 0.3091, 0.3249, 0.3321, 0.2970, 0.3249,\n",
       "           0.3007, 0.3091, 0.3051, 0.3822, 0.2854, 0.2985, 0.3007, 0.3822, 0.2864,\n",
       "           0.3321, 0.3091, 0.3291, 0.3258, 0.3641, 0.3361, 0.5094, 0.3091, 0.3091,\n",
       "           0.3321, 0.3249, 0.2942, 0.3026, 0.3249, 0.2851, 0.3321, 0.3091, 0.2963,\n",
       "           0.2980, 0.2970, 0.3091, 0.3249, 0.5094, 0.2970, 0.3091, 0.3091, 0.3822,\n",
       "           0.4317, 0.3341, 0.4317, 0.3747, 0.3575, 0.3747, 0.4519, 0.3286, 0.3037,\n",
       "           0.5115, 0.4002, 0.3170, 0.4225, 0.3138, 0.5548, 0.3747, 0.5134, 0.4266,\n",
       "           0.4317, 0.4168, 0.4010, 0.3711, 0.4914, 0.4115, 0.3747, 0.3446, 0.5253,\n",
       "           0.3249, 0.3701, 0.3473, 0.3158, 0.3105, 0.4225, 0.2945, 0.4317, 0.3849,\n",
       "           0.4932, 0.3867, 0.3215, 0.3468, 0.6381, 0.4035, 0.3741, 0.4087, 0.4786,\n",
       "           0.2890, 0.4229, 0.3252, 0.3003, 0.4786, 0.2891, 0.5854, 0.3420, 0.3071,\n",
       "           0.4184, 0.4321, 0.3560, 0.3983, 0.2990, 0.3384, 0.3648, 0.3555, 0.3694,\n",
       "           0.3456, 0.4482, 0.3242, 0.3059, 0.3747, 0.4029, 0.4087, 0.3045, 0.3997,\n",
       "           0.5652, 0.2979, 0.3568, 0.3278, 0.3138, 0.4317, 0.3918, 0.4087, 0.4406,\n",
       "           0.3181, 0.3741, 0.4317, 0.4006, 0.4198, 0.2909, 0.3017, 0.4091, 0.3118,\n",
       "           0.4029, 0.3204, 0.6018, 0.4010, 0.4078, 0.4786, 0.3160, 0.4087, 0.4029,\n",
       "           0.3461, 0.3284, 0.3821, 0.5198, 0.3822, 0.3111, 0.9991, 0.3436, 0.3146,\n",
       "           0.3140, 0.4226, 0.3822, 0.2875, 0.3249, 0.7914, 0.3119, 0.3075, 0.3236,\n",
       "           0.3088, 0.5199, 0.3118, 0.3023, 0.2957, 0.3361, 0.5044, 0.2980, 0.3279,\n",
       "           0.3131, 0.3240, 0.3031, 0.3776, 0.3436, 0.3186, 0.5094, 0.2957, 0.3119,\n",
       "           0.3822, 0.3091, 0.5094, 0.3076, 0.3399, 0.3377, 0.4481, 0.3282, 0.2866,\n",
       "           0.3553, 0.3269, 0.3496, 0.3116, 0.3030, 0.2881, 0.3140, 0.3502, 0.3291,\n",
       "           0.3269, 0.3313, 0.5094, 0.3390, 0.3262, 0.3822, 0.9991, 0.3822, 0.5094,\n",
       "           0.3666, 0.2979, 0.3428, 0.3401, 0.3015, 0.3091, 0.9991, 0.2964, 0.3517,\n",
       "           0.3552, 0.3003, 0.5094, 0.3748, 0.3331, 0.3776, 0.3822, 0.3822, 0.4396,\n",
       "           0.4931, 0.9991, 0.9991, 0.3822, 0.3157, 0.3091, 0.3002, 0.3147, 0.9991,\n",
       "           0.3158, 0.4034, 0.2995, 0.3338, 0.3259, 0.9991, 0.3139, 0.3436, 0.3336,\n",
       "           0.3061, 0.5199, 0.2968, 0.3822, 0.3080, 0.3278, 0.3178, 0.5199, 0.3259,\n",
       "           0.3036, 0.3049, 0.3522, 0.3053, 0.3149, 0.4395, 0.3361, 0.3194, 0.3436,\n",
       "           0.3747, 0.3075, 0.3045, 0.5638, 0.3499, 0.3075, 0.3070, 0.3091, 0.3123,\n",
       "           0.3091, 0.3013, 0.3730, 0.4619, 0.3095, 0.3091, 0.3256, 0.3054, 0.3291,\n",
       "           0.4169, 0.3348, 0.3224, 0.2935, 0.3348, 0.2944, 0.3156, 0.3091, 0.3255,\n",
       "           0.2950, 0.2945, 0.3178, 0.2934, 0.3291, 0.2983, 0.9991, 0.9991, 0.3199,\n",
       "           0.3331, 1.1425, 0.3119, 0.3085, 0.3245, 0.3309, 0.3361, 0.3152, 0.3222,\n",
       "           0.3091, 0.3211, 0.2955, 0.3004, 0.3361, 0.3395, 0.3641, 0.4050, 0.3023,\n",
       "           0.3555, 0.3147, 0.3147, 0.3147, 0.2980, 0.3005, 0.3147, 0.2958, 0.2999,\n",
       "           0.2983, 0.3002, 0.3133, 0.2958, 0.3134, 0.3147, 0.3005, 0.3128, 0.2993,\n",
       "           0.3147, 0.3008, 0.3147, 0.3147, 0.2965, 0.3124, 0.3192, 0.3180, 0.3212,\n",
       "           0.3404, 0.3148, 0.3150, 0.3170, 0.3277, 0.3301, 0.3177, 0.3253, 0.3564])]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "class IVSurfaceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data, \n",
    "        proportion, \n",
    "        random_state=0\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.proportion = proportion\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        surface_data = self.data[idx]\n",
    "        \n",
    "        # Extract the surface coordinates and volatilities\n",
    "        points_coordinates = np.stack([\n",
    "            surface_data['Surface']['Log Moneyness'], \n",
    "            surface_data['Surface']['Time to Maturity']\n",
    "        ], axis=1)\n",
    "        points_volatilities = surface_data['Surface']['Implied Volatility']\n",
    "\n",
    "        # Perform clustering\n",
    "        n_clusters = int(np.ceil(1 / self.proportion))\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('kmeans', KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init='auto'))\n",
    "        ])\n",
    "        labels = pipeline.fit_predict(points_coordinates)\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "\n",
    "        masked_indices = np.array([], dtype=int)\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_indices = np.where(labels == cluster)[0]\n",
    "            num_to_mask = int(np.ceil(len(cluster_indices) * proportion))\n",
    "            masked_indices = np.append(masked_indices, [rng.choice(cluster_indices, size=num_to_mask, replace=False)])\n",
    "        \n",
    "        unmasked_indices = np.setdiff1d(range(len(labels)), masked_indices)\n",
    "\n",
    "        data_item = {\n",
    "            'Datetime': surface_data['Datetime'],\n",
    "            'Symbol': surface_data['Symbol'],\n",
    "            'Market Features': {\n",
    "                'Market Return': torch.tensor(surface_data['Market Features']['Market Return'], dtype=torch.float32),\n",
    "                'Market Volatility': torch.tensor(surface_data['Market Features']['Market Volatility'], dtype=torch.float32),\n",
    "                'Treasury Rate': torch.tensor(surface_data['Market Features']['Treasury Rate'], dtype=torch.float32),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[unmasked_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[unmasked_indices, 1], dtype=torch.float32),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[unmasked_indices], dtype=torch.float32)\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': torch.tensor(points_coordinates[masked_indices, 0], dtype=torch.float32),\n",
    "                'Time to Maturity': torch.tensor(points_coordinates[masked_indices, 1], dtype=torch.float32),\n",
    "                'Implied Volatility': torch.tensor(points_volatilities[masked_indices], dtype=torch.float32)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return data_item\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batched_data = {\n",
    "            'Datetime': [item['Datetime'] for item in batch],\n",
    "            'Symbol': [item['Symbol'] for item in batch],\n",
    "            'Market Features': {\n",
    "                'Market Return': default_collate([item['Market Features']['Market Return'] for item in batch]),\n",
    "                'Market Volatility': default_collate([item['Market Features']['Market Volatility'] for item in batch]),\n",
    "                'Treasury Rate': default_collate([item['Market Features']['Treasury Rate'] for item in batch]),\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': [item['Input Surface']['Log Moneyness'].clone().detach() for item in batch],\n",
    "                'Time to Maturity': [item['Input Surface']['Time to Maturity'].clone().detach() for item in batch],\n",
    "                'Implied Volatility': [item['Input Surface']['Implied Volatility'].clone().detach() for item in batch],\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': [item['Query Points']['Log Moneyness'].clone().detach().requires_grad_(True) for item in batch],\n",
    "                'Time to Maturity': [item['Query Points']['Time to Maturity'].clone().detach().requires_grad_(True) for item in batch],\n",
    "                'Implied Volatility': [item['Query Points']['Implied Volatility'].clone().detach() for item in batch],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "# Assuming surfaces is the output from the implied_volatility_surfaces function\n",
    "proportion = 0.2  # example proportion\n",
    "dataset = IVSurfaceDataset(surfaces, proportion)\n",
    "data_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=HYPERPARAMETERS['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Fetch one batch from the DataLoader\n",
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Datetime': [Timestamp('2013-06-10 00:00:00'),\n",
       "  Timestamp('2013-01-28 00:00:00'),\n",
       "  Timestamp('2013-05-20 00:00:00'),\n",
       "  Timestamp('2013-03-07 00:00:00')],\n",
       " 'Symbol': ['AAPL', 'AAPL', 'GOOGL', 'AAPL'],\n",
       " 'Market Features': {'Market Return': tensor([-0.0216, -0.4603, -0.1269,  0.6087], grad_fn=<SqueezeBackward1>),\n",
       "  'Market Volatility': tensor([ 1.6897, -0.2052, -0.7625, -0.7220], grad_fn=<SqueezeBackward1>),\n",
       "  'Treasury Rate': tensor([-0.7439,  0.1717, -0.9728,  1.5450], grad_fn=<SqueezeBackward1>)},\n",
       " 'Input Surface': {'Log Moneyness': [tensor([-1.0316, -0.9561, -0.8829,  ...,  1.8599,  1.8834,  1.8834],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.8722, -0.8722, -0.8374,  ...,  1.7995,  1.8230,  1.8230],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-1.1743, -1.1547, -1.1353,  ...,  0.6968,  0.7331,  0.7511],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.6958, -0.6958, -0.6619,  ...,  1.9068,  1.9303,  1.9303],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.9064, -0.9064, -0.9064,  ...,  2.4460,  2.4460,  2.4460],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9064, -0.9064, -0.9064,  ...,  3.2121,  3.2121,  3.2121],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9064, -0.9064, -0.9064,  ...,  2.5669,  2.5669,  2.5669],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.9237, -0.9237, -0.9237,  ...,  2.9932,  2.9932,  2.9932],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.4187, 0.3716, 0.3716,  ..., 0.3194, 0.3320, 0.3194]),\n",
       "   tensor([0.4101, 0.4484, 0.4101,  ..., 0.3084, 0.2916, 0.3084]),\n",
       "   tensor([0.3347, 0.3347, 0.3347,  ..., 0.2339, 0.2302, 0.2355]),\n",
       "   tensor([0.5199, 0.7914, 0.5199,  ..., 0.3147, 0.2958, 0.3147])]},\n",
       " 'Query Points': {'Log Moneyness': [tensor([ 1.5988,  0.3847,  0.7443,  0.3847,  1.5458,  0.7995,  1.6763,  0.8887,\n",
       "            0.7069,  0.2292,  1.4640,  0.5711,  0.1358,  1.5724,  0.1358,  0.0137,\n",
       "            1.1065,  0.2520,  0.7812,  0.0137,  0.7257,  0.0877,  0.2062,  0.5309,\n",
       "            0.0877, -0.1681,  0.5105, -0.1413,  0.0386,  1.2618,  0.2062, -0.1413,\n",
       "            0.2747, -0.0626,  1.7881,  1.5857,  0.7812,  1.6249,  0.7629,  0.5511,\n",
       "            1.2769,  0.1358,  1.1542,  0.0632,  0.0137,  0.1358,  0.6879, -0.1681,\n",
       "           -0.0626,  0.4693,  1.5458,  1.4640, -0.0886,  0.4061, -0.1681,  1.3213,\n",
       "            0.6879,  0.0386,  0.1595, -0.1681,  0.4061,  1.3794,  0.3631, -0.0115,\n",
       "            0.2747,  0.7443,  0.6107,  0.6689,  0.3193,  0.1829, -0.1413,  0.4274,\n",
       "            1.3794,  0.2747,  1.0742,  0.2747,  0.2062,  0.2971, -0.1148,  0.8356,\n",
       "           -0.1681,  0.3413,  0.5711,  0.4693,  0.2971, -0.1952,  1.0250,  1.0084,\n",
       "            1.6507,  0.2520, -0.0626,  1.3360,  0.2971,  0.2971,  0.1118,  1.4221,\n",
       "           -0.0626,  1.3213,  0.2062,  0.8356, -0.0369,  0.0877,  1.1225,  1.1698,\n",
       "            0.2292,  1.3066,  0.2747,  1.6636,  0.7069, -0.0115,  0.0877,  1.2618,\n",
       "            1.2316,  1.2918,  0.2062,  0.3631,  0.8887,  0.2520,  1.4221,  1.6636,\n",
       "           -0.1952,  1.6379, -0.0886,  0.7069,  1.5323,  0.5105,  0.2062,  0.4061,\n",
       "           -0.1681, -0.0369,  0.0632,  0.3193,  0.0386,  0.4061,  0.5910,  0.7443,\n",
       "           -0.0369,  0.2292,  1.0415,  0.3193,  0.3847, -0.0369,  1.7390,  0.1829,\n",
       "            1.4079,  0.9062,  0.2520,  0.0877,  0.4900,  0.1358,  0.3413,  0.4484,\n",
       "            0.6496,  1.6763, -0.0626,  0.4061,  0.3193,  1.3066,  0.2520,  1.3794,\n",
       "           -0.1681,  0.5105,  0.4061,  0.3631,  0.0877,  0.1595,  0.2971,  0.5711,\n",
       "           -0.5150, -1.8418, -0.5463, -1.1899, -1.7374, -0.8118, -0.5779, -1.1095,\n",
       "           -0.1952, -0.3355, -2.0072, -0.5779, -1.0702, -2.1241, -0.6754, -1.0316,\n",
       "           -0.3939, -0.1681, -1.1899, -1.5886, -0.7770, -1.4943, -1.7890, -1.2312,\n",
       "           -1.4484, -0.7427, -1.8957, -1.9508, -0.7088, -0.3939, -0.2226, -0.8829,\n",
       "           -0.4236, -0.5779, -0.7770, -0.4236, -0.3068, -0.6100, -0.7088, -0.9936,\n",
       "           -0.4236, -1.2312, -0.9561, -0.6100, -1.3592, -1.0702, -1.5410, -1.3158,\n",
       "           -0.7088, -0.5463, -1.1494, -0.5779, -1.1095, -2.0649, -2.0649, -0.9936,\n",
       "           -0.6754, -0.3645, -1.9508, -0.9192, -0.7427, -0.7088, -0.3645, -0.7088,\n",
       "           -1.4034, -0.8118, -1.7374, -0.3068, -1.0316, -0.5463, -0.3645, -1.0316,\n",
       "           -1.7374, -0.9936, -1.2731, -1.4034, -0.6100, -2.0072, -1.2731, -1.3158,\n",
       "           -1.9508, -0.2504, -1.1899, -1.0702, -1.2731, -2.1241, -1.8957, -1.8418,\n",
       "           -0.6100, -0.2226, -1.4484, -1.7374, -0.2504, -0.6425, -0.6100, -1.1494,\n",
       "           -1.9508, -0.2504, -0.1681, -0.7770, -1.4943, -1.7374, -0.4537, -1.0702,\n",
       "           -0.4842, -0.2784, -0.3068, -0.8829, -0.9936, -0.6425, -1.6372, -0.8471,\n",
       "           -0.6425, -1.6868, -0.6754, -2.1846, -0.5150, -1.4484, -0.7427, -0.7427,\n",
       "           -0.3939, -0.5779, -0.8829, -0.5463, -0.3645, -2.2467, -1.7374, -0.9192,\n",
       "           -0.7088, -0.6425, -0.7088, -0.1952, -0.9936, -0.1952, -1.7374, -0.9561,\n",
       "           -0.3939, -1.1095, -0.3645, -0.2504, -0.3645, -0.8118, -0.3645, -0.2504,\n",
       "           -0.3645, -1.5886, -1.8418, -0.8118, -1.0316, -1.5410, -0.4842, -0.2226,\n",
       "           -2.0072, -0.4236, -0.5779, -1.5410, -0.2504, -0.7088, -1.1494, -0.6754,\n",
       "           -0.3939, -1.0316, -0.4537, -2.1846, -0.4842, -1.4034, -1.2312, -0.4236,\n",
       "           -0.5779, -0.3355, -1.1899, -0.5463, -1.8418, -0.4537, -1.1899, -0.6754,\n",
       "           -1.3158, -1.6372, -1.9508,  1.1225,  1.6379,  1.5857,  1.0579,  1.8123,\n",
       "            1.7637,  0.0877,  0.3413, -0.2504,  0.1358,  0.7812,  1.5052,  0.1358,\n",
       "            0.7443,  0.9578,  1.8599, -0.1952,  0.7069, -0.1952, -0.1413,  1.1854,\n",
       "            1.6119,  1.4778,  0.1829,  0.6302, -0.3645,  1.5323, -0.0369,  0.2062,\n",
       "            0.8176,  1.8362,  1.0579,  0.0137,  0.9407,  1.2316,  0.2747,  1.3505,\n",
       "            0.4274,  0.1595,  0.8534,  0.1118,  0.4693,  1.0250, -0.1413,  0.7257,\n",
       "            0.6302,  0.5105,  1.2009,  0.0632,  1.4079,  0.4900,  1.6763,  1.3937,\n",
       "            1.1065,  0.0386, -0.0626,  1.3360,  1.6249,  1.5458,  1.6119,  0.2292,\n",
       "            1.5323,  1.0904,  0.2971,  1.8599,  0.4900,  0.9407,  0.9578,  0.4274,\n",
       "            0.6302,  0.4061,  0.3193,  0.2747,  0.1118,  0.5309,  1.3650,  1.6763,\n",
       "            1.3066,  0.9916,  0.6496,  0.8356,  1.7390,  1.5988, -0.1148,  1.3213,\n",
       "            1.0084,  0.9578,  1.2769,  1.8599,  0.3413,  0.4061,  0.1118,  0.7812,\n",
       "            0.4693,  0.7069,  0.2292,  0.7443,  1.1384,  1.6890,  0.1829,  0.8534,\n",
       "            1.8362,  1.6507,  0.5309,  0.6689,  1.5052,  0.6689,  1.0904,  1.3360,\n",
       "            0.7812,  1.2618,  1.1225,  1.5052], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([ 0.0991,  0.0754, -0.5141, -0.2017, -0.2285, -0.3959,  0.5503,  0.2809,\n",
       "            0.5306,  0.0273,  0.4089,  0.2809, -0.0973,  0.5107, -0.3959, -0.2830,\n",
       "            0.3670, -0.4249, -0.2556, -0.3108, -0.0719, -0.1230, -0.0218,  0.1225,\n",
       "           -0.1230, -0.3108, -0.0218, -0.3672, -0.2285,  0.1688, -0.8031,  0.4501,\n",
       "            0.0514,  0.4089,  0.4089,  0.5892, -0.3959,  0.5107,  0.0028,  0.0028,\n",
       "           -0.3388,  0.0273,  0.2143,  0.2809,  0.5503, -0.2017, -0.1752,  0.2143,\n",
       "           -0.0467,  0.4705,  0.1688,  0.1916,  0.3880, -0.5141,  0.0273, -0.7358,\n",
       "           -0.5446,  0.1916,  0.5698, -0.5141, -0.2285, -0.4543,  0.4501,  0.1916,\n",
       "           -0.0719, -0.1752,  0.4501,  0.2589,  0.5698, -0.3959,  0.0514, -0.2017,\n",
       "           -0.5141, -0.2285,  0.3670,  0.1458, -0.7692,  0.2143, -0.0719,  0.3880,\n",
       "            0.5503, -0.3388, -0.0218, -0.2285,  0.0273, -0.2830,  0.4705, -0.3672,\n",
       "            0.3457,  0.2143, -0.0973, -0.1490,  0.0991, -0.2556,  0.1225,  0.4296,\n",
       "           -0.3108,  0.0754,  0.2367,  0.4907,  0.1916, -0.4543, -0.4249,  0.3670,\n",
       "            0.2367,  0.2367,  0.4907,  0.3243, -0.0218, -0.3108, -0.6704,  0.2143,\n",
       "           -0.0719, -0.0973, -0.2556,  0.2143, -0.6704, -0.3108, -0.2285, -0.1490,\n",
       "           -0.6383,  0.0991,  0.3243, -0.3388, -0.4840, -0.3108,  0.1225,  0.5107,\n",
       "            0.4907,  0.5306,  0.4705,  0.4296,  0.0991,  0.3027, -0.3959, -0.2017,\n",
       "           -0.0218, -0.3672,  0.4705,  0.3457,  0.1916, -0.4840,  0.2809,  0.3457,\n",
       "            0.1688, -0.3108,  0.4907,  0.0028, -0.6704, -0.1230, -0.0467, -0.5754,\n",
       "            0.1916,  0.1458, -0.5446,  0.2367, -0.3388, -0.4840,  0.4296,  0.4089,\n",
       "            0.2589,  0.0273,  0.0028, -0.7029, -0.3959, -0.0218,  0.3243,  0.0754,\n",
       "            0.4089, -0.5446, -0.6067,  0.3880,  0.3670, -0.8374, -0.6704,  0.0754,\n",
       "            1.4719,  1.5515,  0.6085,  0.1225,  0.7391,  1.5775,  1.5645,  0.2367,\n",
       "            1.0938, -0.2556,  0.8803,  0.0991,  0.0514,  1.3046,  0.0273,  1.6412,\n",
       "            0.2589,  1.3757,  0.6275,  0.4705,  0.9811,  0.1688,  0.9975,  0.6653,\n",
       "            0.4907,  0.7391,  0.8458,  1.1864,  1.0461,  1.4584,  1.3617,  0.6275,\n",
       "            0.4089,  0.7930,  1.7995,  1.1712,  0.9646,  0.6085,  0.1458,  1.2014,\n",
       "            0.9312,  1.5645,  0.3243,  0.7930,  0.1458,  0.3243, -0.0467,  1.1864,\n",
       "            1.0461,  1.1405,  0.7930,  0.9646,  1.3046,  0.0514,  0.6085,  1.4987,\n",
       "            0.3880,  0.1916,  0.3027,  1.4448,  0.4501,  0.0028, -0.1230,  1.6159,\n",
       "           -0.0218,  1.1712,  0.9144, -0.0719,  0.1916,  0.6085,  0.8974,  0.0514,\n",
       "            0.2809,  0.6465,  0.9480,  1.3475,  1.5120,  0.2143,  0.5698,  1.0780,\n",
       "            1.1094,  1.5775,  1.6662,  1.6159,  1.0461,  1.5253,  1.1559,  1.5120,\n",
       "            0.9144,  1.0461,  1.4584,  0.9811,  1.3190,  1.7033,  1.3475,  0.7025,\n",
       "            1.1094,  0.8631,  1.4036,  1.5903,  1.7758,  1.3757,  1.0300,  0.8631,\n",
       "            1.2756,  1.4174,  0.8458,  1.2901,  0.7752,  0.8107,  1.5120,  1.6537,\n",
       "            1.5253,  0.8974,  1.0300,  0.8283,  1.7758,  1.0621,  1.2165,  0.5503,\n",
       "            1.2609,  0.8458,  0.8283,  1.4719,  1.6786,  0.8107,  0.8631,  1.5775,\n",
       "            0.6653,  1.5903,  1.7277,  1.6412,  0.7025,  0.9646,  1.5645,  0.7391,\n",
       "            1.3617,  1.5515,  0.6465,  0.8283,  0.6653,  1.2901,  0.6653,  1.5903,\n",
       "            0.7572,  1.6537,  1.8230,  1.0461,  0.7572,  0.7572,  1.4312,  1.5384,\n",
       "            1.0300,  1.1864,  0.9975,  0.7208,  0.6275,  1.0621,  1.4174,  0.8974,\n",
       "            1.4854,  1.3617,  0.7572,  1.5903,  1.1559,  1.4036,  0.8458,  1.3757,\n",
       "            1.1250,  0.8974,  0.8107,  1.2165,  0.5892,  0.7572,  1.4174,  1.3046,\n",
       "            1.1712,  0.7391,  0.7025,  1.4036,  1.2462,  0.6839,  1.6537,  0.6839,\n",
       "            1.2165,  1.5120,  0.7930,  0.9811,  0.7391,  1.8230,  1.0621,  1.2901,\n",
       "            1.3190,  0.6465,  0.5892,  0.7752,  1.6159,  0.6653,  0.6465,  1.6159,\n",
       "            1.3333,  1.0621,  1.3897,  0.9480,  0.9811,  0.6085,  1.6159,  0.6839,\n",
       "            0.8283,  1.4987,  1.5515,  1.0461,  1.3190,  1.3757,  1.0621,  1.6159,\n",
       "            1.2314,  1.1094,  1.4719,  0.5892,  1.1559,  0.6839,  0.6085,  0.7025,\n",
       "            1.6786,  0.8458,  1.6286,  1.3757,  0.5107,  1.4036,  0.5698,  1.1094,\n",
       "            0.9312,  0.5306,  1.4987,  1.4312,  1.4719,  1.3897,  0.7391,  1.6910,\n",
       "            0.9312,  1.2462,  1.5515, -1.3335, -0.4840,  0.5698, -0.8722, -1.0920,\n",
       "            1.2756,  1.6286,  0.0514,  0.2809, -0.0973,  1.0938,  0.4907, -0.7358,\n",
       "           -1.2503,  0.9975,  1.3897, -0.2017, -0.4249,  0.9312, -0.2017,  0.9312,\n",
       "            0.6839, -1.1699, -0.6067, -0.8031,  1.1559,  0.9975, -1.6976,  1.2165,\n",
       "            1.6537,  0.4501, -0.1490, -1.2098, -1.2503, -1.4638, -0.7692, -1.0920,\n",
       "           -1.0540, -1.4196, -0.7692, -1.3335, -1.0540, -0.5141, -1.4196, -1.2916,\n",
       "           -0.8374, -0.7692, -1.3762, -0.9796, -0.3959, -1.0920, -1.3335, -1.3762,\n",
       "           -0.6383, -1.1306, -1.5088, -0.5446, -0.4543, -0.8722, -1.4638, -1.0165,\n",
       "           -0.7692, -0.9075, -0.9075, -1.3335, -0.7358, -1.3762, -1.6490, -1.4638,\n",
       "           -1.5088, -1.1306, -0.9075, -1.4196, -0.6383, -1.4638, -0.9796, -1.1699,\n",
       "           -0.7029, -0.4840, -0.7692, -0.7358, -1.5088, -1.4196, -0.4840, -0.4249,\n",
       "           -1.6014, -0.8374, -0.5754, -1.1699, -1.3335, -1.5547, -0.8374, -0.2017,\n",
       "           -1.7978, -0.8031, -0.9796, -1.5547, -1.6014, -0.6704, -0.8722, -0.8031,\n",
       "           -0.6067, -0.6383, -1.0920, -1.2503, -0.7029, -0.6383, -0.6067, -0.9433,\n",
       "           -0.7358, -1.0920, -0.5446, -1.1306, -1.0540, -0.8374, -1.1306, -0.7029,\n",
       "           -0.7692, -0.6704, -1.0920, -1.0540, -0.8031, -0.4840, -0.8031, -1.2098,\n",
       "           -1.0165, -1.1699, -0.3388, -0.4840, -1.4638, -1.2503, -0.7692, -1.1306,\n",
       "           -0.8722], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.3071, -0.3488, -0.9855, -0.8614, -0.8788, -0.0090, -0.6624, -0.8962,\n",
       "           -0.6945, -0.2125, -0.7765, -0.1993, -0.4199, -0.0583, -0.1993, -0.5231,\n",
       "           -0.1471, -0.4783, -0.5081, -0.5382, -0.7107,  0.4055, -0.6784, -0.9855,\n",
       "           -0.3071, -0.5382, -0.7434, -0.1861, -0.8788, -0.2661, -1.1547, -0.0959,\n",
       "           -1.0406, -0.4490, -0.7933, -0.5995, -0.6151, -0.0583, -0.6624, -0.3912,\n",
       "            0.2348, -1.0592, -0.2391,  0.3002, -0.0335, -0.7933,  0.0868, -0.9494,\n",
       "           -1.1161, -0.7933, -0.9855,  0.2568, -0.4055, -0.6465, -0.3488, -1.1161,\n",
       "            0.0750, -0.5995, -0.0212, -0.6465,  0.0632, -0.6151, -0.7599, -0.4490,\n",
       "           -1.1743, -0.4636, -0.8271,  0.2894, -0.6308, -0.5840, -0.6308, -0.4783,\n",
       "           -0.6784,  0.3216, -0.2526, -0.3071, -0.8101, -0.9138, -0.4055, -0.1342,\n",
       "           -0.6784, -0.5840, -0.4055, -0.8101, -0.3488, -0.5231, -0.7933,  0.2126,\n",
       "           -0.8101,  0.2677,  0.1217, -0.8271, -0.3348, -0.0459, -1.0780, -0.1086,\n",
       "           -0.9315, -1.0592, -0.2526, -0.0708, -0.4490, -0.9494, -0.0212, -0.1730,\n",
       "           -0.5686, -0.2258, -0.9855,  0.3216, -1.0970,  0.2568, -0.2258, -0.5840,\n",
       "           -1.1743, -0.4490, -0.8271, -0.5840, -1.0780, -0.1600, -0.4932, -0.7599,\n",
       "            0.0032, -1.0037, -0.9138, -0.5534, -0.6308, -0.4199, -0.6465, -0.7107,\n",
       "           -0.8442, -0.3348, -0.0833, -0.8614, -0.5995, -0.2125, -0.9138,  0.3429,\n",
       "           -0.3770, -0.0959, -0.0833, -0.1600, -0.2526, -0.6308, -0.3348, -0.4344,\n",
       "           -1.0406, -0.9315, -0.8442, -0.7270, -0.2258,  0.1101, -0.2526, -1.0221,\n",
       "           -0.5382, -0.2797, -0.5382, -0.3912, -0.9494, -0.6308, -0.7765, -0.1214,\n",
       "           -0.5534, -0.7107, -1.0780, -0.7434, -0.5840, -0.3770, -0.2797, -1.0221,\n",
       "           -0.7765, -0.1730, -1.0406, -0.4636, -0.1471, -1.1743, -0.7599, -0.6151,\n",
       "           -1.0406, -2.1494, -1.4879, -1.2744, -2.3949, -1.6731, -0.9673, -2.4604,\n",
       "           -1.5787, -1.0780, -1.4437, -1.0406, -1.2744, -1.4879, -2.1494, -0.8614,\n",
       "           -1.3156, -1.3365, -2.0633, -2.7042, -1.1353, -1.9802, -1.0780, -1.5557,\n",
       "           -1.4437, -2.4274, -1.6492, -1.2138, -1.5787, -1.5557, -2.1204, -1.7463,\n",
       "           -2.1494, -1.4879, -1.0780, -2.4274, -1.4657, -1.2744, -1.2339, -2.2085,\n",
       "           -2.1494, -1.9262, -1.4219, -1.6255, -2.3629, -2.1788, -1.6255, -1.3576,\n",
       "           -1.9802, -1.7463, -1.5787, -2.2085, -2.0633, -1.1547, -1.4879, -2.3000,\n",
       "           -1.2744, -2.2691, -1.2949, -1.4219, -1.5103, -1.4002, -2.2691, -2.0917,\n",
       "           -1.3156, -1.5557, -1.1743, -1.4437, -2.3629, -2.2386, -2.0076, -2.7042,\n",
       "           -2.7042, -1.3365, -2.1494, -2.5276, -1.3365, -1.6020, -1.3576, -2.4604,\n",
       "           -1.6020, -1.5103, -1.1547, -1.2744, -1.3156, -1.8997, -1.5557, -1.2540,\n",
       "           -2.4937, -1.5103, -1.0780, -1.8218, -2.0076, -1.7712, -2.2085, -1.2540,\n",
       "           -1.0970, -1.4002, -1.6731, -1.4219, -1.6973, -2.2691, -1.5329, -1.7712,\n",
       "           -1.6020, -1.4879, -1.4437, -2.1204, -1.3365, -1.4219, -1.4219, -1.3156,\n",
       "           -2.1204, -2.4937, -1.2540, -1.0592, -1.7712, -1.5557, -1.2744, -1.7964,\n",
       "           -2.0917, -2.0076, -0.5382, -0.3071, -0.6945,  0.4666, -0.5686, -0.2258,\n",
       "            0.3848, -0.5686,  0.6968,  0.1448,  0.4866,  0.7150,  0.7331, -0.3348,\n",
       "            0.0032, -0.2526,  0.1217,  0.5065,  0.3216,  0.7511,  0.2348,  0.4866,\n",
       "            0.1217,  0.2126,  0.7150,  0.1902, -0.9315, -0.1861, -0.6945,  0.0985,\n",
       "           -0.0959,  0.0985, -1.0037, -0.1214, -0.7765, -0.3770, -0.6151, -0.7933,\n",
       "           -0.2934, -0.8271,  0.0274, -0.3071, -0.9494, -0.7107,  0.0153, -0.6784,\n",
       "           -0.1861, -0.1730,  0.1676,  0.0032, -0.9673,  0.0985,  0.2348, -0.8101,\n",
       "            0.1902,  0.2126, -0.6945, -0.4932,  0.0032, -0.0335, -0.1471, -0.0959,\n",
       "           -0.7434, -0.1861,  0.1448, -0.1342, -0.3071,  0.0032,  0.1676, -0.3629,\n",
       "           -0.1214,  0.3002, -1.0037, -0.1730,  0.2568, -0.8271, -0.0708, -0.8962,\n",
       "           -0.8962, -0.3348,  0.0032,  0.3002, -0.1600, -0.0833,  0.3002, -0.8788,\n",
       "           -0.3629, -0.1086,  0.0750,  0.1448,  0.3216, -0.5995, -0.5231,  0.3216,\n",
       "           -0.7599, -0.1993, -0.4636, -0.8101,  0.0750, -0.2526,  0.2126, -0.3348,\n",
       "            0.0274,  0.2786, -0.0583,  0.3639, -0.5840,  0.3216, -0.2258, -0.3770,\n",
       "            0.3429, -0.4783, -0.9673,  0.1902, -0.2934, -0.2258,  0.2568, -0.1861],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([ 1.7735,  1.5521,  1.7485,  0.9356,  1.4548,  1.6588,  1.6718,  1.6976,\n",
       "            1.0553,  1.8592,  1.6976,  1.7735,  1.0385,  0.9180,  1.3237,  1.4263,\n",
       "            1.6193,  1.2632,  1.6457,  1.7232,  1.3087,  0.8645,  0.8464,  1.7859,\n",
       "            1.1211,  1.3682,  1.4690,  1.3087,  1.6588,  0.9704,  1.0553,  1.4970,\n",
       "            1.1694,  0.8281,  1.2167,  1.7232,  1.4406,  0.7726,  1.4406,  1.1048,\n",
       "            1.1211,  1.5109,  1.9068,  1.8831,  1.4406,  1.4548,  0.2761,  1.3829,\n",
       "            1.5927,  0.8098,  0.9876,  1.1534,  1.0047,  1.0884,  0.9180,  1.4119,\n",
       "            0.6965,  1.8831,  1.4263,  0.8281,  1.3237,  1.2632,  1.7232,  1.4263,\n",
       "            1.6326,  1.0884,  1.2478,  1.0047,  1.4406,  0.7726,  1.6848,  1.5109,\n",
       "            1.1534,  0.6180,  1.4406,  1.0047,  0.7726,  1.4548,  0.9530,  1.7105,\n",
       "            0.7348,  1.0047,  1.0217,  1.1534,  1.4830,  1.1211,  0.7348,  1.6588,\n",
       "            0.9356,  1.6976,  0.7726,  1.6976,  0.9356,  1.6326,  0.8098,  0.9704,\n",
       "            0.8824,  1.1373,  1.3974,  1.3829,  1.5521,  1.1694,  1.3829,  1.3087,\n",
       "            1.1853,  1.3387,  1.7485,  1.6718,  1.1694,  0.5162,  1.7232,  0.4743,\n",
       "            0.3662,  0.8098,  0.9003,  1.3535,  1.2323,  0.4100,  1.2785,  1.5792,\n",
       "            1.2011,  1.6326,  1.4548,  0.9876,  1.1048,  0.6379,  1.1534,  1.0385,\n",
       "            1.1048,  0.9356,  1.7232,  0.5980,  0.7726,  1.6326,  0.5574,  0.9704,\n",
       "            1.3535,  1.0217,  1.6457,  1.1211,  1.0217,  0.8281,  0.5778,  1.4119,\n",
       "            1.0553,  1.8831,  0.1827,  1.3535,  1.6588,  0.9003,  1.4548,  1.3387,\n",
       "            1.1534,  1.5792,  1.8592,  0.6965,  1.2323,  0.3440,  0.4953,  0.8098,\n",
       "            1.3535,  0.2761,  1.4406,  1.7735,  1.5521,  0.6576,  1.0553,  0.8645,\n",
       "            0.9530,  1.7359,  1.3682,  0.8098,  1.6718,  0.9530,  0.6180,  1.5927,\n",
       "            0.3882,  0.9356,  1.3387,  1.0385,  1.2167,  0.9876,  0.9180,  1.4119,\n",
       "            1.1211,  1.8592,  1.0553,  1.6457,  1.0047, -1.2262, -1.0233, -0.7649,\n",
       "           -0.9467, -0.6619, -0.7301, -1.5903, -0.4994, -0.2315, -1.4474, -1.3565,\n",
       "           -0.4681, -1.5903, -0.5311, -1.5903, -0.8723, -1.6399, -0.8002, -0.8360,\n",
       "           -0.9847, -1.7949, -1.6905, -0.9847, -1.4015, -1.4941, -0.9847, -1.6905,\n",
       "           -0.4681, -1.1025, -0.9092, -0.4373, -0.4681, -1.4474, -0.0944, -0.8002,\n",
       "           -1.2262, -1.3565, -1.2689, -0.7301, -0.8723, -1.3123, -1.3123, -1.6399,\n",
       "           -1.4941, -1.7421,  0.1101, -1.1430, -0.5956, -0.0944, -1.6905,  0.0606,\n",
       "           -1.6905, -1.1843, -0.3470, -1.1025, -1.4941, -0.8360, -0.9467, -0.2599,\n",
       "           -1.1025, -1.0233, -1.2689, -1.1843, -1.1025, -1.4015, -0.7649, -0.4068,\n",
       "           -1.2689, -1.3565, -1.3123, -0.2886, -0.8723, -1.6399, -0.1212, -1.4474,\n",
       "           -0.6285, -0.4068, -1.1430, -1.1430, -0.9467, -1.1843, -0.4681, -1.5417,\n",
       "           -0.8723, -0.9092, -1.2689,  0.0100, -0.1757, -0.7301, -0.4994, -1.3123,\n",
       "           -0.6958, -1.7949, -1.4015, -1.0233, -1.2689, -0.5956, -0.9847, -0.9467,\n",
       "           -0.6285, -0.8360, -1.1430, -1.0626,  0.6771, -0.1483,  0.1827,  0.1827,\n",
       "           -0.2315, -0.3767,  0.1587,  0.0606,  0.1587,  0.3662, -0.5631, -0.3176,\n",
       "            0.1587,  0.2298,  0.1827, -0.4681, -0.3767, -0.2886,  0.1827,  0.4953,\n",
       "            0.3216, -0.0417, -0.0944,  0.0606,  0.3882,  0.1101,  0.5574,  0.4743,\n",
       "           -0.2886,  0.7348, -0.0157,  0.1346,  0.4316,  0.5574,  0.5980, -0.0679,\n",
       "            0.3216, -0.4373,  0.1101, -0.2315,  0.2298, -0.0417, -0.4994, -0.4994,\n",
       "           -0.2315, -0.3176,  0.1101, -0.2035,  0.2531,  0.5369, -0.0417,  0.0855,\n",
       "            0.3440, -0.5956, -0.0944,  0.7912,  0.1101,  0.7348,  0.6379, -0.5311,\n",
       "           -0.1483, -0.3767,  0.2064,  0.0855,  0.7538,  0.1346, -0.0157, -0.5631,\n",
       "            0.3882,  0.0100,  0.6180, -0.4681,  0.3216,  0.7726,  0.5778,  0.3216,\n",
       "           -0.6285,  0.2989,  0.3882,  0.4100,  0.4953,  0.1346,  0.3882, -0.0157,\n",
       "            0.4530,  0.5162, -0.1757, -0.6285, -0.2315, -0.3767, -0.0944,  0.5574,\n",
       "           -0.0679,  0.1346, -0.3767, -0.2599, -0.2886, -0.0679,  0.1827,  0.0354,\n",
       "           -0.1757,  0.5369, -0.4068, -0.1483, -0.0157,  0.1101, -0.0679,  0.2531,\n",
       "            0.3216, -0.6285,  0.4100, -0.2886,  0.2531, -0.6958,  0.0855, -0.2886,\n",
       "           -0.2886, -0.4373,  0.2761, -0.3176,  0.6180, -0.0944, -0.1212,  0.2761,\n",
       "            0.3216,  0.1346, -0.1483,  0.2298, -0.0679, -0.2599,  0.5980,  0.0606,\n",
       "            0.2531, -0.3176,  0.1827,  0.2298,  0.3216,  0.1827,  0.4743,  0.0354,\n",
       "            0.3440,  0.1346, -0.1212,  0.0100,  0.5574, -0.2599,  0.0606,  0.0100,\n",
       "            0.5369,  0.2298,  0.4316, -0.4068, -0.3176, -0.3767, -0.3176,  0.5162,\n",
       "           -0.4068, -0.3176,  0.5162,  0.5574, -0.1757, -0.1757,  0.5369, -0.3470,\n",
       "            0.6576, -0.5631, -0.2886,  0.4530,  1.6326,  1.8831,  1.8106,  0.5162,\n",
       "            0.5980,  1.8350,  1.8350,  1.3237,  0.4316,  1.4406,  1.2011,  1.8592,\n",
       "            0.2531,  1.5521,  0.4743,  0.5574,  0.5574,  1.2323,  0.2531,  1.6060,\n",
       "            1.3535,  1.0047, -0.3176, -0.5631, -0.4373, -0.6285, -1.3123,  0.1101,\n",
       "            0.0606, -0.1483, -0.8360, -0.8360, -0.0944, -0.6285, -1.2262],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Time to Maturity': [tensor([-0.6991, -0.6991, -0.5378, -0.5378, -0.6991, -0.1749, -0.6991, -0.6991,\n",
       "           -0.8603, -0.6991, -0.8603, -0.5378, -0.1749, -0.6991, -0.9064, -0.8603,\n",
       "           -0.6991, -0.5378, -0.5378, -0.8258, -0.1749, -0.5378, -0.3362, -0.3362,\n",
       "           -0.8603, -0.5378, -0.8603, -0.3362, -0.3362, -0.8603, -0.3362, -0.9064,\n",
       "           -0.5378, -0.6991, -0.6991, -0.8603, -0.1749, -0.6991, -0.6991, -0.6991,\n",
       "           -0.6991, -0.8258, -0.6991, -0.9064, -0.7855, -0.8603, -0.6991, -0.7855,\n",
       "           -0.9064, -0.3362, -0.6991, -0.6991, -0.1749, -0.8258, -0.8258, -0.8603,\n",
       "           -0.8603, -0.8603, -0.8258, -0.9064, -0.5378, -0.6991, -0.8603, -0.1749,\n",
       "           -0.6991, -0.1749, -0.5378, -0.1749, -0.7855, -0.5378, -0.6991, -0.3362,\n",
       "           -0.8603, -0.8258, -0.8603, -0.3362, -0.6991, -0.5378, -0.8603, -0.6991,\n",
       "           -0.6991, -0.8258, -0.1749, -0.5378, -0.1749, -0.9064, -0.6991, -0.6991,\n",
       "           -0.6991, -0.1749, -0.8258, -0.6991, -0.8258, -0.7855, -0.8603, -0.6991,\n",
       "           -0.7855, -0.6991, -0.1749, -0.6991, -0.8603, -0.9064, -0.6991, -0.8603,\n",
       "           -0.8603, -0.6991, -0.6991, -0.6991, -0.6991, -0.9064, -0.8258, -0.8603,\n",
       "           -0.8603, -0.6991, -0.8258, -0.5378, -0.6991, -0.9064, -0.8603, -0.8603,\n",
       "           -0.8603, -0.8603, -0.5378, -0.6991, -0.6991, -0.3362, -0.5378, -0.1749,\n",
       "           -0.8603, -0.9064, -0.3362, -0.1749, -0.5378, -0.8603, -0.5378, -0.8603,\n",
       "           -0.3362, -0.7855, -0.8603, -0.8258, -0.8258, -0.6991, -0.6991, -0.8258,\n",
       "           -0.6991, -0.6991, -0.7855, -0.3362, -0.3362, -0.6991, -0.7855, -0.3362,\n",
       "           -0.1749, -0.8603, -0.1749, -0.7855, -0.3362, -0.8603, -0.6991, -0.6991,\n",
       "           -0.8603, -0.6991, -0.5378, -0.1749, -0.7855, -0.6991, -0.6991, -0.6991,\n",
       "            0.8735,  2.4460,  2.4460,  0.8735,  0.8735,  2.4460,  2.4460,  0.8735,\n",
       "            0.8735,  0.8735,  2.4460,  2.4460,  0.8735,  2.4460,  0.8735,  2.4460,\n",
       "            0.8735,  0.8735,  2.4460,  0.8735,  2.4460,  0.8735,  2.4460,  0.8735,\n",
       "            2.4460,  2.4460,  2.4460,  0.8735,  2.4460,  0.8735,  0.8735,  2.4460,\n",
       "            0.8735, -0.7855, -0.8603, -0.7855,  0.3493, -0.7855,  0.3493, -0.5378,\n",
       "           -0.8603, -0.8603,  0.3493, -0.6991,  0.3493, -0.8258,  0.3493, -0.6991,\n",
       "           -0.6991, -0.1749, -0.5378, -0.8258,  0.3493, -0.5378, -0.3362, -0.3362,\n",
       "           -0.8258,  0.3493, -0.8603, -0.1749, -0.6991, -0.5378, -0.6991,  0.3493,\n",
       "           -0.1749, -0.8603, -0.8603, -0.9064, -0.1749,  0.3493, -0.3362, -0.9064,\n",
       "           -0.1749, -0.1749, -0.1749, -0.3362,  0.3493, -0.3362, -0.8603,  0.3493,\n",
       "           -0.8603, -0.7855, -0.1749,  0.3493, -0.8603, -0.8603,  0.3493, -0.1749,\n",
       "           -0.6991, -0.9064, -0.5378, -0.5378, -0.6991, -0.8603, -0.8258, -0.1749,\n",
       "           -0.1749, -0.1749, -0.1749, -0.3362, -0.1749, -0.3362, -0.5378, -0.1749,\n",
       "           -0.3362, -0.9064, -0.8258, -0.3362, -0.3362, -0.9064, -0.3362, -0.5378,\n",
       "           -0.6991, -0.3362, -0.1749, -0.8603, -0.5378, -0.8603, -0.9064, -0.8603,\n",
       "            0.3493, -0.5378, -0.8603, -0.8603, -0.6991,  0.3493, -0.1749, -0.3362,\n",
       "           -0.6991, -0.6991, -0.8258, -0.3362,  0.3493, -0.5378, -0.6991, -0.9064,\n",
       "           -0.3362, -0.5378, -0.8258, -0.5378, -0.8603, -0.5378, -0.1749, -0.1749,\n",
       "           -0.7855, -0.1749, -0.8603, -0.8258, -0.8258, -0.1749, -0.1749, -0.8603,\n",
       "           -0.5378, -0.9064, -0.6991, -0.6991, -0.8258, -0.7855, -0.1749,  0.3493,\n",
       "           -0.5378,  0.3493,  0.3493, -0.5378, -0.9064, -0.6991,  0.3493,  0.3493,\n",
       "           -0.8258, -0.7855, -0.1749, -0.7855, -0.5378, -0.8603, -0.3362, -0.3362,\n",
       "           -0.8603, -0.5378,  0.3493,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,\n",
       "            2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,\n",
       "            2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,\n",
       "            2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,  2.4460,\n",
       "            2.4460,  2.4460,  2.4460,  2.4460,  0.3493,  0.3493,  0.8735,  0.3493,\n",
       "            0.3493,  0.3493,  0.3493,  0.8735,  0.3493,  0.3493,  0.8735,  0.8735,\n",
       "            0.8735,  0.8735, -0.1749,  0.8735, -0.1749,  0.8735,  0.3493,  0.3493,\n",
       "            0.3493,  0.3493,  0.8735,  0.3493,  0.3493,  0.3493,  0.3493,  0.8735,\n",
       "            0.3493,  0.3493,  0.8735,  0.3493,  0.3493, -0.1749,  0.3493,  0.8735,\n",
       "            0.3493,  0.8735,  0.3493,  0.3493,  0.8735,  0.3493, -0.1749,  0.3493,\n",
       "            0.3493,  0.3493,  0.8735, -0.1749,  0.3493,  0.3493,  0.8735,  0.3493,\n",
       "            0.3493, -0.1749, -0.1749,  0.3493,  0.8735,  0.3493,  0.3493,  0.8735,\n",
       "            0.8735,  0.8735,  0.8735,  0.3493, -0.1749,  0.3493,  0.8735, -0.1749,\n",
       "            0.3493,  0.3493,  0.8735,  0.3493,  0.3493,  0.8735, -0.1749, -0.1749,\n",
       "            0.8735, -0.1749, -0.1749,  0.3493], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.2958, -0.8200, -0.9064,  0.0670,  0.0670, -0.0942, -0.8200, -0.4571,\n",
       "           -0.4571, -0.7855, -0.7451, -0.2958, -0.2958, -0.2958, -0.8661, -0.6587,\n",
       "           -0.2958, -0.7855, -0.2958, -0.6587, -0.9064, -0.4571, -0.9064, -0.7451,\n",
       "           -0.7855, -0.2958,  0.0670, -0.8661,  0.0670, -0.6587, -0.9064, -0.6587,\n",
       "           -0.7451,  0.0670, -0.4571, -0.8200, -0.8200, -0.6587, -0.9064, -0.8200,\n",
       "           -0.4571, -0.7451, -0.6587, -0.0942, -0.6587, -0.9064, -0.0942, -0.4571,\n",
       "           -0.7451, -0.8200, -0.4571, -0.0942, -0.9064, -0.2958, -0.6587, -0.9064,\n",
       "           -0.4571,  0.0670, -0.6587, -0.6587, -0.0942, -0.2958, -0.4571, -0.7855,\n",
       "           -0.4571, -0.6587, -0.8200, -0.0942, -0.8200, -0.6587, -0.9064, -0.6587,\n",
       "           -0.8200, -0.4571, -0.7855, -0.8200, -0.8200, -0.7451, -0.2958, -0.6587,\n",
       "           -0.4571, -0.8200, -0.8200, -0.9064, -0.6587, -0.2958, -0.8200, -0.4571,\n",
       "           -0.7451, -0.0942, -0.0942,  0.0670, -0.8200, -0.7451, -0.8661, -0.2958,\n",
       "            0.0670, -0.4571, -0.8200, -0.2958, -0.7855, -0.9064, -0.0942, -0.8200,\n",
       "            0.0670, -0.2958, -0.0942, -0.0942, -0.7451, -0.4571, -0.7451,  0.0670,\n",
       "           -0.4571, -0.8200, -0.9064,  0.0670, -0.9064, -0.8661, -0.6587, -0.9064,\n",
       "           -0.4571, -0.7451, -0.8661, -0.2958, -0.6587, -0.7855,  0.0670, -0.4571,\n",
       "           -0.8661, -0.9064, -0.6587, -0.8200, -0.9064, -0.8200, -0.9064, -0.4571,\n",
       "           -0.6587, -0.7451, -0.6587, -0.2958, -0.2958, -0.2958, -0.7855, -0.9064,\n",
       "           -0.8661,  0.0670, -0.8661,  0.0670, -0.7451, -0.0942, -0.7855, -0.9064,\n",
       "           -0.9064, -0.6587, -0.7855, -0.7855,  0.0670, -0.6587, -0.4571, -0.6587,\n",
       "            0.0670,  0.0670, -0.7451, -0.6587, -0.2958, -0.2958, -0.7855, -0.7451,\n",
       "           -0.7451, -0.7451, -0.9064,  0.0670, -0.6587, -0.9064, -0.8200, -0.9064,\n",
       "            1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  0.5912,\n",
       "            1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,\n",
       "            0.5912,  0.5912,  1.1154,  1.1154,  1.1154,  0.5912,  1.1154,  1.1154,\n",
       "            0.5912,  0.5912,  1.1154,  0.5912,  0.5912,  1.1154,  1.1154,  0.5912,\n",
       "            1.1154,  1.1154,  1.1154,  0.5912,  0.5912,  0.5912,  1.1154,  1.1154,\n",
       "            1.1154,  1.1154,  1.1154,  1.1154,  0.5912,  1.1154,  1.1154,  1.1154,\n",
       "            1.1154,  1.1154,  0.5912,  0.5912,  0.5912,  0.5912,  0.5912,  1.1154,\n",
       "            0.5912,  1.1154,  1.1154,  1.1154,  1.1154,  0.5912,  0.5912,  1.1154,\n",
       "            1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,  1.1154,\n",
       "            1.1154,  0.5912,  1.1154,  0.5912,  1.1154,  0.5912,  1.1154,  0.5912,\n",
       "            0.5912,  0.0670, -0.4571, -0.8200, -0.6587, -0.8200, -0.8200, -0.4571,\n",
       "           -0.6587, -0.4571, -0.0942, -0.4571, -0.8200, -0.8200, -0.8200, -0.6587,\n",
       "            0.0670,  0.0670, -0.6587, -0.8200, -0.8200, -0.6587, -0.6587, -0.8200,\n",
       "           -0.6587,  0.0670, -0.4571, -0.8200, -0.0942, -0.2958, -0.4571, -0.8200,\n",
       "            0.0670, -0.6587, -0.2958, -0.2958,  0.0670, -0.8200, -0.4571, -0.0942,\n",
       "           -0.4571, -0.6587, -0.0942,  0.0670, -0.4571,  0.0670, -0.2958, -0.8200,\n",
       "            0.0670, -0.4571, -0.4571, -0.8200, -0.4571, -0.6587, -0.4571,  0.0670,\n",
       "           -0.0942, -0.0942,  0.0670, -0.4571, -0.2958, -0.0942, -0.8200,  0.0670,\n",
       "           -0.8200,  0.0670, -0.4571, -0.8200,  0.0670, -0.0942, -0.4571, -0.0942,\n",
       "            0.0670, -0.0942, -0.6587, -0.8200, -0.2958, -0.6587, -0.0942, -0.6587,\n",
       "           -0.0942, -0.8200, -0.6587,  0.0670, -0.0942, -0.0942, -0.0942, -0.8200,\n",
       "           -0.4571, -0.2958, -0.2958, -0.0942, -0.4571, -0.0942, -0.6587,  0.0670,\n",
       "           -0.4571, -0.2958, -0.4571, -0.0942, -0.6587, -0.4571, -0.4571, -0.0942,\n",
       "           -0.8200, -0.8200, -0.0942,  0.0670, -0.4571, -0.8200, -0.4571,  0.0670,\n",
       "            0.0670, -0.9064, -0.6587,  0.0670, -0.8200, -0.0942, -0.4571,  0.0670,\n",
       "           -0.6587, -0.0942,  0.0670, -0.0942, -0.0942, -0.0942,  0.0670, -0.8200,\n",
       "            0.0670, -0.6587, -0.0942, -0.0942, -0.0942, -0.4571, -0.6587, -0.0942,\n",
       "            0.0670, -0.6587, -0.6587, -0.2958, -0.6587, -0.2958, -0.8200, -0.0942,\n",
       "            0.0670, -0.8200,  0.0670,  0.0670,  0.0670,  0.0670, -0.4571, -0.0942,\n",
       "           -0.0942, -0.0942,  0.0670, -0.6587, -0.4571, -0.0942, -0.2958, -0.8200,\n",
       "           -0.6587,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,\n",
       "            3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,\n",
       "            3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,\n",
       "            3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,  3.2121,\n",
       "            3.2121,  3.2121,  3.2121, -0.8200,  0.0670,  0.5912, -0.0942, -0.8200,\n",
       "           -0.6587, -0.0942, -0.6587,  0.0670, -0.2958,  1.1154, -0.6587,  0.0670,\n",
       "            1.1154, -0.6587, -0.2958,  1.1154,  1.1154, -0.0942, -0.0942,  0.5912,\n",
       "            0.5912, -0.2958,  1.1154,  0.0670,  0.0670,  1.1154, -0.2958,  0.0670,\n",
       "           -0.2958, -0.6587, -0.6587,  0.5912,  0.5912, -0.2958,  0.5912, -0.0942,\n",
       "            0.5912,  0.0670, -0.2958, -0.2958, -0.0942,  0.0670, -0.8200,  0.5912,\n",
       "           -0.4571,  0.5912, -0.2958, -0.2958,  0.0670,  0.0670,  0.0670,  0.5912,\n",
       "           -0.4571, -0.4571,  0.0670,  0.0670, -0.2958,  0.0670, -0.6587,  0.5912,\n",
       "            1.1154, -0.0942,  0.5912,  1.1154,  1.1154, -0.2958, -0.0942,  0.0670,\n",
       "           -0.0942,  0.0670, -0.8200, -0.0942, -0.0942,  1.1154,  1.1154,  1.1154,\n",
       "            0.5912, -0.2958,  1.1154, -0.8200,  0.0670,  0.0670, -0.6587,  1.1154,\n",
       "            0.0670, -0.0942,  0.0670,  0.5912,  0.5912,  1.1154, -0.6587, -0.6587,\n",
       "           -0.8200, -0.8200,  1.1154,  1.1154, -0.4571, -0.4571, -0.0942, -0.4571,\n",
       "            0.5912], grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([-0.5781, -0.8661, -0.9064, -0.2152, -0.2152, -0.5781, -0.8258, -0.5781,\n",
       "           -0.5781, -0.8258, -0.7394, -0.5781, -0.5781, -0.5781, -0.9064, -0.7394,\n",
       "           -0.5781, -0.8258, -0.5781, -0.7394, -0.9064, -0.7394, -0.9064, -0.7394,\n",
       "           -0.8258, -0.5781, -0.2152, -0.9064, -0.2152, -0.7394, -0.9064, -0.7394,\n",
       "           -0.7394, -0.2152, -0.5781, -0.8258, -0.8661, -0.7394, -0.9064, -0.8661,\n",
       "           -0.7394, -0.7394, -0.7394, -0.5781, -0.7394, -0.9064, -0.5781, -0.5781,\n",
       "           -0.7394, -0.8258, -0.5781, -0.5781, -0.9064, -0.5781, -0.7394, -0.9064,\n",
       "           -0.7394, -0.2152, -0.7394, -0.7394, -0.5781, -0.5781, -0.5781, -0.7855,\n",
       "           -0.5781, -0.7394, -0.8258, -0.5781, -0.8258, -0.7394, -0.9064, -0.7394,\n",
       "           -0.8661, -0.7394, -0.7855, -0.8661, -0.8661, -0.7394, -0.5781, -0.7394,\n",
       "           -0.5781, -0.8661, -0.8661, -0.9064, -0.7394, -0.5781, -0.8258, -0.7394,\n",
       "           -0.7394, -0.5781, -0.5781, -0.2152, -0.8661, -0.7855, -0.8661, -0.5781,\n",
       "           -0.2152, -0.5781, -0.8661, -0.5781, -0.7855, -0.9064, -0.5781, -0.8661,\n",
       "           -0.2152, -0.5781, -0.2152, -0.5781, -0.7394, -0.7394, -0.7855, -0.2152,\n",
       "           -0.5781, -0.8661, -0.9064, -0.2152, -0.9064, -0.9064, -0.7394, -0.9064,\n",
       "           -0.7394, -0.7394, -0.8661, -0.5781, -0.7394, -0.8258, -0.2152, -0.5781,\n",
       "           -0.8661, -0.9064, -0.7394, -0.8258, -0.9064, -0.8661, -0.9064, -0.7394,\n",
       "           -0.7394, -0.7855, -0.7394, -0.5781, -0.5781, -0.5781, -0.7855, -0.9064,\n",
       "           -0.8661, -0.2152, -0.8661, -0.2152, -0.7855, -0.5781, -0.8258, -0.9064,\n",
       "           -0.9064, -0.7394, -0.8258, -0.7855, -0.2152, -0.7394, -0.5781, -0.7394,\n",
       "           -0.2152, -0.2152, -0.7394, -0.7394, -0.5781, -0.5781, -0.7855, -0.7394,\n",
       "           -0.7394, -0.7855, -0.9064, -0.2152, -0.7394, -0.9064, -0.8661, -0.9064,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669, -0.2152, -0.2152,  0.4703,  0.4703,  0.3090,  0.4703, -0.5781,\n",
       "           -0.5781, -0.7394, -0.2152,  0.4703, -0.7394, -0.7394, -0.7394, -0.5781,\n",
       "           -0.2152,  0.3090,  0.4703,  0.4703,  0.4703, -0.2152, -0.5781, -0.7394,\n",
       "           -0.7394,  0.3090, -0.7394,  0.3090,  0.4703, -0.7394,  0.4703,  0.3090,\n",
       "            0.3090,  0.3090, -0.2152,  0.4703,  0.4703, -0.2152,  0.3090,  0.4703,\n",
       "           -0.7394, -0.2152, -0.2152, -0.2152,  0.4703, -0.5781,  0.4703,  0.3090,\n",
       "            0.3090, -0.5781,  0.3090,  0.3090, -0.7394, -0.2152,  0.4703,  0.4703,\n",
       "           -0.7394, -0.7394,  0.4703, -0.7394, -0.2152, -0.2152, -0.7394,  0.4703,\n",
       "           -0.5781, -0.7394, -0.2152, -0.2152,  0.4703, -0.7394,  0.3090, -0.5781,\n",
       "            0.4703, -0.5781,  0.3090, -0.7394,  0.4703, -0.7394, -0.7394, -0.2152,\n",
       "            0.4703,  0.4703, -0.2152,  0.4703,  0.3090, -0.7394, -0.2152,  0.3090,\n",
       "           -0.2152, -0.5781,  0.3090,  0.4703, -0.5781,  0.3090, -0.2152,  0.4703,\n",
       "            0.3090, -0.7394,  0.4703,  0.3090,  0.4703,  0.3090,  0.3090, -0.7394,\n",
       "           -0.7394, -0.7394,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,  2.5669,\n",
       "            2.5669,  2.5669,  2.5669,  2.5669,  0.3090,  0.4703,  0.4703, -0.2152,\n",
       "            0.3090,  0.3090,  0.3090,  0.4703,  0.4703,  0.4703,  0.4703,  0.3090,\n",
       "            0.4703,  0.4703,  0.4703, -0.2152,  0.4703,  0.3090,  0.3090,  0.3090,\n",
       "            0.4703, -0.2152,  0.3090,  0.4703,  0.3090,  0.4703,  0.3090,  0.3090,\n",
       "            0.3090, -0.2152,  0.3090,  0.4703,  0.4703, -0.2152,  0.3090,  0.4703,\n",
       "            0.3090, -0.2152,  0.3090,  0.3090,  0.4703,  0.3090, -0.2152, -0.2152,\n",
       "            0.4703,  0.3090,  0.4703,  0.4703,  0.3090,  0.3090,  0.4703,  0.3090,\n",
       "            0.4703, -0.2152, -0.2152,  0.3090,  0.4703,  0.3090,  0.4703,  0.4703,\n",
       "            0.4703,  0.3090, -0.2152,  0.3090, -0.2152,  0.3090,  0.4703,  0.4703,\n",
       "            0.4703, -0.2152,  0.3090,  0.3090, -0.2152,  0.3090, -0.2152, -0.2152,\n",
       "            0.4703,  0.3090, -0.2152,  0.4703,  0.3090, -0.2152,  0.4703,  0.3090,\n",
       "           -0.2152,  0.3090,  0.4703, -0.2152,  0.4703,  0.3090, -0.2152, -0.2152],\n",
       "          grad_fn=<SplitWithSizesBackward0>),\n",
       "   tensor([ 0.8965, -0.1518, -0.1518, -0.8776, -0.8776,  0.8965, -0.3131, -0.1518,\n",
       "           -0.3131,  0.8965, -0.6760, -0.6760, -0.1518, -0.5147, -0.3131,  0.8965,\n",
       "            0.8965,  0.3723, -0.1518, -0.3131, -0.8776,  0.3723, -0.6760,  0.8965,\n",
       "           -0.3131,  0.8965, -0.6760,  0.8965, -0.6760, -0.3131,  0.3723, -0.8776,\n",
       "           -0.1518, -0.3131,  0.3723, -0.6760, -0.8776,  0.8965,  0.3723,  0.8965,\n",
       "            0.3723, -0.8776, -0.6760,  0.8965, -0.3131, -0.1518,  0.8965, -0.8776,\n",
       "            0.8965, -0.1518, -0.5147,  0.3723,  0.3723, -0.1518, -0.8776, -0.1518,\n",
       "            0.3723, -0.1518, -0.6760, -0.1518,  0.8965, -0.8776, -0.3131, -0.3131,\n",
       "           -0.1518, -0.8776, -0.8776, -0.1518,  0.3723,  0.3723, -0.3131, -0.3131,\n",
       "           -0.8776,  0.8965,  0.8965, -0.8776,  0.3723, -0.8776, -0.6760, -0.6760,\n",
       "            0.8965, -0.5147, -0.8776, -0.1518, -0.3131,  0.8965, -0.3131, -0.6760,\n",
       "           -0.5147,  0.8965, -0.1518, -0.3131,  0.3723,  0.8965, -0.3131, -0.6760,\n",
       "            0.8965, -0.6760, -0.8776,  0.8965, -0.1518, -0.3131,  0.3723,  0.8965,\n",
       "           -0.1518, -0.3131,  0.8965, -0.6760, -0.6760,  0.3723, -0.1518,  0.8965,\n",
       "            0.8965, -0.6760, -0.3131,  0.3723,  0.3723,  0.3723, -0.1518, -0.6760,\n",
       "           -0.3131, -0.1518, -0.6760,  0.8965, -0.6760,  0.8965,  0.8965, -0.8776,\n",
       "            0.3723, -0.3131,  0.8965, -0.1518,  0.8965, -0.6760,  0.8965,  0.3723,\n",
       "           -0.3131, -0.3131, -0.1518, -0.5147,  0.3723, -0.3131,  0.3723, -0.3131,\n",
       "            0.3723, -0.1518,  0.8965, -0.1518,  0.8965, -0.1518, -0.3131, -0.1518,\n",
       "           -0.1518,  0.8965, -0.6760, -0.1518, -0.8776,  0.3723,  0.3723,  0.8965,\n",
       "           -0.8776,  0.8965, -0.3131,  0.8965, -0.3131, -0.3131, -0.5147, -0.5147,\n",
       "           -0.8776,  0.8965,  0.8965, -0.3131, -0.1518,  0.8965,  0.3723, -0.1518,\n",
       "            0.3723, -0.3131,  0.8965,  0.8965,  0.3723, -0.1518, -0.6760, -0.1518,\n",
       "           -0.8776, -0.1518, -0.6760,  0.8965, -0.8776, -0.8776,  0.8965, -0.8776,\n",
       "           -0.8776, -0.5147, -0.8776,  0.8965, -0.3131,  0.8965, -0.1518,  0.8965,\n",
       "           -0.1518, -0.5147,  0.8965, -0.1518, -0.8776,  0.3723, -0.5147, -0.8776,\n",
       "           -0.3131, -0.1518,  0.8965, -0.5147,  0.8965, -0.8776,  0.8965,  0.3723,\n",
       "           -0.3131, -0.1518,  0.3723, -0.1518,  0.3723, -0.5147,  0.3723, -0.8776,\n",
       "           -0.1518, -0.1518,  0.8965,  0.8965, -0.1518, -0.5147, -0.3131,  0.3723,\n",
       "           -0.6760, -0.3131,  0.8965, -0.1518, -0.1518,  0.8965, -0.3131,  0.8965,\n",
       "           -0.1518,  0.8965,  0.8965, -0.1518,  0.8965, -0.7624, -0.5147,  0.8965,\n",
       "            0.8965,  0.3723,  0.3723,  0.8965,  0.3723,  0.3723,  0.3723,  0.3723,\n",
       "           -0.8776, -0.6760, -0.6760,  0.8965, -0.6760, -0.1518,  0.3723,  0.8965,\n",
       "           -0.1518, -0.1518, -0.8776, -0.3131, -0.6760, -0.1518, -0.1518,  0.3723,\n",
       "           -0.8776, -0.5147,  0.3723,  0.8965,  0.8965, -0.5147,  0.8965, -0.6760,\n",
       "            0.8965, -0.1518, -0.1518, -0.5147, -0.3131,  0.3723, -0.6760, -0.6760,\n",
       "           -0.6760,  0.3723,  0.3723, -0.5147, -0.8776, -0.8776, -0.9237, -0.8431,\n",
       "           -0.8431, -0.7624, -0.8431, -0.8776,  0.3723, -0.3131, -0.9237, -0.8085,\n",
       "           -0.7624, -0.6760, -0.3131, -0.9237, -0.6760, -0.6760, -0.1518, -0.5147,\n",
       "           -0.8431, -0.6760, -0.8776, -0.7624, -0.3131, -0.3131, -0.6760, -0.8431,\n",
       "           -0.8431, -0.8776,  0.3723, -0.6760, -0.8776, -0.6760, -0.8776, -0.3131,\n",
       "           -0.6760, -0.5147, -0.8776, -0.5147,  0.3723, -0.8776, -0.6760, -0.8085,\n",
       "           -0.3131, -0.6760,  0.3723, -0.8431, -0.7624, -0.3131, -0.5147, -0.8431,\n",
       "           -0.8776, -0.6760, -0.5147, -0.8776, -0.9237, -0.8776, -0.8776, -0.8776,\n",
       "           -0.7624, -0.5147, -0.5147, -0.6760, -0.6760, -0.9237, -0.6760, -0.7624,\n",
       "           -0.6760, -0.6760, -0.8776, -0.8776, -0.8085, -0.6760, -0.8776, -0.8776,\n",
       "           -0.8431, -0.8431, -0.9237, -0.9237, -0.8776, -0.8085, -0.6760, -0.7624,\n",
       "           -0.3131, -0.9237, -0.8431, -0.8085, -0.8776, -0.8431, -0.5147, -0.9237,\n",
       "           -0.8085, -0.8431, -0.5147, -0.8085, -0.9237,  0.3723, -0.8776, -0.3131,\n",
       "           -0.5147, -0.1518, -0.9237, -0.5147, -0.7624, -0.6760, -0.8776, -0.3131,\n",
       "           -0.3131, -0.8431, -0.5147, -0.8431, -0.8431, -0.8776, -0.6760, -0.8776,\n",
       "           -0.9237, -0.8431, -0.7624, -0.7624, -0.6760, -0.8085, -0.3131, -0.1518,\n",
       "           -0.7624, -0.8776, -0.3131, -0.6760, -0.5147, -0.1518, -0.3131, -0.8776,\n",
       "           -0.5147, -0.8431, -0.1518, -0.5147, -0.1518, -0.6760, -0.6760, -0.8085,\n",
       "           -0.1518, -0.1518, -0.8776,  0.3723, -0.3131, -0.6760, -0.9237, -0.9237,\n",
       "           -0.3131, -0.8085, -0.9237, -0.6760, -0.1518, -0.8776, -0.5147, -0.5147,\n",
       "           -0.6760, -0.8431, -0.6760, -0.3131, -0.7624, -0.1518, -0.5147, -0.5147,\n",
       "           -0.5147, -0.8431, -0.8776, -0.5147,  2.9932,  2.9932,  2.9932,  2.9932,\n",
       "            2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,\n",
       "            2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,\n",
       "            2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,\n",
       "            2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932,  2.9932],\n",
       "          grad_fn=<SplitWithSizesBackward0>)],\n",
       "  'Implied Volatility': [tensor([0.2695, 0.2695, 0.3415, 0.3091, 0.3302, 0.2849, 0.2695, 0.3302, 0.2561,\n",
       "           0.2912, 0.3295, 0.3461, 0.2658, 0.3302, 0.3133, 0.3065, 0.3302, 0.2950,\n",
       "           0.3461, 0.2847, 0.3088, 0.2847, 0.2711, 0.2940, 0.2561, 0.2887, 0.2561,\n",
       "           0.2683, 0.2662, 0.3295, 0.2817, 0.3133, 0.3271, 0.2548, 0.3302, 0.3295,\n",
       "           0.2849, 0.3302, 0.3302, 0.2695, 0.3302, 0.2702, 0.3302, 0.3716, 0.2707,\n",
       "           0.3295, 0.3302, 0.2604, 0.3133, 0.3104, 0.2695, 0.3302, 0.2631, 0.2702,\n",
       "           0.2654, 0.3295, 0.2561, 0.3138, 0.2702, 0.3153, 0.3128, 0.2695, 0.2561,\n",
       "           0.2665, 0.3024, 0.2849, 0.3461, 0.2849, 0.2661, 0.3066, 0.2564, 0.2844,\n",
       "           0.2561, 0.2702, 0.3295, 0.2745, 0.2695, 0.3000, 0.2636, 0.3302, 0.2585,\n",
       "           0.3356, 0.2849, 0.3216, 0.2785, 0.3190, 0.3302, 0.2695, 0.3302, 0.2676,\n",
       "           0.2737, 0.2695, 0.3356, 0.2661, 0.3295, 0.3302, 0.2601, 0.2695, 0.2717,\n",
       "           0.2695, 0.2859, 0.3133, 0.2695, 0.2561, 0.3295, 0.3302, 0.2695, 0.2695,\n",
       "           0.3302, 0.3133, 0.3047, 0.2561, 0.2561, 0.2695, 0.3356, 0.3413, 0.2695,\n",
       "           0.3133, 0.2561, 0.3295, 0.2616, 0.3295, 0.2888, 0.2695, 0.3302, 0.3104,\n",
       "           0.3096, 0.2740, 0.2607, 0.3133, 0.2666, 0.2775, 0.2929, 0.3295, 0.3461,\n",
       "           0.2561, 0.2710, 0.2661, 0.2561, 0.3356, 0.2702, 0.2581, 0.3302, 0.2702,\n",
       "           0.3302, 0.3302, 0.3215, 0.2669, 0.2877, 0.2759, 0.2661, 0.2854, 0.2985,\n",
       "           0.3295, 0.2631, 0.2661, 0.2770, 0.2561, 0.2609, 0.3302, 0.2628, 0.2695,\n",
       "           0.3461, 0.2716, 0.2871, 0.2804, 0.3116, 0.3302, 0.2894, 0.4638, 0.3104,\n",
       "           0.3651, 0.3439, 0.3153, 0.3111, 0.3495, 0.2803, 0.2844, 0.3451, 0.3081,\n",
       "           0.3100, 0.3491, 0.2940, 0.3186, 0.2867, 0.2829, 0.3227, 0.4616, 0.3150,\n",
       "           0.4355, 0.4535, 0.3170, 0.3788, 0.3126, 0.4782, 0.3439, 0.3120, 0.2870,\n",
       "           0.2817, 0.3202, 0.2878, 0.3081, 0.3473, 0.2819, 0.2757, 0.3187, 0.2901,\n",
       "           0.4840, 0.2996, 0.3473, 0.3051, 0.3044, 0.4344, 0.3507, 0.4801, 0.3369,\n",
       "           0.3235, 0.2824, 0.3695, 0.3381, 0.3175, 0.3695, 0.3459, 0.3335, 0.3648,\n",
       "           0.2771, 0.3258, 0.3508, 0.3295, 0.3267, 0.2716, 0.2983, 0.3472, 0.3258,\n",
       "           0.3258, 0.3338, 0.3280, 0.2834, 0.2778, 0.3716, 0.3472, 0.3235, 0.3472,\n",
       "           0.3459, 0.2899, 0.3459, 0.3473, 0.3347, 0.3473, 0.2607, 0.3822, 0.3535,\n",
       "           0.3258, 0.3258, 0.3469, 0.3472, 0.3114, 0.3147, 0.3695, 0.3695, 0.2622,\n",
       "           0.3473, 0.3632, 0.3422, 0.3472, 0.2690, 0.2665, 0.3398, 0.3472, 0.6848,\n",
       "           0.3001, 0.3320, 0.2909, 0.3227, 0.2803, 0.3700, 0.4059, 0.4187, 0.3459,\n",
       "           0.3469, 0.3103, 0.3459, 0.2912, 0.3258, 0.3273, 0.3258, 0.4187, 0.3473,\n",
       "           0.2778, 0.3379, 0.3473, 0.3473, 0.2693, 0.6979, 0.3822, 0.3242, 0.3397,\n",
       "           0.3146, 0.3507, 0.2703, 0.3390, 0.2887, 0.3510, 0.4187, 0.2792, 0.5274,\n",
       "           0.2886, 0.2902, 0.2826, 0.3419, 0.2725, 0.2686, 0.2735, 0.3472, 0.3473,\n",
       "           0.3507, 0.3381, 0.3822, 0.2809, 0.2643, 0.3695, 0.3716, 0.2985, 0.3510,\n",
       "           0.2723, 0.3187, 0.3822, 0.2952, 0.2957, 0.3109, 0.2810, 0.5483, 0.4187,\n",
       "           0.3369, 0.3277, 0.2782, 0.3401, 0.2721, 0.3472, 0.3002, 0.3695, 0.3209,\n",
       "           0.3459, 0.2989, 0.3473, 0.5483, 0.3469, 0.3216, 0.3192, 0.3167, 0.3136,\n",
       "           0.3194, 0.3264, 0.3043, 0.3047, 0.3011, 0.3043, 0.2967, 0.3194, 0.2968,\n",
       "           0.2973, 0.3001, 0.3310, 0.3061, 0.2967, 0.3011, 0.3056, 0.3047, 0.3182,\n",
       "           0.3194, 0.3037, 0.2964, 0.3076, 0.3194, 0.2982, 0.3043, 0.2974, 0.3292,\n",
       "           0.3020, 0.3053, 0.3042, 0.3089, 0.2767, 0.3089, 0.2854, 0.2671, 0.2943,\n",
       "           0.2808, 0.2738, 0.3089, 0.2829, 0.2853, 0.2823, 0.2788, 0.3124, 0.2811,\n",
       "           0.2849, 0.2871, 0.3116, 0.3116, 0.3089, 0.2682, 0.2819, 0.3089, 0.3089,\n",
       "           0.3089, 0.3116, 0.2819, 0.3089, 0.3116, 0.2767, 0.3116, 0.2879, 0.2849,\n",
       "           0.3089, 0.2858, 0.2794, 0.2777, 0.2785, 0.2685, 0.2765, 0.2760, 0.3124,\n",
       "           0.3089, 0.3116, 0.3089, 0.2829, 0.2849, 0.3116, 0.3116, 0.2792, 0.3116,\n",
       "           0.3096, 0.3069, 0.3124, 0.3089, 0.2824, 0.2717, 0.2672, 0.2872, 0.2779,\n",
       "           0.3000, 0.2763, 0.3089, 0.3124, 0.3089, 0.2814, 0.3229, 0.3089, 0.3116,\n",
       "           0.2791, 0.2818, 0.3116, 0.2978, 0.2849, 0.2849, 0.2999, 0.2849, 0.2849,\n",
       "           0.3089]),\n",
       "   tensor([0.2906, 0.3365, 0.5096, 0.2726, 0.2813, 0.2881, 0.4041, 0.2970, 0.3017,\n",
       "           0.3056, 0.3919, 0.3156, 0.2899, 0.3284, 0.4003, 0.2914, 0.3032, 0.3120,\n",
       "           0.2953, 0.2862, 0.4731, 0.2803, 0.4101, 0.3146, 0.3002, 0.2974, 0.2711,\n",
       "           0.3724, 0.2737, 0.3042, 0.4484, 0.3578, 0.3025, 0.2807, 0.3123, 0.4041,\n",
       "           0.3174, 0.3151, 0.4731, 0.3529, 0.2829, 0.2985, 0.3123, 0.2841, 0.3578,\n",
       "           0.3676, 0.2858, 0.2881, 0.2924, 0.5174, 0.2851, 0.2802, 0.4101, 0.3086,\n",
       "           0.3042, 0.4101, 0.2949, 0.2717, 0.3578, 0.3110, 0.2854, 0.3037, 0.3211,\n",
       "           0.3597, 0.2819, 0.2819, 0.4041, 0.2829, 0.5174, 0.2948, 0.4731, 0.2908,\n",
       "           0.3375, 0.2759, 0.4201, 0.4041, 0.6361, 0.2893, 0.3017, 0.3151, 0.3017,\n",
       "           0.3124, 0.3546, 0.3681, 0.2894, 0.3011, 0.4041, 0.2805, 0.3755, 0.2891,\n",
       "           0.2829, 0.2718, 0.3510, 0.2962, 0.3968, 0.3091, 0.2810, 0.2859, 0.4136,\n",
       "           0.3163, 0.3556, 0.4101, 0.2838, 0.4041, 0.2839, 0.2957, 0.3016, 0.2990,\n",
       "           0.2940, 0.2829, 0.3308, 0.2726, 0.2756, 0.3271, 0.3696, 0.2801, 0.6375,\n",
       "           0.3452, 0.2908, 0.3983, 0.3010, 0.3096, 0.4799, 0.3030, 0.3020, 0.3011,\n",
       "           0.2828, 0.3276, 0.3968, 0.4731, 0.3151, 0.4041, 0.4731, 0.4456, 0.4101,\n",
       "           0.2811, 0.2869, 0.3084, 0.3578, 0.3271, 0.2935, 0.3068, 0.3597, 0.4101,\n",
       "           0.4562, 0.2751, 0.4799, 0.2696, 0.4300, 0.2833, 0.2944, 0.4101, 0.4101,\n",
       "           0.3013, 0.3262, 0.3597, 0.2774, 0.3007, 0.3017, 0.3151, 0.2728, 0.2706,\n",
       "           0.2875, 0.3255, 0.3013, 0.3018, 0.3597, 0.2893, 0.2893, 0.3695, 0.4101,\n",
       "           0.2791, 0.3151, 0.4484, 0.3801, 0.4731, 0.2950, 0.2950, 0.2879, 0.2686,\n",
       "           0.2922, 0.2675, 0.2950, 0.2749, 0.2950, 0.2827, 0.2675, 0.2659, 0.2781,\n",
       "           0.2950, 0.2811, 0.2675, 0.2753, 0.2768, 0.2864, 0.2832, 0.2675, 0.2768,\n",
       "           0.2675, 0.2718, 0.2800, 0.2751, 0.2675, 0.2768, 0.2768, 0.2950, 0.2675,\n",
       "           0.2703, 0.2664, 0.2675, 0.2950, 0.2938, 0.2938, 0.2668, 0.2675, 0.2675,\n",
       "           0.2675, 0.2675, 0.2775, 0.2929, 0.2671, 0.2646, 0.2672, 0.2675, 0.2950,\n",
       "           0.2675, 0.2768, 0.2768, 0.2768, 0.2636, 0.2903, 0.2675, 0.2788, 0.2781,\n",
       "           0.2671, 0.2675, 0.2680, 0.2664, 0.2693, 0.2675, 0.2708, 0.2675, 0.2950,\n",
       "           0.2804, 0.2651, 0.2708, 0.2950, 0.2660, 0.2781, 0.2938, 0.2950, 0.2768,\n",
       "           0.2675, 0.2740, 0.2700, 0.2938, 0.2938, 0.3270, 0.3017, 0.5174, 0.3578,\n",
       "           0.4041, 0.5174, 0.3017, 0.3151, 0.3017, 0.3016, 0.3784, 0.5174, 0.5174,\n",
       "           0.4041, 0.3578, 0.3048, 0.3207, 0.3578, 0.4041, 0.4041, 0.3578, 0.3578,\n",
       "           0.4041, 0.3578, 0.3270, 0.3017, 0.4041, 0.3315, 0.3619, 0.3784, 0.5174,\n",
       "           0.3048, 0.3578, 0.3678, 0.3284, 0.3048, 0.4041, 0.3784, 0.3016, 0.3784,\n",
       "           0.3151, 0.3378, 0.3270, 0.3017, 0.3151, 0.3689, 0.4041, 0.3048, 0.3784,\n",
       "           0.3784, 0.5174, 0.3017, 0.3151, 0.3784, 0.3048, 0.3701, 0.3016, 0.2963,\n",
       "           0.3784, 0.3284, 0.3701, 0.5174, 0.3048, 0.4041, 0.3048, 0.3784, 0.5174,\n",
       "           0.3048, 0.3016, 0.3017, 0.3016, 0.3270, 0.3016, 0.3151, 0.5174, 0.3284,\n",
       "           0.3578, 0.3016, 0.3151, 0.3016, 0.4041, 0.3151, 0.3270, 0.3701, 0.3701,\n",
       "           0.3411, 0.5174, 0.3784, 0.3678, 0.3284, 0.3701, 0.3434, 0.3277, 0.3578,\n",
       "           0.3270, 0.3017, 0.3284, 0.3662, 0.3016, 0.3578, 0.3017, 0.3784, 0.3181,\n",
       "           0.5174, 0.5174, 0.3336, 0.3270, 0.3703, 0.4041, 0.3784, 0.3048, 0.3270,\n",
       "           0.4101, 0.3151, 0.3048, 0.4041, 0.3016, 0.3017, 0.3048, 0.3578, 0.3016,\n",
       "           0.3048, 0.3543, 0.3016, 0.3093, 0.3270, 0.5174, 0.3163, 0.3151, 0.3701,\n",
       "           0.3669, 0.3701, 0.3784, 0.3151, 0.3701, 0.3270, 0.3578, 0.3151, 0.3284,\n",
       "           0.3151, 0.3413, 0.5174, 0.3205, 0.3270, 0.4041, 0.3270, 0.3048, 0.3048,\n",
       "           0.3270, 0.3017, 0.3016, 0.3016, 0.3016, 0.3270, 0.3578, 0.3784, 0.3701,\n",
       "           0.3511, 0.4041, 0.3151, 0.3046, 0.2916, 0.3240, 0.3065, 0.3002, 0.3105,\n",
       "           0.3118, 0.3075, 0.3084, 0.2999, 0.2997, 0.2924, 0.3028, 0.2899, 0.3026,\n",
       "           0.3173, 0.3020, 0.3065, 0.3036, 0.3047, 0.2881, 0.2925, 0.3054, 0.3008,\n",
       "           0.3165, 0.2992, 0.3081, 0.2915, 0.2908, 0.3932, 0.3090, 0.2916, 0.2999,\n",
       "           0.2935, 0.7247, 0.3792, 0.4230, 0.3071, 0.3801, 0.3962, 0.3439, 0.3909,\n",
       "           0.3370, 0.3464, 0.2896, 0.3962, 0.3792, 0.2993, 0.3374, 0.4610, 0.3030,\n",
       "           0.2798, 0.3346, 0.4089, 0.3296, 0.2859, 0.4352, 0.3372, 0.2892, 0.2870,\n",
       "           0.2988, 0.3484, 0.3353, 0.3237, 0.3962, 0.3481, 0.3264, 0.2927, 0.3484,\n",
       "           0.4797, 0.3532, 0.3400, 0.3274, 0.3358, 0.3484, 0.3030, 0.3370, 0.3801,\n",
       "           0.3088, 0.3068, 0.2844, 0.3413, 0.3221, 0.3370, 0.3792, 0.2834, 0.2795,\n",
       "           0.3632, 0.3194, 0.2912, 0.3690, 0.4610, 0.3792, 0.3962, 0.2711, 0.3372,\n",
       "           0.3226, 0.3030, 0.3372, 0.3372, 0.3161, 0.3260, 0.3081, 0.3005, 0.2949,\n",
       "           0.7247, 0.4089, 0.3063, 0.2854, 0.2914, 0.3025, 0.2935, 0.4214, 0.2888,\n",
       "           0.7247, 0.3232, 0.3071, 0.3962, 0.2934, 0.3024, 0.3044, 0.3534, 0.3227,\n",
       "           0.2948, 0.2795, 0.3962, 0.3481, 0.7247, 0.3801, 0.2745, 0.2870, 0.3355,\n",
       "           0.3632, 0.3162, 0.3632, 0.3043]),\n",
       "   tensor([0.2497, 0.2415, 0.3682, 0.2836, 0.2791, 0.2361, 0.3716, 0.3175, 0.3010,\n",
       "           0.2096, 0.2970, 0.2365, 0.2639, 0.2309, 0.2737, 0.2746, 0.2380, 0.2812,\n",
       "           0.2721, 0.2829, 0.3682, 0.2055, 0.3347, 0.2970, 0.2167, 0.2839, 0.2673,\n",
       "           0.2757, 0.2891, 0.2076, 0.3682, 0.2042, 0.2970, 0.2364, 0.3224, 0.3352,\n",
       "           0.3506, 0.2034, 0.3682, 0.2625, 0.2055, 0.2970, 0.2046, 0.2519, 0.2141,\n",
       "           0.3347, 0.2343, 0.3246, 0.2970, 0.3190, 0.3246, 0.2519, 0.3347, 0.2921,\n",
       "           0.2207, 0.3347, 0.2055, 0.2545, 0.2164, 0.2970, 0.2335, 0.2929, 0.3170,\n",
       "           0.2582, 0.3175, 0.2611, 0.3352, 0.2519, 0.3190, 0.2891, 0.3682, 0.2569,\n",
       "           0.3506, 0.2359, 0.2098, 0.2248, 0.2507, 0.3306, 0.2564, 0.1938, 0.2974,\n",
       "           0.3506, 0.2708, 0.3347, 0.2268, 0.2726, 0.4354, 0.2359, 0.2970, 0.2191,\n",
       "           0.2314, 0.2767, 0.2455, 0.2285, 0.3506, 0.2363, 0.2837, 0.3175, 0.2169,\n",
       "           0.2369, 0.2640, 0.3347, 0.2370, 0.2151, 0.2457, 0.2417, 0.2887, 0.2191,\n",
       "           0.2970, 0.2055, 0.2012, 0.2506, 0.3246, 0.2977, 0.3347, 0.2469, 0.3682,\n",
       "           0.2896, 0.2631, 0.3682, 0.2055, 0.2970, 0.2507, 0.2769, 0.3066, 0.2538,\n",
       "           0.2526, 0.3090, 0.3506, 0.2965, 0.2030, 0.3352, 0.3682, 0.2166, 0.3347,\n",
       "           0.2055, 0.2293, 0.2176, 0.2039, 0.2346, 0.2443, 0.2908, 0.2201, 0.3347,\n",
       "           0.2507, 0.2917, 0.2507, 0.2656, 0.2061, 0.2348, 0.2121, 0.3347, 0.3347,\n",
       "           0.2099, 0.3091, 0.2373, 0.2969, 0.2970, 0.3079, 0.1956, 0.2478, 0.2642,\n",
       "           0.3306, 0.3306, 0.2859, 0.2535, 0.2090, 0.3306, 0.3306, 0.2080, 0.3347,\n",
       "           0.2390, 0.1946, 0.3682, 0.3506, 0.3682, 0.2698, 0.3032, 0.2884, 0.2801,\n",
       "           0.3305, 0.2929, 0.2670, 0.2960, 0.2907, 0.2720, 0.2879, 0.2707, 0.2789,\n",
       "           0.2910, 0.3190, 0.2626, 0.2814, 0.2919, 0.2952, 0.2949, 0.2642, 0.3009,\n",
       "           0.2827, 0.5224, 0.4978, 0.3306, 0.2919, 0.2642, 0.2970, 0.2970, 0.3306,\n",
       "           0.5668, 0.2919, 0.3009, 0.2642, 0.2949, 0.2949, 0.2952, 0.4176, 0.2970,\n",
       "           0.3306, 0.3009, 0.2970, 0.3009, 0.2642, 0.3306, 0.2949, 0.3009, 0.2944,\n",
       "           0.2944, 0.2952, 0.2642, 0.2949, 0.2919, 0.2944, 0.2642, 0.2970, 0.2919,\n",
       "           0.2919, 0.2919, 0.2949, 0.4812, 0.2642, 0.2944, 0.2944, 0.3246, 0.2932,\n",
       "           0.3009, 0.2970, 0.2919, 0.2949, 0.2642, 0.2970, 0.2970, 0.2642, 0.2970,\n",
       "           0.2952, 0.2919, 0.2970, 0.2949, 0.5395, 0.2970, 0.2952, 0.2919, 0.2642,\n",
       "           0.3306, 0.2944, 0.4376, 0.2642, 0.3246, 0.2881, 0.3306, 0.2642, 0.2970,\n",
       "           0.3306, 0.2952, 0.2858, 0.2949, 0.2919, 0.2949, 0.2944, 0.2970, 0.2952,\n",
       "           0.3009, 0.2952, 0.3246, 0.2944, 0.2949, 0.3246, 0.3009, 0.2952, 0.2949,\n",
       "           0.2944, 0.3306, 0.2642, 0.2877, 0.2949, 0.3009, 0.2944, 0.2970, 0.2970,\n",
       "           0.2970, 0.2528, 0.2449, 0.2582, 0.2317, 0.2532, 0.2433, 0.2319, 0.2529,\n",
       "           0.2308, 0.2347, 0.2322, 0.2351, 0.2350, 0.2460, 0.2372, 0.2441, 0.2352,\n",
       "           0.2316, 0.2324, 0.2302, 0.2334, 0.2316, 0.2359, 0.2337, 0.2310, 0.2343,\n",
       "           0.2686, 0.2222, 0.2513, 0.2195, 0.2228, 0.2217, 0.2813, 0.2181, 0.2524,\n",
       "           0.2297, 0.2451, 0.2631, 0.2256, 0.2597, 0.2192, 0.2250, 0.2642, 0.2570,\n",
       "           0.2184, 0.2489, 0.2198, 0.2217, 0.2205, 0.2190, 0.2780, 0.2151, 0.2229,\n",
       "           0.2575, 0.2248, 0.2241, 0.2541, 0.2335, 0.2164, 0.2190, 0.2235, 0.2172,\n",
       "           0.2597, 0.2196, 0.2196, 0.2217, 0.2274, 0.2184, 0.2123, 0.2317, 0.2214,\n",
       "           0.2239, 0.2642, 0.2220, 0.2236, 0.2596, 0.2203, 0.2631, 0.2607, 0.2294,\n",
       "           0.2143, 0.2268, 0.2214, 0.2199, 0.2206, 0.2646, 0.2306, 0.2228, 0.2171,\n",
       "           0.2215, 0.2299, 0.2441, 0.2352, 0.2217, 0.2553, 0.2212, 0.2363, 0.2626,\n",
       "           0.2191, 0.2252, 0.2156, 0.2262, 0.2146, 0.2230, 0.2186, 0.2220, 0.2426,\n",
       "           0.2137, 0.2213, 0.2346, 0.2137, 0.2405, 0.2731, 0.2231, 0.2238, 0.2262,\n",
       "           0.2139, 0.2221]),\n",
       "   tensor([0.3007, 0.2970, 0.2970, 0.5094, 0.5094, 0.3007, 0.3291, 0.3249, 0.3291,\n",
       "           0.3091, 0.3091, 0.3091, 0.2970, 0.3641, 0.3321, 0.3007, 0.3007, 0.2970,\n",
       "           0.3249, 0.3291, 0.5094, 0.2969, 0.3091, 0.3007, 0.3321, 0.3007, 0.3776,\n",
       "           0.3091, 0.3091, 0.3291, 0.2970, 0.3822, 0.2970, 0.3321, 0.2970, 0.3776,\n",
       "           0.5094, 0.3007, 0.2970, 0.3007, 0.2970, 0.3822, 0.3091, 0.3091, 0.3291,\n",
       "           0.3249, 0.2954, 0.5094, 0.3091, 0.3249, 0.3361, 0.2970, 0.2980, 0.2970,\n",
       "           0.5094, 0.2970, 0.2899, 0.3249, 0.3091, 0.3051, 0.3091, 0.3822, 0.3321,\n",
       "           0.3321, 0.3249, 0.5094, 0.3822, 0.3249, 0.2980, 0.2938, 0.3321, 0.3291,\n",
       "           0.3822, 0.2848, 0.3007, 0.5094, 0.2980, 0.3822, 0.3776, 0.3776, 0.2879,\n",
       "           0.3641, 0.5094, 0.2970, 0.3321, 0.3007, 0.3321, 0.3776, 0.3361, 0.3091,\n",
       "           0.3249, 0.3291, 0.2980, 0.3007, 0.3291, 0.3776, 0.2901, 0.3091, 0.3822,\n",
       "           0.3007, 0.3249, 0.3291, 0.2980, 0.3007, 0.2970, 0.3291, 0.3007, 0.3776,\n",
       "           0.3091, 0.2871, 0.2970, 0.2962, 0.2849, 0.3776, 0.3291, 0.2970, 0.2970,\n",
       "           0.2940, 0.3249, 0.3776, 0.3291, 0.2970, 0.3776, 0.3007, 0.3091, 0.3012,\n",
       "           0.3007, 0.3822, 0.2980, 0.3291, 0.3007, 0.3042, 0.2889, 0.3776, 0.3011,\n",
       "           0.2980, 0.3291, 0.3321, 0.2970, 0.3641, 0.2970, 0.3291, 0.2870, 0.3291,\n",
       "           0.2980, 0.2970, 0.2949, 0.2970, 0.3091, 0.3249, 0.3321, 0.2970, 0.3249,\n",
       "           0.3007, 0.3091, 0.3051, 0.3822, 0.2854, 0.2985, 0.3007, 0.3822, 0.2864,\n",
       "           0.3321, 0.3091, 0.3291, 0.3258, 0.3641, 0.3361, 0.5094, 0.3091, 0.3091,\n",
       "           0.3321, 0.3249, 0.2942, 0.3026, 0.3249, 0.2851, 0.3321, 0.3091, 0.2963,\n",
       "           0.2980, 0.2970, 0.3091, 0.3249, 0.5094, 0.2970, 0.3091, 0.3091, 0.3822,\n",
       "           0.4317, 0.3341, 0.4317, 0.3747, 0.3575, 0.3747, 0.4519, 0.3286, 0.3037,\n",
       "           0.5115, 0.4002, 0.3170, 0.4225, 0.3138, 0.5548, 0.3747, 0.5134, 0.4266,\n",
       "           0.4317, 0.4168, 0.4010, 0.3711, 0.4914, 0.4115, 0.3747, 0.3446, 0.5253,\n",
       "           0.3249, 0.3701, 0.3473, 0.3158, 0.3105, 0.4225, 0.2945, 0.4317, 0.3849,\n",
       "           0.4932, 0.3867, 0.3215, 0.3468, 0.6381, 0.4035, 0.3741, 0.4087, 0.4786,\n",
       "           0.2890, 0.4229, 0.3252, 0.3003, 0.4786, 0.2891, 0.5854, 0.3420, 0.3071,\n",
       "           0.4184, 0.4321, 0.3560, 0.3983, 0.2990, 0.3384, 0.3648, 0.3555, 0.3694,\n",
       "           0.3456, 0.4482, 0.3242, 0.3059, 0.3747, 0.4029, 0.4087, 0.3045, 0.3997,\n",
       "           0.5652, 0.2979, 0.3568, 0.3278, 0.3138, 0.4317, 0.3918, 0.4087, 0.4406,\n",
       "           0.3181, 0.3741, 0.4317, 0.4006, 0.4198, 0.2909, 0.3017, 0.4091, 0.3118,\n",
       "           0.4029, 0.3204, 0.6018, 0.4010, 0.4078, 0.4786, 0.3160, 0.4087, 0.4029,\n",
       "           0.3461, 0.3284, 0.3821, 0.5198, 0.3822, 0.3111, 0.9991, 0.3436, 0.3146,\n",
       "           0.3140, 0.4226, 0.3822, 0.2875, 0.3249, 0.7914, 0.3119, 0.3075, 0.3236,\n",
       "           0.3088, 0.5199, 0.3118, 0.3023, 0.2957, 0.3361, 0.5044, 0.2980, 0.3279,\n",
       "           0.3131, 0.3240, 0.3031, 0.3776, 0.3436, 0.3186, 0.5094, 0.2957, 0.3119,\n",
       "           0.3822, 0.3091, 0.5094, 0.3076, 0.3399, 0.3377, 0.4481, 0.3282, 0.2866,\n",
       "           0.3553, 0.3269, 0.3496, 0.3116, 0.3030, 0.2881, 0.3140, 0.3502, 0.3291,\n",
       "           0.3269, 0.3313, 0.5094, 0.3390, 0.3262, 0.3822, 0.9991, 0.3822, 0.5094,\n",
       "           0.3666, 0.2979, 0.3428, 0.3401, 0.3015, 0.3091, 0.9991, 0.2964, 0.3517,\n",
       "           0.3552, 0.3003, 0.5094, 0.3748, 0.3331, 0.3776, 0.3822, 0.3822, 0.4396,\n",
       "           0.4931, 0.9991, 0.9991, 0.3822, 0.3157, 0.3091, 0.3002, 0.3147, 0.9991,\n",
       "           0.3158, 0.4034, 0.2995, 0.3338, 0.3259, 0.9991, 0.3139, 0.3436, 0.3336,\n",
       "           0.3061, 0.5199, 0.2968, 0.3822, 0.3080, 0.3278, 0.3178, 0.5199, 0.3259,\n",
       "           0.3036, 0.3049, 0.3522, 0.3053, 0.3149, 0.4395, 0.3361, 0.3194, 0.3436,\n",
       "           0.3747, 0.3075, 0.3045, 0.5638, 0.3499, 0.3075, 0.3070, 0.3091, 0.3123,\n",
       "           0.3091, 0.3013, 0.3730, 0.4619, 0.3095, 0.3091, 0.3256, 0.3054, 0.3291,\n",
       "           0.4169, 0.3348, 0.3224, 0.2935, 0.3348, 0.2944, 0.3156, 0.3091, 0.3255,\n",
       "           0.2950, 0.2945, 0.3178, 0.2934, 0.3291, 0.2983, 0.9991, 0.9991, 0.3199,\n",
       "           0.3331, 1.1425, 0.3119, 0.3085, 0.3245, 0.3309, 0.3361, 0.3152, 0.3222,\n",
       "           0.3091, 0.3211, 0.2955, 0.3004, 0.3361, 0.3395, 0.3641, 0.4050, 0.3023,\n",
       "           0.3555, 0.3147, 0.3147, 0.3147, 0.2980, 0.3005, 0.3147, 0.2958, 0.2999,\n",
       "           0.2983, 0.3002, 0.3133, 0.2958, 0.3134, 0.3147, 0.3005, 0.3128, 0.2993,\n",
       "           0.3147, 0.3008, 0.3147, 0.3147, 0.2965, 0.3124, 0.3192, 0.3180, 0.3212,\n",
       "           0.3404, 0.3148, 0.3150, 0.3170, 0.3277, 0.3301, 0.3177, 0.3253, 0.3564])]}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "class SurfaceBatchNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features=1, \n",
    "        momentum=0.1\n",
    "    ):\n",
    "        super(SurfaceBatchNorm, self).__init__()\n",
    "        self.log_moneyness_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.time_to_maturity_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_return_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.market_volatility_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "        self.treasury_rate_bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Concatenate all tensors from the Input Surface into one tensor for each feature\n",
    "        input_surface_log_moneyness = torch.cat([x for x in batch['Input Surface']['Log Moneyness']])\n",
    "        input_surface_time_to_maturity = torch.cat([x for x in batch['Input Surface']['Time to Maturity']])\n",
    "\n",
    "        # Concatenate Input Surface tensors with Query Points tensors\n",
    "        total_log_moneyness = torch.cat([input_surface_log_moneyness] + [x for x in batch['Query Points']['Log Moneyness']])\n",
    "        total_time_to_maturity = torch.cat([input_surface_time_to_maturity] + [x for x in batch['Query Points']['Time to Maturity']])\n",
    "\n",
    "        # Normalize Log Moneyness and Time to Maturity\n",
    "        norm_log_moneyness = self.log_moneyness_bn(total_log_moneyness.unsqueeze(1)).squeeze(1)\n",
    "        norm_time_to_maturity = self.time_to_maturity_bn(total_time_to_maturity.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Split the normalized results back to corresponding structures\n",
    "        input_surface_sizes = [len(x) for x in batch['Input Surface']['Log Moneyness']]\n",
    "        query_points_sizes = [len(x) for x in batch['Query Points']['Log Moneyness']]\n",
    "        total_input_size = sum(input_surface_sizes)\n",
    "\n",
    "        # Normalizing Market Features\n",
    "        market_features = batch['Market Features']\n",
    "        norm_market_return = self.market_return_bn(market_features['Market Return'].unsqueeze(1)).squeeze(1)\n",
    "        norm_market_volatility = self.market_volatility_bn(market_features['Market Volatility'].unsqueeze(1)).squeeze(1)\n",
    "        norm_treasury_rate = self.treasury_rate_bn(market_features['Treasury Rate'].unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Reconstructing the batch with normalized data\n",
    "        output = {\n",
    "            'Datetime': batch['Datetime'],\n",
    "            'Symbol': batch['Symbol'],\n",
    "            'Market Features': {\n",
    "                'Market Return': norm_market_return,\n",
    "                'Market Volatility': norm_market_volatility,\n",
    "                'Treasury Rate': norm_treasury_rate\n",
    "            },\n",
    "            'Input Surface': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[:total_input_size], input_surface_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[:total_input_size], input_surface_sizes)),\n",
    "                'Implied Volatility': batch['Input Surface']['Implied Volatility']\n",
    "            },\n",
    "            'Query Points': {\n",
    "                'Log Moneyness': list(torch.split(norm_log_moneyness[total_input_size:], query_points_sizes)),\n",
    "                'Time to Maturity': list(torch.split(norm_time_to_maturity[total_input_size:], query_points_sizes)),\n",
    "                'Implied Volatility': batch['Query Points']['Implied Volatility']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Ensure requires_grad is True for query point values\n",
    "        for key in output['Query Points']:\n",
    "            if key != 'Implied Volatility':  # We only set requires_grad for Log Moneyness and Time to Maturity\n",
    "                for tensor in output['Query Points'][key]:\n",
    "                    tensor.requires_grad_()\n",
    "\n",
    "        return output\n",
    "\n",
    "# Usage\n",
    "surfacebatchnorm = SurfaceBatchNorm()\n",
    "processed_batch = surfacebatchnorm(batch)\n",
    "processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input Surface': [tensor([[ 0.2954,  2.0830, -0.5693,  ...,  0.6434, -0.9860,  0.2933],\n",
       "          [ 0.5820,  2.0090, -0.7119,  ...,  0.6715, -0.9329,  0.3644],\n",
       "          [ 0.8457,  1.9012, -0.8559,  ...,  0.7113, -0.8666,  0.4503],\n",
       "          ...,\n",
       "          [ 2.7806,  0.6177,  1.5776,  ...,  0.3637, -0.9704,  0.8234],\n",
       "          [ 2.7725,  0.5866,  1.5793,  ...,  0.3655, -0.9779,  0.8089],\n",
       "          [ 2.7725,  0.5866,  1.5793,  ...,  0.3655, -0.9779,  0.8089]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[ 1.3062,  1.5115, -1.1618,  ...,  0.7953, -0.7169,  0.6365],\n",
       "          [ 1.3062,  1.5115, -1.1618,  ...,  0.7953, -0.7169,  0.6365],\n",
       "          [ 1.3474,  1.5246, -1.1666,  ...,  0.8006, -0.7135,  0.6377],\n",
       "          ...,\n",
       "          [ 1.2324, -0.5439, -0.4405,  ...,  0.8758,  0.6794,  3.5103],\n",
       "          [ 1.2221, -0.5732, -0.4372,  ...,  0.8731,  0.6769,  3.5101],\n",
       "          [ 1.2221, -0.5732, -0.4372,  ...,  0.8731,  0.6769,  3.5101]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[ 0.3913,  1.8861, -0.6716,  ...,  0.6385, -0.9398,  0.3712],\n",
       "          [ 0.4226,  1.8968, -0.6847,  ...,  0.6412, -0.9348,  0.3779],\n",
       "          [ 0.4560,  1.9053, -0.6986,  ...,  0.6440, -0.9295,  0.3850],\n",
       "          ...,\n",
       "          [-0.0979,  0.1727,  0.0526,  ...,  1.8188,  1.1613,  3.2020],\n",
       "          [-0.0445,  0.1425,  0.0503,  ...,  1.8105,  1.1601,  3.2100],\n",
       "          [-0.0185,  0.1272,  0.0492,  ...,  1.8063,  1.1594,  3.2140]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[ 1.1002,  1.7467, -0.6764,  ...,  0.7350, -1.0008,  0.1581],\n",
       "          [ 1.1002,  1.7467, -0.6764,  ...,  0.7350, -1.0008,  0.1581],\n",
       "          [ 1.1307,  1.7884, -0.6677,  ...,  0.7300, -1.0019,  0.1613],\n",
       "          ...,\n",
       "          [ 0.6806, -1.0847, -0.3904,  ...,  1.3295,  0.9814,  3.5876],\n",
       "          [ 0.6712, -1.1160, -0.3898,  ...,  1.3273,  0.9791,  3.5889],\n",
       "          [ 0.6712, -1.1160, -0.3898,  ...,  1.3273,  0.9791,  3.5889]],\n",
       "         grad_fn=<AddBackward0>)],\n",
       " 'Query Points': [tensor([[ 0.3628, -0.7261, -1.1622,  ...,  1.5613,  0.7390,  2.9457],\n",
       "          [ 0.2811,  0.2374, -1.9163,  ...,  1.9179,  0.9791,  2.8471],\n",
       "          [ 2.2110,  0.5259, -1.7528,  ...,  1.4318,  0.5770,  2.5588],\n",
       "          ...,\n",
       "          [ 0.9030,  0.4426, -0.6452,  ...,  1.3521,  0.6789,  2.9423],\n",
       "          [ 0.9727,  0.5818, -0.8049,  ...,  1.4163,  0.7364,  2.9531],\n",
       "          [ 2.7053,  0.7373,  0.2065,  ...,  0.8003,  0.0166,  2.2001]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[-2.1859,  0.7395, -0.3093,  ...,  2.0384,  0.6405,  2.0523],\n",
       "          [ 1.8877,  2.1180, -0.7651,  ...,  0.8453, -0.8280,  0.3536],\n",
       "          [ 1.5853,  1.7780, -1.0330,  ...,  0.8081, -0.7846,  0.4869],\n",
       "          ...,\n",
       "          [-3.0891,  0.3138, -0.0144,  ...,  1.9991,  0.5966,  1.9889],\n",
       "          [ 0.0909,  2.1668, -0.0764,  ...,  0.6320, -0.9388,  0.3606],\n",
       "          [-2.7189, -0.1926,  0.1882,  ...,  2.1856,  0.9948,  2.5518]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[-2.6095,  0.7538, -0.8107,  ...,  2.0269,  0.6968,  2.1784],\n",
       "          [-2.0862,  1.0376, -0.9952,  ...,  1.8057,  0.4313,  1.8765],\n",
       "          [ 0.7638,  1.9090, -0.8261,  ...,  0.6762, -0.8760,  0.4540],\n",
       "          ...,\n",
       "          [-2.0091,  0.0996, -0.0654,  ...,  2.1989,  0.9467,  2.4689],\n",
       "          [-1.3093,  0.1194, -0.8272,  ...,  2.1935,  0.9736,  2.5359],\n",
       "          [-2.3380,  0.4406, -0.5082,  ...,  2.1261,  0.8276,  2.3280]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[ 0.3350, -1.4008,  0.1436,  ...,  2.1288,  1.1828,  2.9606],\n",
       "          [-0.3864, -1.0711, -0.6797,  ...,  2.1641,  0.9616,  2.5430],\n",
       "          [-0.3876, -1.3149, -0.7108,  ...,  2.1603,  0.9842,  2.5899],\n",
       "          ...,\n",
       "          [-0.9553,  0.6561, -0.4487,  ...,  1.5489,  1.1099,  3.4450],\n",
       "          [-1.6412,  0.4452, -0.1956,  ...,  1.3030,  0.9658,  3.4862],\n",
       "          [-1.1524,  1.6515,  1.1188,  ...,  0.0862, -0.4596,  2.1158]],\n",
       "         grad_fn=<AddBackward0>)]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class EllipticalRBFKernel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        bandwidth, \n",
    "        remove_kernel=False\n",
    "    ):\n",
    "        super(EllipticalRBFKernel, self).__init__()\n",
    "        self.bandwidth = bandwidth\n",
    "        # Initialize the log of the scale vector to zero, which corresponds to scale factors of one\n",
    "        self.log_scale = nn.Parameter(torch.zeros(input_dim))\n",
    "        self.remove_kernel = remove_kernel\n",
    "\n",
    "    def forward(self, distances):\n",
    "        if self.remove_kernel:\n",
    "            # Create a mask for the condition check\n",
    "            all_zeros = torch.all(distances==0.0, dim=-1)\n",
    "            result = torch.where(\n",
    "                all_zeros, \n",
    "                torch.full(distances.shape[:-1], 1.0, device=distances.device),\n",
    "                torch.full(distances.shape[:-1], 1e-10, device=distances.device)\n",
    "            )\n",
    "            return result\n",
    "        # Convert log scale to actual scale values\n",
    "        scale = torch.exp(self.log_scale)\n",
    "        \n",
    "        # Calculate the scaled distances\n",
    "        scaled_distances = (distances ** 2) * scale  # Element-wise multiplication by scale\n",
    "\n",
    "        # Normalize by the trace of the scale matrix\n",
    "        trace_scale_matrix = torch.sum(scale)\n",
    "        normalized_distances = torch.sum(scaled_distances, dim=-1) / trace_scale_matrix\n",
    "\n",
    "        # Compute the RBF kernel output using the normalized distances\n",
    "        kernel_values = torch.exp(-normalized_distances / (2 * self.bandwidth ** 2))\n",
    "\n",
    "        return kernel_values\n",
    "\n",
    "class SurfaceContinuousKernelPositionalEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceContinuousKernelPositionalEmbedding, self).__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "        self.remove_positional_embedding = remove_positional_embedding\n",
    "\n",
    "        # Initialize multiple RBF kernels, each with a different fixed bandwidth\n",
    "        self.kernels = nn.ModuleList()\n",
    "        for i in range(1, d_embedding + 1):\n",
    "            bandwidth_value = torch.erfinv(torch.tensor(i / (d_embedding + 1))) * np.sqrt(2)\n",
    "            self.kernels.append(\n",
    "                EllipticalRBFKernel(\n",
    "                    bandwidth=bandwidth_value, \n",
    "                    input_dim=2, \n",
    "                    remove_kernel=remove_kernel\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.input_surface_layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.query_points_layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "        # Initialize learnable scaling parameter (the base for positional embedding)\n",
    "        self.log_scale = nn.Parameter(torch.log(torch.tensor(10000.0)))\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_surface_batch, \n",
    "        query_points_batch\n",
    "    ):\n",
    "        batch_size = len(input_surface_batch['Log Moneyness'])\n",
    "\n",
    "        input_surface_embeddings = []\n",
    "        query_points_embeddings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Extract the coordinates and implied volatilities for each surface in the batch\n",
    "            surface_coords = torch.stack([\n",
    "                input_surface_batch['Log Moneyness'][i], \n",
    "                input_surface_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "            surface_ivs = input_surface_batch['Implied Volatility'][i]\n",
    "\n",
    "            query_coords = torch.stack([\n",
    "                query_points_batch['Log Moneyness'][i], \n",
    "                query_points_batch['Time to Maturity'][i]\n",
    "            ], dim=-1)\n",
    "\n",
    "            all_coords = torch.cat((surface_coords, query_coords), dim=0)\n",
    "\n",
    "            # Compute the pairwise differences between all points and the input surface points\n",
    "            point_differences = all_coords.unsqueeze(1) - surface_coords.unsqueeze(0)  # (n+m, n, 2)\n",
    "\n",
    "            # Initialize the output embeddings for the current surface with d_embedding channels\n",
    "            all_embedded = torch.zeros((all_coords.shape[0], self.d_embedding), dtype=torch.float32, device=surface_coords.device)\n",
    "\n",
    "            for kernel_idx, kernel in enumerate(self.kernels):\n",
    "                # Apply the RBF kernel to each distance vector \n",
    "                kernel_outputs = kernel(point_differences)\n",
    "\n",
    "                # Compute the weighted sum of IVs based on the kernel outputs\n",
    "                weighted_sum = (kernel_outputs * surface_ivs.unsqueeze(0)).sum(dim=1)\n",
    "                normalization_factor = kernel_outputs.sum(dim=1)\n",
    "\n",
    "                all_embedded[:, kernel_idx] = weighted_sum / normalization_factor    \n",
    "\n",
    "            # Split the embeddings into input surface and query points embeddings\n",
    "            input_surface_embedded = all_embedded[:surface_coords.shape[0], :]\n",
    "            query_points_embedded = all_embedded[surface_coords.shape[0]:, :]\n",
    "\n",
    "            # Normalize the embedded surfaces\n",
    "            input_surface_embedded = self.input_surface_layer_norm(input_surface_embedded)\n",
    "            query_points_embedded = self.query_points_layer_norm(query_points_embedded)\n",
    "\n",
    "            # Positional embedding for input surface points\n",
    "            input_surface_pe = self._compute_positional_embedding(surface_coords)\n",
    "\n",
    "            # Positional embedding for query points\n",
    "            query_points_pe = self._compute_positional_embedding(query_coords)\n",
    "\n",
    "            # Add positional embeddings with a factor of sqrt(2)\n",
    "            input_surface_final = input_surface_embedded + input_surface_pe * np.sqrt(2)\n",
    "            query_points_final = query_points_embedded + query_points_pe * np.sqrt(2)\n",
    "\n",
    "            # Append the encoded surface for this input surface to the batch list\n",
    "            input_surface_embeddings.append(input_surface_final)\n",
    "            query_points_embeddings.append(query_points_final)\n",
    "            # Explicitly delete temporary variables and free up memory\n",
    "            del surface_coords, surface_ivs, query_coords, all_coords, point_differences, all_embedded, input_surface_embedded, query_points_embedded, input_surface_pe, query_points_pe, input_surface_final, query_points_final\n",
    "            gc.collect()\n",
    "\n",
    "        # Keep all encoded surfaces as lists to handle variable lengths\n",
    "        return {\n",
    "            'Input Surface': input_surface_embeddings,\n",
    "            'Query Points': query_points_embeddings\n",
    "        }\n",
    "\n",
    "    def _compute_positional_embedding(\n",
    "        self, \n",
    "        coords, \n",
    "    ):\n",
    "        positional_embedding = torch.zeros(coords.size(0), self.d_embedding, device=coords.device)\n",
    "\n",
    "        if not self.remove_positional_embedding:\n",
    "            for i in range(self.d_embedding // 4):\n",
    "                div_factor = torch.exp(self.log_scale) ** (4 * i / self.d_embedding)\n",
    "                positional_embedding[:, 4 * i] = torch.sin(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 1] = torch.cos(coords[:, 0] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 2] = torch.sin(coords[:, 1] / div_factor)\n",
    "                positional_embedding[:, 4 * i + 3] = torch.cos(coords[:, 1] / div_factor)\n",
    "\n",
    "        return positional_embedding\n",
    "\n",
    "# Example of initializing and using this module\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "\n",
    "continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "kernel_positional_embedded_batch = continuous_kernel_positional_embedding(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "kernel_positional_embedded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before initialization] Memory Usage: 10765.21 MB\n",
      "[After initialization] Memory Usage: 10765.21 MB\n",
      "[After model run] Memory Usage: 13460.41 MB\n",
      "[After cleanup] Memory Usage: 10765.23 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "def print_memory_usage(stage):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"[{stage}] Memory Usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "def initialize_model(d_embedding):\n",
    "    model = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "    return model\n",
    "\n",
    "def run_model(model, processed_batch):\n",
    "    return model(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "\n",
    "print_memory_usage(\"Before initialization\")\n",
    "model = initialize_model(d_embedding)\n",
    "print_memory_usage(\"After initialization\")\n",
    "\n",
    "kernel_positional_embedded_batch = run_model(model, processed_batch)\n",
    "print_memory_usage(\"After model run\")\n",
    "\n",
    "# Clear references and collect garbage\n",
    "del model, kernel_positional_embedded_batch\n",
    "gc.collect()\n",
    "print_memory_usage(\"After cleanup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS: 4176.30 MB\n",
      "RSS: 4176.30 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"RSS: {mem_info.rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# Cleanup to free up RAM\n",
    "del continuous_kernel_positional_embedding\n",
    "del kernel_positional_embedded_batch\n",
    "del processed_batch\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n",
      "Memory reserved: 0\n",
      "Max memory reserved: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_model(processed_batch, HYPERPARAMETERS):\n",
    "    d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']\n",
    "    continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "    kernel_positional_embedded_batch = continuous_kernel_positional_embedding(\n",
    "        processed_batch['Input Surface'], processed_batch['Query Points']\n",
    "    )\n",
    "    return kernel_positional_embedded_batch\n",
    "\n",
    "import torch\n",
    "\n",
    "def hook(module, input, output):\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated()}\")\n",
    "    print(f\"Max memory allocated: {torch.cuda.max_memory_allocated()}\")\n",
    "    print(f\"Memory reserved: {torch.cuda.memory_reserved()}\")\n",
    "    print(f\"Max memory reserved: {torch.cuda.max_memory_reserved()}\")\n",
    "\n",
    "continuous_kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding=d_embedding)\n",
    "\n",
    "# Register the hook to your model layers\n",
    "for name, module in continuous_kernel_positional_embedding.named_modules():\n",
    "    module.register_forward_hook(hook)\n",
    "    \n",
    "kernel_positional_embedded_batch = continuous_kernel_positional_embedding(processed_batch['Input Surface'], processed_batch['Query Points'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SurfaceEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        momentum=0.1,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False\n",
    "    ):\n",
    "        super(SurfaceEmbedding, self).__init__()\n",
    "        self.batch_norm = SurfaceBatchNorm(num_features=1, momentum=momentum)\n",
    "        self.kernel_positional_embedding = SurfaceContinuousKernelPositionalEmbedding(d_embedding, remove_kernel, remove_positional_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "        self.mask_token = nn.Parameter(torch.randn(d_embedding))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Apply batch normalization\n",
    "        norm_batch = self.batch_norm(batch)\n",
    "\n",
    "        # Extract market features from processed batch and create external_features_batch tensor\n",
    "        market_features = norm_batch['Market Features']\n",
    "        external_features_batch = torch.stack([\n",
    "            market_features['Market Return'],\n",
    "            market_features['Market Volatility'],\n",
    "            market_features['Treasury Rate']\n",
    "        ], dim=-1)  # (batch, features)\n",
    "\n",
    "        # Compute kernel and positional embeddings\n",
    "        embeddings = self.kernel_positional_embedding(norm_batch['Input Surface'], norm_batch['Query Points'])\n",
    "\n",
    "        input_surface_embeddings = embeddings['Input Surface']\n",
    "        query_points_embeddings = embeddings['Query Points']\n",
    "\n",
    "        embedded_sequences = []\n",
    "\n",
    "        for input_surface_embedding, query_points_embedding in zip(input_surface_embeddings, query_points_embeddings):\n",
    "            # Add mask token to the query point embeddings\n",
    "            masked_query_points_embedding = query_points_embedding + self.mask_token\n",
    "\n",
    "            # Combine input surface embeddings and masked query points embeddings\n",
    "            combined_sequence = torch.cat((input_surface_embedding, masked_query_points_embedding), dim=0)\n",
    "\n",
    "            # Apply layer normalization\n",
    "            combined_sequence = self.layer_norm(combined_sequence)\n",
    "\n",
    "            embedded_sequences.append(combined_sequence)\n",
    "\n",
    "        return embedded_sequences, external_features_batch\n",
    "\n",
    "\n",
    "# # Example of initializing and using this module\n",
    "# d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "# surface_embedding = SurfaceEmbedding(d_embedding=d_embedding)\n",
    "# embedded_sequences_batch, external_features_batch = surface_embedding(batch)\n",
    "# embedded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualNorm(nn.Module):\n",
    "    def __init__(self, d_embedding):\n",
    "        super(ResidualNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x, \n",
    "        sublayer_output\n",
    "    ):\n",
    "        return self.norm(x + sublayer_output)\n",
    "    \n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding,\n",
    "        gate_dropout,\n",
    "        weight_initializer_std=0.02,\n",
    "        bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(GatedAttentionFusion, self).__init__()\n",
    "        self.gate_layer = nn.Sequential(\n",
    "            nn.Linear(d_embedding * 2, d_embedding),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(gate_dropout)\n",
    "        )\n",
    "        self.remove_external_attention = remove_external_attention\n",
    "        self.remove_gate = remove_gate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for module in self.gate_layer:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        self_attention_output, \n",
    "        external_attention_output\n",
    "    ):\n",
    "        if self.remove_external_attention:\n",
    "\n",
    "            return self_attention_output\n",
    "\n",
    "        if self.remove_gate:  \n",
    "\n",
    "            return self_attention_output + external_attention_output\n",
    "        # Concatenate self-attention and external attention outputs\n",
    "        concatenated_output = torch.cat((self_attention_output, external_attention_output), dim=-1)\n",
    "        # Compute gate values\n",
    "        gate_values = self.gate_layer(concatenated_output)\n",
    "        # Calculate gated embedding\n",
    "        gated_embedding = gate_values * self_attention_output + (1 - gate_values) * external_attention_output\n",
    "\n",
    "        return gated_embedding\n",
    "    \n",
    "    \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        ffn_hidden_dim, \n",
    "        ffn_dropout, \n",
    "        layer_depth, \n",
    "        weight_initializer_std=0.02, \n",
    "        bias_initializer_value=0,\n",
    "    ):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_embedding, ffn_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ffn_dropout),\n",
    "            nn.Linear(ffn_hidden_dim, d_embedding),\n",
    "            nn.Dropout(ffn_dropout)\n",
    "        )\n",
    "\n",
    "        self.layer_depth = layer_depth\n",
    "        self._initialize_weights(weight_initializer_std, bias_initializer_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feedforward(x)\n",
    "    \n",
    "    def _initialize_weights(\n",
    "        self, \n",
    "        std, \n",
    "        bias_value\n",
    "    ):\n",
    "        for i, module in enumerate(self.feedforward):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                nn.init.constant_(module.bias, bias_value)\n",
    "                \n",
    "                # Rescale the output matrices of the last linear projection\n",
    "                if i == len(self.feedforward) - 2:\n",
    "                    scale_factor = 1 / (2 * self.layer_depth) ** 0.5\n",
    "                    module.weight.data *= scale_factor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        n_heads, \n",
    "        ffn_hidden_dim, \n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        layer_depth,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.external_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_embedding, \n",
    "            num_heads=n_heads, \n",
    "            kdim=external_dim, \n",
    "            vdim=external_dim, \n",
    "            dropout=attention_dropout\n",
    "        )\n",
    "        self.gated_attention_fusion = GatedAttentionFusion(\n",
    "            d_embedding, \n",
    "            gate_dropout,\n",
    "            weight_initializer_std,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention, \n",
    "            remove_gate,\n",
    "        )\n",
    "        self.residual_norm1 = ResidualNorm(d_embedding)\n",
    "        self.feed_forward = FeedForwardNetwork(\n",
    "            d_embedding, \n",
    "            ffn_hidden_dim, \n",
    "            ffn_dropout, \n",
    "            layer_depth, \n",
    "            weight_initializer_std, \n",
    "            linear_bias_initializer_value\n",
    "        )\n",
    "        self.residual_norm2 = ResidualNorm(d_embedding)\n",
    "        # Initialize self-attention\n",
    "        self._initialize_attention_weights(self.self_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "        # Initialize external-attention\n",
    "        self._initialize_attention_weights(self.external_attention, weight_initializer_std, linear_bias_initializer_value, layer_depth)\n",
    "\n",
    "    def _initialize_attention_weights(\n",
    "        self, \n",
    "        attention_module, \n",
    "        weight_initializer_std, \n",
    "        linear_bias_initializer_value, \n",
    "        layer_depth\n",
    "    ):\n",
    "        if attention_module._qkv_same_embed_dim:\n",
    "            nn.init.normal_(attention_module.in_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "        else:\n",
    "            nn.init.normal_(attention_module.q_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.k_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "            nn.init.normal_(attention_module.v_proj_weight, mean=0.0, std=weight_initializer_std)\n",
    "\n",
    "        if attention_module.in_proj_bias is not None:\n",
    "            nn.init.constant_(attention_module.in_proj_bias, linear_bias_initializer_value)\n",
    "            nn.init.constant_(attention_module.out_proj.bias, linear_bias_initializer_value)\n",
    "        \n",
    "        if attention_module.bias_k is not None:\n",
    "            nn.init.constant_(attention_module.bias_k, linear_bias_initializer_value)\n",
    "        if attention_module.bias_v is not None:\n",
    "            nn.init.constant_(attention_module.bias_v, linear_bias_initializer_value)\n",
    "        \n",
    "        # Transformer layer rescaling for output weights\n",
    "        scale_factor = 1 / (2 * layer_depth) ** 0.5\n",
    "        nn.init.normal_(attention_module.out_proj.weight, mean=0.0, std=weight_initializer_std * scale_factor)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        surface_embeddings, \n",
    "        external_features,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Self-Attention\n",
    "        self_attention_output, self_attention_weights = self.self_attention(surface_embeddings, surface_embeddings, surface_embeddings)\n",
    "        # External Attention\n",
    "        external_attention_output, external_attention_weights = self.external_attention(surface_embeddings, external_features, external_features) \n",
    "        # Gated Attention Fusion\n",
    "        gated_embedding = self.gated_attention_fusion(self_attention_output, external_attention_output)\n",
    "        # Residual Connection and Layer Normalization\n",
    "        surface_embeddings = self.residual_norm1(surface_embeddings, gated_embedding)\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.feed_forward(surface_embeddings)\n",
    "        # Final Residual Connection and Layer Normalization\n",
    "        surface_embeddings = self.residual_norm2(surface_embeddings, ffn_output)\n",
    "\n",
    "        if output_attention_map:\n",
    "            # Remove the batch dimension for attention weights\n",
    "            return surface_embeddings, self_attention_weights.squeeze(0), external_attention_weights.squeeze(0)\n",
    "        \n",
    "        return surface_embeddings, None, None\n",
    "\n",
    "class SurfaceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(SurfaceEncoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            Encoder(\n",
    "                d_embedding, \n",
    "                n_heads, \n",
    "                ffn_hidden_dim, \n",
    "                attention_dropout, \n",
    "                gate_dropout,\n",
    "                ffn_dropout,\n",
    "                external_dim,\n",
    "                (i + 1),\n",
    "                weight_initializer_std,\n",
    "                linear_bias_initializer_value,\n",
    "                gate_bias_initializer_value,\n",
    "                remove_external_attention,\n",
    "                remove_gate\n",
    "            )\n",
    "            for i in range(num_encoder_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        embedded_sequences_batch, \n",
    "        external_features_batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        batch_size = len(embedded_sequences_batch)\n",
    "        encoded_sequences_batch = []\n",
    "        self_attention_maps = []\n",
    "        external_attention_maps = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            surface_embeddings = embedded_sequences_batch[i].unsqueeze(1) \n",
    "            external_features = external_features_batch[i].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            for j, encoder in enumerate(self.encoders):\n",
    "                if j == len(self.encoders) - 1 and output_attention_map:\n",
    "                    surface_embeddings, self_attention_map, external_attention_map = encoder(surface_embeddings, external_features, output_attention_map)\n",
    "                    \n",
    "                else:\n",
    "                    surface_embeddings, _, _ = encoder(surface_embeddings, external_features)\n",
    "                \n",
    "            encoded_sequences_batch.append(surface_embeddings.squeeze(1))\n",
    "            if output_attention_map:\n",
    "                self_attention_maps.append(self_attention_map)\n",
    "                external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return encoded_sequences_batch, self_attention_maps, external_attention_maps\n",
    "        \n",
    "        return encoded_sequences_batch, None, None    \n",
    "\n",
    "# Example of initializing and using these modules\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "external_dim = 3\n",
    "\n",
    "surface_encoder = SurfaceEncoder(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim, \n",
    "    attention_dropout, \n",
    "    gate_dropout, \n",
    "    ffn_dropout, \n",
    "    external_dim, \n",
    ")\n",
    "\n",
    "# Assume embedded_sequences_batch is the output of the SurfaceEmbedding module and\n",
    "# external_features is the formatted external market features batch\n",
    "# encoded_sequences_batch, self_attention_map_batch, external_attention_map_batch = surface_encoder(embedded_sequences_batch, external_features_batch)\n",
    "# encoded_sequences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IvySPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0189, -0.0159, -0.0154, -0.0165, -0.0186, -0.0154, -0.0192, -0.0148,\n",
       "         -0.0143, -0.0164, -0.0178, -0.0158, -0.0174, -0.0187, -0.0158, -0.0158,\n",
       "         -0.0158, -0.0169, -0.0153, -0.0159, -0.0156, -0.0168, -0.0172, -0.0162,\n",
       "         -0.0159, -0.0165, -0.0148, -0.0171, -0.0172, -0.0166, -0.0172, -0.0154,\n",
       "         -0.0168, -0.0162, -0.0197, -0.0184, -0.0154, -0.0190, -0.0148, -0.0153,\n",
       "         -0.0169, -0.0160, -0.0161, -0.0158, -0.0160, -0.0160, -0.0149, -0.0157,\n",
       "         -0.0156, -0.0165, -0.0186, -0.0181, -0.0173, -0.0153, -0.0156, -0.0169,\n",
       "         -0.0144, -0.0159, -0.0160, -0.0154, -0.0164, -0.0176, -0.0154, -0.0174,\n",
       "         -0.0163, -0.0155, -0.0157, -0.0158, -0.0158, -0.0169, -0.0160, -0.0167,\n",
       "         -0.0173, -0.0158, -0.0154, -0.0171, -0.0164, -0.0168, -0.0156, -0.0147,\n",
       "         -0.0160, -0.0156, -0.0162, -0.0162, -0.0172, -0.0154, -0.0153, -0.0152,\n",
       "         -0.0191, -0.0173, -0.0158, -0.0173, -0.0158, -0.0159, -0.0159, -0.0178,\n",
       "         -0.0159, -0.0172, -0.0173, -0.0147, -0.0157, -0.0158, -0.0159, -0.0160,\n",
       "         -0.0158, -0.0171, -0.0163, -0.0192, -0.0149, -0.0157, -0.0160, -0.0166,\n",
       "         -0.0164, -0.0170, -0.0160, -0.0166, -0.0148, -0.0156, -0.0175, -0.0188,\n",
       "         -0.0155, -0.0186, -0.0166, -0.0149, -0.0185, -0.0163, -0.0169, -0.0169,\n",
       "         -0.0155, -0.0156, -0.0172, -0.0172, -0.0168, -0.0152, -0.0158, -0.0143,\n",
       "         -0.0171, -0.0161, -0.0153, -0.0157, -0.0154, -0.0162, -0.0195, -0.0160,\n",
       "         -0.0178, -0.0148, -0.0160, -0.0172, -0.0164, -0.0164, -0.0158, -0.0166,\n",
       "         -0.0159, -0.0188, -0.0173, -0.0155, -0.0170, -0.0169, -0.0163, -0.0176,\n",
       "         -0.0155, -0.0154, -0.0164, -0.0171, -0.0161, -0.0165, -0.0162, -0.0152,\n",
       "         -0.0156, -0.0055, -0.0029, -0.0160, -0.0181, -0.0056, -0.0031, -0.0168,\n",
       "         -0.0151, -0.0153, -0.0073, -0.0032, -0.0175, -0.0082, -0.0161, -0.0103,\n",
       "         -0.0154, -0.0151, -0.0053, -0.0170, -0.0051, -0.0164, -0.0049, -0.0158,\n",
       "         -0.0028, -0.0046, -0.0061, -0.0194, -0.0042, -0.0153, -0.0151, -0.0074,\n",
       "         -0.0154, -0.0184, -0.0167, -0.0168, -0.0173, -0.0179, -0.0172, -0.0179,\n",
       "         -0.0170, -0.0171, -0.0178, -0.0186, -0.0185, -0.0167, -0.0196, -0.0186,\n",
       "         -0.0175, -0.0167, -0.0185, -0.0181, -0.0175, -0.0235, -0.0233, -0.0181,\n",
       "         -0.0172, -0.0173, -0.0234, -0.0172, -0.0173, -0.0181, -0.0162, -0.0171,\n",
       "         -0.0197, -0.0166, -0.0211, -0.0157, -0.0179, -0.0171, -0.0168, -0.0163,\n",
       "         -0.0216, -0.0177, -0.0191, -0.0201, -0.0171, -0.0232, -0.0174, -0.0183,\n",
       "         -0.0234, -0.0157, -0.0187, -0.0174, -0.0174, -0.0250, -0.0218, -0.0223,\n",
       "         -0.0186, -0.0154, -0.0200, -0.0217, -0.0159, -0.0173, -0.0177, -0.0185,\n",
       "         -0.0228, -0.0171, -0.0172, -0.0177, -0.0202, -0.0218, -0.0169, -0.0181,\n",
       "         -0.0168, -0.0156, -0.0157, -0.0177, -0.0181, -0.0170, -0.0213, -0.0177,\n",
       "         -0.0180, -0.0216, -0.0170, -0.0256, -0.0178, -0.0185, -0.0167, -0.0168,\n",
       "         -0.0172, -0.0197, -0.0165, -0.0182, -0.0162, -0.0230, -0.0216, -0.0178,\n",
       "         -0.0175, -0.0181, -0.0170, -0.0170, -0.0176, -0.0165, -0.0215, -0.0163,\n",
       "         -0.0168, -0.0184, -0.0161, -0.0165, -0.0161, -0.0177, -0.0170, -0.0171,\n",
       "         -0.0161, -0.0207, -0.0222, -0.0167, -0.0166, -0.0204, -0.0168, -0.0155,\n",
       "         -0.0232, -0.0172, -0.0191, -0.0199, -0.0156, -0.0172, -0.0185, -0.0171,\n",
       "         -0.0166, -0.0174, -0.0172, -0.0239, -0.0182, -0.0191, -0.0179, -0.0172,\n",
       "         -0.0181, -0.0159, -0.0187, -0.0188, -0.0224, -0.0177, -0.0190, -0.0194,\n",
       "         -0.0177, -0.0211, -0.0220, -0.0046, -0.0071, -0.0068, -0.0044, -0.0080,\n",
       "         -0.0077, -0.0019, -0.0024, -0.0019, -0.0019, -0.0037, -0.0064, -0.0019,\n",
       "         -0.0036, -0.0041, -0.0082, -0.0018, -0.0035, -0.0018, -0.0018, -0.0048,\n",
       "         -0.0070, -0.0063, -0.0020, -0.0033, -0.0022, -0.0066, -0.0017, -0.0021,\n",
       "         -0.0038, -0.0081, -0.0044, -0.0018, -0.0146, -0.0142, -0.0150, -0.0143,\n",
       "         -0.0170, -0.0174, -0.0150, -0.0150, -0.0169, -0.0143, -0.0151, -0.0142,\n",
       "         -0.0145, -0.0148, -0.0159, -0.0150, -0.0169, -0.0148, -0.0153, -0.0144,\n",
       "         -0.0142, -0.0175, -0.0150, -0.0143, -0.0151, -0.0148, -0.0151, -0.0150,\n",
       "         -0.0148, -0.0142, -0.0150, -0.0161, -0.0169, -0.0153, -0.0145, -0.0149,\n",
       "         -0.0163, -0.0149, -0.0172, -0.0173, -0.0150, -0.0167, -0.0167, -0.0153,\n",
       "         -0.0143, -0.0144, -0.0144, -0.0153, -0.0156, -0.0151, -0.0151, -0.0143,\n",
       "         -0.0144, -0.0152, -0.0162, -0.0161, -0.0149, -0.0171, -0.0174, -0.0140,\n",
       "         -0.0148, -0.0143, -0.0150, -0.0156, -0.0156, -0.0154, -0.0150, -0.0153,\n",
       "         -0.0160, -0.0152, -0.0147, -0.0161, -0.0147, -0.0144, -0.0155, -0.0166,\n",
       "         -0.0140, -0.0162, -0.0156, -0.0147], grad_fn=<SqueezeBackward1>),\n",
       " tensor([-0.0182, -0.0164, -0.0165, -0.0181, -0.0181, -0.0176, -0.0151, -0.0184,\n",
       "         -0.0178, -0.0178, -0.0154, -0.0187, -0.0177, -0.0194, -0.0172, -0.0170,\n",
       "         -0.0189, -0.0182, -0.0173, -0.0172, -0.0166, -0.0171, -0.0162, -0.0169,\n",
       "         -0.0183, -0.0172, -0.0185, -0.0174, -0.0181, -0.0184, -0.0166, -0.0155,\n",
       "         -0.0185, -0.0196, -0.0186, -0.0151, -0.0179, -0.0154, -0.0160, -0.0174,\n",
       "         -0.0166, -0.0191, -0.0172, -0.0192, -0.0153, -0.0171, -0.0181, -0.0182,\n",
       "         -0.0193, -0.0152, -0.0181, -0.0190, -0.0150, -0.0167, -0.0195, -0.0166,\n",
       "         -0.0165, -0.0191, -0.0153, -0.0195, -0.0180, -0.0169, -0.0185, -0.0159,\n",
       "         -0.0173, -0.0170, -0.0152, -0.0192, -0.0151, -0.0191, -0.0157, -0.0170,\n",
       "         -0.0170, -0.0168, -0.0154, -0.0159, -0.0169, -0.0160, -0.0178, -0.0157,\n",
       "         -0.0176, -0.0186, -0.0178, -0.0172, -0.0195, -0.0173, -0.0152, -0.0166,\n",
       "         -0.0156, -0.0191, -0.0183, -0.0183, -0.0161, -0.0177, -0.0157, -0.0191,\n",
       "         -0.0179, -0.0178, -0.0155, -0.0193, -0.0159, -0.0165, -0.0175, -0.0153,\n",
       "         -0.0192, -0.0185, -0.0198, -0.0193, -0.0196, -0.0167, -0.0172, -0.0191,\n",
       "         -0.0172, -0.0184, -0.0172, -0.0191, -0.0166, -0.0178, -0.0169, -0.0171,\n",
       "         -0.0168, -0.0173, -0.0152, -0.0171, -0.0206, -0.0189, -0.0189, -0.0180,\n",
       "         -0.0150, -0.0149, -0.0155, -0.0152, -0.0155, -0.0154, -0.0167, -0.0169,\n",
       "         -0.0185, -0.0199, -0.0155, -0.0188, -0.0184, -0.0168, -0.0156, -0.0150,\n",
       "         -0.0155, -0.0179, -0.0150, -0.0186, -0.0172, -0.0182, -0.0190, -0.0165,\n",
       "         -0.0153, -0.0191, -0.0172, -0.0157, -0.0178, -0.0205, -0.0186, -0.0156,\n",
       "         -0.0192, -0.0187, -0.0195, -0.0174, -0.0170, -0.0179, -0.0155, -0.0179,\n",
       "         -0.0154, -0.0175, -0.0165, -0.0195, -0.0158, -0.0167, -0.0169, -0.0156,\n",
       "         -0.0158, -0.0160, -0.0138, -0.0127, -0.0141, -0.0160, -0.0160, -0.0172,\n",
       "         -0.0149, -0.0118, -0.0144, -0.0126, -0.0125, -0.0154, -0.0125, -0.0161,\n",
       "         -0.0173, -0.0197, -0.0138, -0.0135, -0.0146, -0.0171, -0.0147, -0.0139,\n",
       "         -0.0178, -0.0184, -0.0143, -0.0193, -0.0190, -0.0158, -0.0156, -0.0181,\n",
       "         -0.0133, -0.0142, -0.0164, -0.0193, -0.0188, -0.0181, -0.0127, -0.0152,\n",
       "         -0.0145, -0.0160, -0.0131, -0.0142, -0.0170, -0.0131, -0.0123, -0.0152,\n",
       "         -0.0148, -0.0150, -0.0185, -0.0188, -0.0196, -0.0169, -0.0181, -0.0159,\n",
       "         -0.0176, -0.0128, -0.0131, -0.0157, -0.0134, -0.0168, -0.0166, -0.0161,\n",
       "         -0.0124, -0.0151, -0.0145, -0.0123, -0.0128, -0.0138, -0.0144, -0.0126,\n",
       "         -0.0130, -0.0182, -0.0146, -0.0197, -0.0159, -0.0172, -0.0137, -0.0191,\n",
       "         -0.0191, -0.0239, -0.0194, -0.0171, -0.0157, -0.0168, -0.0158, -0.0187,\n",
       "         -0.0154, -0.0164, -0.0252, -0.0161, -0.0162, -0.0174, -0.0163, -0.0152,\n",
       "         -0.0214, -0.0206, -0.0168, -0.0171, -0.0176, -0.0167, -0.0157, -0.0153,\n",
       "         -0.0164, -0.0230, -0.0158, -0.0161, -0.0208, -0.0203, -0.0187, -0.0172,\n",
       "         -0.0236, -0.0154, -0.0194, -0.0202, -0.0247, -0.0156, -0.0171, -0.0199,\n",
       "         -0.0174, -0.0153, -0.0211, -0.0233, -0.0195, -0.0205, -0.0200, -0.0170,\n",
       "         -0.0201, -0.0191, -0.0197, -0.0172, -0.0160, -0.0155, -0.0189, -0.0203,\n",
       "         -0.0245, -0.0257, -0.0201, -0.0158, -0.0202, -0.0240, -0.0151, -0.0240,\n",
       "         -0.0151, -0.0243, -0.0202, -0.0155, -0.0204, -0.0207, -0.0183, -0.0257,\n",
       "         -0.0211, -0.0233, -0.0156, -0.0151, -0.0200, -0.0157, -0.0249, -0.0154,\n",
       "         -0.0253, -0.0163, -0.0152, -0.0240, -0.0231, -0.0248, -0.0212, -0.0163,\n",
       "         -0.0167, -0.0198, -0.0203, -0.0235, -0.0171, -0.0207, -0.0169, -0.0224,\n",
       "         -0.0169, -0.0205, -0.0160, -0.0248, -0.0163, -0.0161, -0.0194, -0.0204,\n",
       "         -0.0159, -0.0168, -0.0209, -0.0210, -0.0159, -0.0177, -0.0164, -0.0223,\n",
       "         -0.0225, -0.0149, -0.0153, -0.0204, -0.0171, -0.0203, -0.0164, -0.0241,\n",
       "         -0.0166, -0.0225, -0.0229, -0.0218, -0.0220, -0.0201, -0.0241, -0.0151,\n",
       "         -0.0206, -0.0172, -0.0257, -0.0224, -0.0242, -0.0180, -0.0158, -0.0261,\n",
       "         -0.0220, -0.0159, -0.0171, -0.0198, -0.0160, -0.0203, -0.0151, -0.0205,\n",
       "         -0.0244, -0.0152, -0.0242, -0.0228, -0.0198, -0.0229, -0.0173, -0.0228,\n",
       "         -0.0217, -0.0199, -0.0235, -0.0169, -0.0185, -0.0247, -0.0205, -0.0173,\n",
       "         -0.0155, -0.0058, -0.0065, -0.0037, -0.0032, -0.0043, -0.0030, -0.0030,\n",
       "         -0.0058, -0.0066, -0.0035, -0.0038, -0.0034, -0.0054, -0.0042, -0.0031,\n",
       "         -0.0034, -0.0052, -0.0061, -0.0034, -0.0033, -0.0051, -0.0034, -0.0051,\n",
       "         -0.0046, -0.0032, -0.0031, -0.0030, -0.0055, -0.0052, -0.0056, -0.0057,\n",
       "         -0.0067, -0.0041, -0.0034, -0.0176, -0.0186, -0.0187, -0.0169, -0.0173,\n",
       "         -0.0172, -0.0196, -0.0173, -0.0189, -0.0168, -0.0111, -0.0187, -0.0188,\n",
       "         -0.0102, -0.0173, -0.0187, -0.0097, -0.0114, -0.0181, -0.0191, -0.0181,\n",
       "         -0.0151, -0.0172, -0.0087, -0.0173, -0.0175, -0.0101, -0.0193, -0.0196,\n",
       "         -0.0165, -0.0172, -0.0172, -0.0178, -0.0148, -0.0188, -0.0197, -0.0199,\n",
       "         -0.0190, -0.0185, -0.0167, -0.0190, -0.0170, -0.0196, -0.0171, -0.0161,\n",
       "         -0.0172, -0.0156, -0.0164, -0.0164, -0.0198, -0.0194, -0.0174, -0.0158,\n",
       "         -0.0198, -0.0162, -0.0172, -0.0184, -0.0185, -0.0201, -0.0172, -0.0163,\n",
       "         -0.0126, -0.0169, -0.0142, -0.0090, -0.0095, -0.0164, -0.0176, -0.0167,\n",
       "         -0.0171, -0.0170, -0.0173, -0.0187, -0.0169, -0.0107, -0.0108, -0.0098,\n",
       "         -0.0148, -0.0170, -0.0110, -0.0173, -0.0195, -0.0168, -0.0173, -0.0105,\n",
       "         -0.0168, -0.0169, -0.0189, -0.0144, -0.0146, -0.0112, -0.0172, -0.0176,\n",
       "         -0.0171, -0.0174, -0.0116, -0.0111, -0.0188, -0.0173, -0.0169, -0.0166,\n",
       "         -0.0144], grad_fn=<SqueezeBackward1>),\n",
       " tensor([-0.0159, -0.0161, -0.0160, -0.0166, -0.0166, -0.0171, -0.0162, -0.0169,\n",
       "         -0.0168, -0.0156, -0.0164, -0.0162, -0.0162, -0.0168, -0.0156, -0.0167,\n",
       "         -0.0164, -0.0168, -0.0180, -0.0166, -0.0160, -0.0176, -0.0160, -0.0166,\n",
       "         -0.0156, -0.0177, -0.0169, -0.0157, -0.0166, -0.0156, -0.0164, -0.0162,\n",
       "         -0.0166, -0.0164, -0.0168, -0.0161, -0.0160, -0.0164, -0.0159, -0.0170,\n",
       "         -0.0173, -0.0167, -0.0157, -0.0180, -0.0165, -0.0160, -0.0174, -0.0169,\n",
       "         -0.0169, -0.0162, -0.0170, -0.0179, -0.0174, -0.0168, -0.0157, -0.0163,\n",
       "         -0.0169, -0.0160, -0.0165, -0.0164, -0.0173, -0.0169, -0.0168, -0.0174,\n",
       "         -0.0178, -0.0175, -0.0162, -0.0179, -0.0161, -0.0164, -0.0159, -0.0174,\n",
       "         -0.0160, -0.0175, -0.0156, -0.0157, -0.0161, -0.0165, -0.0161, -0.0161,\n",
       "         -0.0168, -0.0160, -0.0172, -0.0160, -0.0157, -0.0179, -0.0162, -0.0173,\n",
       "         -0.0164, -0.0179, -0.0175, -0.0168, -0.0159, -0.0164, -0.0163, -0.0166,\n",
       "         -0.0166, -0.0172, -0.0156, -0.0168, -0.0174, -0.0160, -0.0170, -0.0158,\n",
       "         -0.0161, -0.0161, -0.0167, -0.0180, -0.0168, -0.0174, -0.0157, -0.0160,\n",
       "         -0.0178, -0.0171, -0.0160, -0.0160, -0.0162, -0.0158, -0.0171, -0.0160,\n",
       "         -0.0167, -0.0166, -0.0161, -0.0175, -0.0164, -0.0173, -0.0161, -0.0168,\n",
       "         -0.0161, -0.0161, -0.0163, -0.0162, -0.0159, -0.0156, -0.0160, -0.0175,\n",
       "         -0.0159, -0.0161, -0.0163, -0.0164, -0.0161, -0.0168, -0.0156, -0.0170,\n",
       "         -0.0162, -0.0166, -0.0161, -0.0167, -0.0156, -0.0175, -0.0156, -0.0161,\n",
       "         -0.0160, -0.0156, -0.0163, -0.0163, -0.0166, -0.0164, -0.0168, -0.0161,\n",
       "         -0.0161, -0.0166, -0.0167, -0.0164, -0.0171, -0.0159, -0.0156, -0.0166,\n",
       "         -0.0164, -0.0158, -0.0161, -0.0163, -0.0160, -0.0164, -0.0161, -0.0159,\n",
       "         -0.0004, -0.0073, -0.0022, -0.0008, -0.0090, -0.0035, -0.0004, -0.0093,\n",
       "         -0.0029, -0.0004, -0.0018, -0.0003, -0.0008, -0.0022, -0.0073, -0.0006,\n",
       "         -0.0010, -0.0185, -0.0259, -0.0280, -0.0235, -0.0279, -0.0232, -0.0203,\n",
       "         -0.0196, -0.0271, -0.0211, -0.0232, -0.0197, -0.0196, -0.0245, -0.0214,\n",
       "         -0.0270, -0.0231, -0.0232, -0.0292, -0.0232, -0.0181, -0.0182, -0.0255,\n",
       "         -0.0248, -0.0274, -0.0187, -0.0242, -0.0295, -0.0252, -0.0243, -0.0225,\n",
       "         -0.0279, -0.0255, -0.0204, -0.0295, -0.0286, -0.0174, -0.0231, -0.0296,\n",
       "         -0.0177, -0.0284, -0.0182, -0.0191, -0.0234, -0.0193, -0.0296, -0.0289,\n",
       "         -0.0225, -0.0203, -0.0234, -0.0228, -0.0268, -0.0281, -0.0282, -0.0280,\n",
       "         -0.0273, -0.0181, -0.0292, -0.0274, -0.0185, -0.0206, -0.0183, -0.0290,\n",
       "         -0.0206, -0.0193, -0.0173, -0.0180, -0.0229, -0.0220, -0.0235, -0.0183,\n",
       "         -0.0289, -0.0200, -0.0245, -0.0213, -0.0282, -0.0210, -0.0255, -0.0179,\n",
       "         -0.0234, -0.0230, -0.0213, -0.0230, -0.0249, -0.0261, -0.0200, -0.0258,\n",
       "         -0.0206, -0.0199, -0.0228, -0.0290, -0.0189, -0.0227, -0.0191, -0.0229,\n",
       "         -0.0291, -0.0273, -0.0230, -0.0245, -0.0258, -0.0236, -0.0226, -0.0211,\n",
       "         -0.0241, -0.0231, -0.0012, -0.0014, -0.0010, -0.0030, -0.0011, -0.0015,\n",
       "         -0.0028, -0.0011, -0.0036, -0.0022, -0.0030, -0.0037, -0.0038, -0.0014,\n",
       "         -0.0019, -0.0015, -0.0022, -0.0031, -0.0026, -0.0038, -0.0024, -0.0030,\n",
       "         -0.0022, -0.0023, -0.0037, -0.0023, -0.0207, -0.0167, -0.0168, -0.0181,\n",
       "         -0.0175, -0.0177, -0.0234, -0.0168, -0.0172, -0.0165, -0.0166, -0.0180,\n",
       "         -0.0166, -0.0176, -0.0169, -0.0168, -0.0197, -0.0174, -0.0176, -0.0173,\n",
       "         -0.0167, -0.0173, -0.0178, -0.0169, -0.0220, -0.0170, -0.0179, -0.0182,\n",
       "         -0.0179, -0.0184, -0.0174, -0.0165, -0.0169, -0.0177, -0.0174, -0.0168,\n",
       "         -0.0176, -0.0172, -0.0178, -0.0174, -0.0166, -0.0176, -0.0183, -0.0166,\n",
       "         -0.0168, -0.0180, -0.0214, -0.0167, -0.0179, -0.0184, -0.0168, -0.0196,\n",
       "         -0.0186, -0.0167, -0.0178, -0.0180, -0.0167, -0.0175, -0.0173, -0.0183,\n",
       "         -0.0166, -0.0175, -0.0181, -0.0178, -0.0186, -0.0172, -0.0165, -0.0173,\n",
       "         -0.0170, -0.0172, -0.0172, -0.0181, -0.0181, -0.0173, -0.0184, -0.0167,\n",
       "         -0.0169, -0.0180, -0.0176, -0.0173, -0.0172, -0.0186, -0.0167, -0.0172,\n",
       "         -0.0186, -0.0171, -0.0203, -0.0183, -0.0166, -0.0173, -0.0185, -0.0172],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-0.0159, -0.0209, -0.0207, -0.0152, -0.0176, -0.0158, -0.0209, -0.0208,\n",
       "         -0.0213, -0.0159, -0.0203, -0.0200, -0.0211, -0.0213, -0.0212, -0.0158,\n",
       "         -0.0158, -0.0196, -0.0208, -0.0208, -0.0174, -0.0195, -0.0160, -0.0159,\n",
       "         -0.0213, -0.0158, -0.0209, -0.0157, -0.0204, -0.0213, -0.0196, -0.0177,\n",
       "         -0.0211, -0.0209, -0.0196, -0.0202, -0.0176, -0.0155, -0.0196, -0.0157,\n",
       "         -0.0196, -0.0177, -0.0197, -0.0159, -0.0211, -0.0209, -0.0150, -0.0175,\n",
       "         -0.0158, -0.0209, -0.0218, -0.0196, -0.0196, -0.0211, -0.0151, -0.0210,\n",
       "         -0.0194, -0.0206, -0.0211, -0.0210, -0.0158, -0.0172, -0.0208, -0.0211,\n",
       "         -0.0208, -0.0164, -0.0172, -0.0211, -0.0196, -0.0195, -0.0209, -0.0211,\n",
       "         -0.0167, -0.0154, -0.0158, -0.0157, -0.0195, -0.0176, -0.0187, -0.0202,\n",
       "         -0.0155, -0.0219, -0.0158, -0.0211, -0.0211, -0.0157, -0.0205, -0.0204,\n",
       "         -0.0215, -0.0159, -0.0209, -0.0208, -0.0196, -0.0159, -0.0209, -0.0192,\n",
       "         -0.0156, -0.0228, -0.0176, -0.0158, -0.0208, -0.0213, -0.0196, -0.0158,\n",
       "         -0.0211, -0.0212, -0.0159, -0.0203, -0.0228, -0.0193, -0.0207, -0.0153,\n",
       "         -0.0151, -0.0154, -0.0212, -0.0196, -0.0196, -0.0191, -0.0210, -0.0206,\n",
       "         -0.0213, -0.0208, -0.0210, -0.0157, -0.0225, -0.0154, -0.0157, -0.0159,\n",
       "         -0.0196, -0.0212, -0.0159, -0.0204, -0.0155, -0.0205, -0.0153, -0.0196,\n",
       "         -0.0212, -0.0213, -0.0208, -0.0221, -0.0196, -0.0209, -0.0194, -0.0212,\n",
       "         -0.0196, -0.0206, -0.0149, -0.0210, -0.0159, -0.0210, -0.0211, -0.0210,\n",
       "         -0.0211, -0.0158, -0.0198, -0.0207, -0.0171, -0.0190, -0.0193, -0.0156,\n",
       "         -0.0175, -0.0150, -0.0211, -0.0159, -0.0210, -0.0201, -0.0221, -0.0208,\n",
       "         -0.0153, -0.0159, -0.0158, -0.0209, -0.0207, -0.0156, -0.0194, -0.0208,\n",
       "         -0.0191, -0.0212, -0.0158, -0.0157, -0.0196, -0.0211, -0.0177, -0.0210,\n",
       "         -0.0166, -0.0206, -0.0216, -0.0159, -0.0157, -0.0188, -0.0124, -0.0188,\n",
       "         -0.0191, -0.0213, -0.0186, -0.0150, -0.0181, -0.0142, -0.0198, -0.0106,\n",
       "         -0.0185, -0.0208, -0.0137, -0.0202, -0.0190, -0.0196, -0.0228, -0.0190,\n",
       "         -0.0208, -0.0211, -0.0161, -0.0204, -0.0117, -0.0196, -0.0126, -0.0199,\n",
       "         -0.0180, -0.0195, -0.0163, -0.0184, -0.0176, -0.0203, -0.0183, -0.0189,\n",
       "         -0.0193, -0.0196, -0.0095, -0.0134, -0.0219, -0.0200, -0.0199, -0.0196,\n",
       "         -0.0202, -0.0211, -0.0148, -0.0193, -0.0187, -0.0144, -0.0209, -0.0147,\n",
       "         -0.0206, -0.0108, -0.0140, -0.0194, -0.0137, -0.0198, -0.0207, -0.0141,\n",
       "         -0.0118, -0.0161, -0.0174, -0.0108, -0.0165, -0.0179, -0.0168, -0.0177,\n",
       "         -0.0189, -0.0198, -0.0197, -0.0141, -0.0203, -0.0204, -0.0183, -0.0127,\n",
       "         -0.0187, -0.0184, -0.0188, -0.0197, -0.0200, -0.0193, -0.0184, -0.0189,\n",
       "         -0.0190, -0.0211, -0.0174, -0.0146, -0.0143, -0.0227, -0.0138, -0.0197,\n",
       "         -0.0135, -0.0211, -0.0197, -0.0201, -0.0198, -0.0173, -0.0198, -0.0200,\n",
       "         -0.0210, -0.0166, -0.0176, -0.0200, -0.0148, -0.0146, -0.0145, -0.0146,\n",
       "         -0.0148, -0.0166, -0.0146, -0.0145, -0.0187, -0.0184, -0.0173, -0.0156,\n",
       "         -0.0145, -0.0143, -0.0177, -0.0165, -0.0169, -0.0158, -0.0189, -0.0154,\n",
       "         -0.0147, -0.0141, -0.0145, -0.0144, -0.0184, -0.0175, -0.0146, -0.0148,\n",
       "         -0.0152, -0.0147, -0.0184, -0.0141, -0.0147, -0.0145, -0.0148, -0.0174,\n",
       "         -0.0145, -0.0174, -0.0145, -0.0162, -0.0189, -0.0145, -0.0193, -0.0178,\n",
       "         -0.0175, -0.0161, -0.0187, -0.0147, -0.0146, -0.0193, -0.0156, -0.0145,\n",
       "         -0.0147, -0.0208, -0.0158, -0.0147, -0.0144, -0.0147, -0.0148, -0.0174,\n",
       "         -0.0145, -0.0168, -0.0149, -0.0140, -0.0148, -0.0145, -0.0140, -0.0191,\n",
       "         -0.0145, -0.0140, -0.0148, -0.0168, -0.0147, -0.0150, -0.0148, -0.0147,\n",
       "         -0.0185, -0.0147, -0.0146, -0.0146, -0.0148, -0.0145, -0.0145, -0.0143,\n",
       "         -0.0188, -0.0147, -0.0146, -0.0189, -0.0148, -0.0160, -0.0158, -0.0147,\n",
       "         -0.0145, -0.0145, -0.0168, -0.0150, -0.0150, -0.0184, -0.0145, -0.0174,\n",
       "         -0.0160, -0.0202, -0.0159, -0.0159, -0.0143, -0.0140, -0.0145, -0.0179,\n",
       "         -0.0181, -0.0184, -0.0149, -0.0152, -0.0146, -0.0185, -0.0140, -0.0151,\n",
       "         -0.0150, -0.0168, -0.0147, -0.0158, -0.0145, -0.0145, -0.0174, -0.0192,\n",
       "         -0.0147, -0.0145, -0.0174, -0.0143, -0.0157, -0.0183, -0.0197, -0.0145,\n",
       "         -0.0147, -0.0154, -0.0189, -0.0148, -0.0193, -0.0142, -0.0146, -0.0144,\n",
       "         -0.0194, -0.0188, -0.0145, -0.0185, -0.0195, -0.0155, -0.0144, -0.0144,\n",
       "         -0.0193, -0.0146, -0.0146, -0.0174, -0.0183, -0.0158, -0.0165, -0.0156,\n",
       "         -0.0174, -0.0154, -0.0146, -0.0195, -0.0146, -0.0184, -0.0158, -0.0167,\n",
       "         -0.0176, -0.0180, -0.0151, -0.0151, -0.0061, -0.0067, -0.0066, -0.0031,\n",
       "         -0.0033, -0.0066, -0.0066, -0.0054, -0.0029, -0.0057, -0.0050, -0.0067,\n",
       "         -0.0025, -0.0059, -0.0030, -0.0032, -0.0032, -0.0051, -0.0025, -0.0061,\n",
       "         -0.0054, -0.0044, -0.0022, -0.0023, -0.0023, -0.0023, -0.0017, -0.0023,\n",
       "         -0.0022, -0.0021, -0.0023, -0.0024, -0.0021, -0.0024, -0.0012],\n",
       "        grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class IvySPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_embedding, \n",
    "        num_encoder_blocks,\n",
    "        n_heads, \n",
    "        ffn_hidden_dim,\n",
    "        attention_dropout, \n",
    "        gate_dropout,\n",
    "        ffn_dropout,\n",
    "        external_dim,\n",
    "        weight_initializer_std=0.02,\n",
    "        linear_bias_initializer_value=0.0,\n",
    "        gate_bias_initializer_value=10.0,\n",
    "        remove_kernel=False,\n",
    "        remove_positional_embedding=False,\n",
    "        remove_external_attention=False,\n",
    "        remove_gate=False\n",
    "    ):\n",
    "        super(IvySPT, self).__init__()\n",
    "        self.surface_embedding = SurfaceEmbedding(\n",
    "            d_embedding, \n",
    "            remove_kernel, \n",
    "            remove_positional_embedding\n",
    "        )\n",
    "        self.surface_encoder = SurfaceEncoder(\n",
    "            d_embedding, \n",
    "            num_encoder_blocks,\n",
    "            n_heads, \n",
    "            ffn_hidden_dim,\n",
    "            attention_dropout, \n",
    "            gate_dropout,\n",
    "            ffn_dropout,\n",
    "            external_dim,\n",
    "            weight_initializer_std,\n",
    "            linear_bias_initializer_value,\n",
    "            gate_bias_initializer_value,\n",
    "            remove_external_attention,\n",
    "            remove_gate\n",
    "        )\n",
    "        self.final_layer = nn.Linear(d_embedding, 1)\n",
    "        nn.init.normal_(self.final_layer.weight, mean=0.0, std=weight_initializer_std * (1 / (2 * (num_encoder_blocks + 1)) ** 0.5))\n",
    "        nn.init.constant_(self.final_layer.bias, linear_bias_initializer_value)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        batch,\n",
    "        output_attention_map=False\n",
    "    ):\n",
    "        # Obtain the embedded sequences and external features from the SurfaceEmbedding module\n",
    "        embedded_sequences_batch, external_features_batch = self.surface_embedding(batch)\n",
    "\n",
    "        # Encode the sequences using the SurfaceEncoder module\n",
    "        encoded_sequences_batch, self_attention_maps, external_attention_maps = self.surface_encoder(\n",
    "            embedded_sequences_batch, \n",
    "            external_features_batch, \n",
    "            output_attention_map\n",
    "        )\n",
    "\n",
    "        # List to hold the implied volatility estimates for each query point in the batch\n",
    "        iv_estimates_batch = []\n",
    "\n",
    "        query_self_attention_maps = []\n",
    "        query_external_attention_maps = []\n",
    "\n",
    "        for i in range(len(encoded_sequences_batch)):\n",
    "            # Extract the encoded sequence\n",
    "            encoded_sequence = encoded_sequences_batch[i]\n",
    "\n",
    "            # Determine the number of query points for this sequence\n",
    "            num_query_points = len(batch['Query Points']['Log Moneyness'][i])\n",
    "\n",
    "            # Extract the encoded query points (last num_query_points elements in the sequence)\n",
    "            encoded_query_points = encoded_sequence[-num_query_points:]\n",
    "\n",
    "            # Estimate the implied volatility for each query point using the fully connected layer\n",
    "            iv_estimates = self.final_layer(encoded_query_points).squeeze(-1)\n",
    "\n",
    "            # Append the estimates to the batch list\n",
    "            iv_estimates_batch.append(iv_estimates)\n",
    "\n",
    "            if output_attention_map:\n",
    "                # Extract the attention maps for the query points\n",
    "                self_attention_map = self_attention_maps[i][-num_query_points:]\n",
    "                external_attention_map = external_attention_maps[i][-num_query_points:]\n",
    "\n",
    "                query_self_attention_maps.append(self_attention_map)\n",
    "                query_external_attention_maps.append(external_attention_map)\n",
    "\n",
    "        if output_attention_map:\n",
    "            return iv_estimates_batch, self_attention_maps, external_attention_maps\n",
    "        \n",
    "        return iv_estimates_batch, None, None\n",
    "\n",
    "# Example of initializing and using this module\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "n_heads = HYPERPARAMETERS['Surface Encoding']['Number of Heads']\n",
    "ffn_hidden_dim = HYPERPARAMETERS['Surface Encoding']['FFN Hidden Dimension']\n",
    "attention_dropout = HYPERPARAMETERS['Surface Encoding']['Attention Dropout']\n",
    "gate_dropout = HYPERPARAMETERS['Surface Encoding']['Gate Dropout']\n",
    "ffn_dropout = HYPERPARAMETERS['Surface Encoding']['FFN Dropout']\n",
    "num_encoder_blocks = HYPERPARAMETERS['Surface Encoding']['Number of Blocks']\n",
    "d_embedding = HYPERPARAMETERS['Surface Embedding']['Embedding Dimension']  # Desired number of output channels\n",
    "external_dim = 3\n",
    "\n",
    "ivy_spt = IvySPT(\n",
    "    d_embedding, \n",
    "    num_encoder_blocks,\n",
    "    n_heads, \n",
    "    ffn_hidden_dim,\n",
    "    attention_dropout, \n",
    "    gate_dropout,\n",
    "    ffn_dropout,\n",
    "    external_dim\n",
    ")\n",
    "\n",
    "# Pass the batch through the IvySPT model to get implied volatility estimates\n",
    "iv_estimates_batch, self_attention_maps, external_attention_maps = ivy_spt(batch, output_attention_map=False)\n",
    "iv_estimates_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ivy_spt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m ivy_spt, iv_estimates_batch, self_attention_maps, external_attention_maps\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ivy_spt' is not defined"
     ]
    }
   ],
   "source": [
    "del ivy_spt, iv_estimates_batch, self_attention_maps, external_attention_maps\n",
    "\n",
    "\n",
    "# import gc\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.2695, 0.2695, 0.3415, 0.3091, 0.3302, 0.2849, 0.2695, 0.3302, 0.2561,\n",
       "         0.2912, 0.3295, 0.3461, 0.2658, 0.3302, 0.3133, 0.3065, 0.3302, 0.2950,\n",
       "         0.3461, 0.2847, 0.3088, 0.2847, 0.2711, 0.2940, 0.2561, 0.2887, 0.2561,\n",
       "         0.2683, 0.2662, 0.3295, 0.2817, 0.3133, 0.3271, 0.2548, 0.3302, 0.3295,\n",
       "         0.2849, 0.3302, 0.3302, 0.2695, 0.3302, 0.2702, 0.3302, 0.3716, 0.2707,\n",
       "         0.3295, 0.3302, 0.2604, 0.3133, 0.3104, 0.2695, 0.3302, 0.2631, 0.2702,\n",
       "         0.2654, 0.3295, 0.2561, 0.3138, 0.2702, 0.3153, 0.3128, 0.2695, 0.2561,\n",
       "         0.2665, 0.3024, 0.2849, 0.3461, 0.2849, 0.2661, 0.3066, 0.2564, 0.2844,\n",
       "         0.2561, 0.2702, 0.3295, 0.2745, 0.2695, 0.3000, 0.2636, 0.3302, 0.2585,\n",
       "         0.3356, 0.2849, 0.3216, 0.2785, 0.3190, 0.3302, 0.2695, 0.3302, 0.2676,\n",
       "         0.2737, 0.2695, 0.3356, 0.2661, 0.3295, 0.3302, 0.2601, 0.2695, 0.2717,\n",
       "         0.2695, 0.2859, 0.3133, 0.2695, 0.2561, 0.3295, 0.3302, 0.2695, 0.2695,\n",
       "         0.3302, 0.3133, 0.3047, 0.2561, 0.2561, 0.2695, 0.3356, 0.3413, 0.2695,\n",
       "         0.3133, 0.2561, 0.3295, 0.2616, 0.3295, 0.2888, 0.2695, 0.3302, 0.3104,\n",
       "         0.3096, 0.2740, 0.2607, 0.3133, 0.2666, 0.2775, 0.2929, 0.3295, 0.3461,\n",
       "         0.2561, 0.2710, 0.2661, 0.2561, 0.3356, 0.2702, 0.2581, 0.3302, 0.2702,\n",
       "         0.3302, 0.3302, 0.3215, 0.2669, 0.2877, 0.2759, 0.2661, 0.2854, 0.2985,\n",
       "         0.3295, 0.2631, 0.2661, 0.2770, 0.2561, 0.2609, 0.3302, 0.2628, 0.2695,\n",
       "         0.3461, 0.2716, 0.2871, 0.2804, 0.3116, 0.3302, 0.2894, 0.4638, 0.3104,\n",
       "         0.3651, 0.3439, 0.3153, 0.3111, 0.3495, 0.2803, 0.2844, 0.3451, 0.3081,\n",
       "         0.3100, 0.3491, 0.2940, 0.3186, 0.2867, 0.2829, 0.3227, 0.4616, 0.3150,\n",
       "         0.4355, 0.4535, 0.3170, 0.3788, 0.3126, 0.4782, 0.3439, 0.3120, 0.2870,\n",
       "         0.2817, 0.3202, 0.2878, 0.3081, 0.3473, 0.2819, 0.2757, 0.3187, 0.2901,\n",
       "         0.4840, 0.2996, 0.3473, 0.3051, 0.3044, 0.4344, 0.3507, 0.4801, 0.3369,\n",
       "         0.3235, 0.2824, 0.3695, 0.3381, 0.3175, 0.3695, 0.3459, 0.3335, 0.3648,\n",
       "         0.2771, 0.3258, 0.3508, 0.3295, 0.3267, 0.2716, 0.2983, 0.3472, 0.3258,\n",
       "         0.3258, 0.3338, 0.3280, 0.2834, 0.2778, 0.3716, 0.3472, 0.3235, 0.3472,\n",
       "         0.3459, 0.2899, 0.3459, 0.3473, 0.3347, 0.3473, 0.2607, 0.3822, 0.3535,\n",
       "         0.3258, 0.3258, 0.3469, 0.3472, 0.3114, 0.3147, 0.3695, 0.3695, 0.2622,\n",
       "         0.3473, 0.3632, 0.3422, 0.3472, 0.2690, 0.2665, 0.3398, 0.3472, 0.6848,\n",
       "         0.3001, 0.3320, 0.2909, 0.3227, 0.2803, 0.3700, 0.4059, 0.4187, 0.3459,\n",
       "         0.3469, 0.3103, 0.3459, 0.2912, 0.3258, 0.3273, 0.3258, 0.4187, 0.3473,\n",
       "         0.2778, 0.3379, 0.3473, 0.3473, 0.2693, 0.6979, 0.3822, 0.3242, 0.3397,\n",
       "         0.3146, 0.3507, 0.2703, 0.3390, 0.2887, 0.3510, 0.4187, 0.2792, 0.5274,\n",
       "         0.2886, 0.2902, 0.2826, 0.3419, 0.2725, 0.2686, 0.2735, 0.3472, 0.3473,\n",
       "         0.3507, 0.3381, 0.3822, 0.2809, 0.2643, 0.3695, 0.3716, 0.2985, 0.3510,\n",
       "         0.2723, 0.3187, 0.3822, 0.2952, 0.2957, 0.3109, 0.2810, 0.5483, 0.4187,\n",
       "         0.3369, 0.3277, 0.2782, 0.3401, 0.2721, 0.3472, 0.3002, 0.3695, 0.3209,\n",
       "         0.3459, 0.2989, 0.3473, 0.5483, 0.3469, 0.3216, 0.3192, 0.3167, 0.3136,\n",
       "         0.3194, 0.3264, 0.3043, 0.3047, 0.3011, 0.3043, 0.2967, 0.3194, 0.2968,\n",
       "         0.2973, 0.3001, 0.3310, 0.3061, 0.2967, 0.3011, 0.3056, 0.3047, 0.3182,\n",
       "         0.3194, 0.3037, 0.2964, 0.3076, 0.3194, 0.2982, 0.3043, 0.2974, 0.3292,\n",
       "         0.3020, 0.3053, 0.3042, 0.3089, 0.2767, 0.3089, 0.2854, 0.2671, 0.2943,\n",
       "         0.2808, 0.2738, 0.3089, 0.2829, 0.2853, 0.2823, 0.2788, 0.3124, 0.2811,\n",
       "         0.2849, 0.2871, 0.3116, 0.3116, 0.3089, 0.2682, 0.2819, 0.3089, 0.3089,\n",
       "         0.3089, 0.3116, 0.2819, 0.3089, 0.3116, 0.2767, 0.3116, 0.2879, 0.2849,\n",
       "         0.3089, 0.2858, 0.2794, 0.2777, 0.2785, 0.2685, 0.2765, 0.2760, 0.3124,\n",
       "         0.3089, 0.3116, 0.3089, 0.2829, 0.2849, 0.3116, 0.3116, 0.2792, 0.3116,\n",
       "         0.3096, 0.3069, 0.3124, 0.3089, 0.2824, 0.2717, 0.2672, 0.2872, 0.2779,\n",
       "         0.3000, 0.2763, 0.3089, 0.3124, 0.3089, 0.2814, 0.3229, 0.3089, 0.3116,\n",
       "         0.2791, 0.2818, 0.3116, 0.2978, 0.2849, 0.2849, 0.2999, 0.2849, 0.2849,\n",
       "         0.3089]),\n",
       " tensor([0.2906, 0.3365, 0.5096, 0.2726, 0.2813, 0.2881, 0.4041, 0.2970, 0.3017,\n",
       "         0.3056, 0.3919, 0.3156, 0.2899, 0.3284, 0.4003, 0.2914, 0.3032, 0.3120,\n",
       "         0.2953, 0.2862, 0.4731, 0.2803, 0.4101, 0.3146, 0.3002, 0.2974, 0.2711,\n",
       "         0.3724, 0.2737, 0.3042, 0.4484, 0.3578, 0.3025, 0.2807, 0.3123, 0.4041,\n",
       "         0.3174, 0.3151, 0.4731, 0.3529, 0.2829, 0.2985, 0.3123, 0.2841, 0.3578,\n",
       "         0.3676, 0.2858, 0.2881, 0.2924, 0.5174, 0.2851, 0.2802, 0.4101, 0.3086,\n",
       "         0.3042, 0.4101, 0.2949, 0.2717, 0.3578, 0.3110, 0.2854, 0.3037, 0.3211,\n",
       "         0.3597, 0.2819, 0.2819, 0.4041, 0.2829, 0.5174, 0.2948, 0.4731, 0.2908,\n",
       "         0.3375, 0.2759, 0.4201, 0.4041, 0.6361, 0.2893, 0.3017, 0.3151, 0.3017,\n",
       "         0.3124, 0.3546, 0.3681, 0.2894, 0.3011, 0.4041, 0.2805, 0.3755, 0.2891,\n",
       "         0.2829, 0.2718, 0.3510, 0.2962, 0.3968, 0.3091, 0.2810, 0.2859, 0.4136,\n",
       "         0.3163, 0.3556, 0.4101, 0.2838, 0.4041, 0.2839, 0.2957, 0.3016, 0.2990,\n",
       "         0.2940, 0.2829, 0.3308, 0.2726, 0.2756, 0.3271, 0.3696, 0.2801, 0.6375,\n",
       "         0.3452, 0.2908, 0.3983, 0.3010, 0.3096, 0.4799, 0.3030, 0.3020, 0.3011,\n",
       "         0.2828, 0.3276, 0.3968, 0.4731, 0.3151, 0.4041, 0.4731, 0.4456, 0.4101,\n",
       "         0.2811, 0.2869, 0.3084, 0.3578, 0.3271, 0.2935, 0.3068, 0.3597, 0.4101,\n",
       "         0.4562, 0.2751, 0.4799, 0.2696, 0.4300, 0.2833, 0.2944, 0.4101, 0.4101,\n",
       "         0.3013, 0.3262, 0.3597, 0.2774, 0.3007, 0.3017, 0.3151, 0.2728, 0.2706,\n",
       "         0.2875, 0.3255, 0.3013, 0.3018, 0.3597, 0.2893, 0.2893, 0.3695, 0.4101,\n",
       "         0.2791, 0.3151, 0.4484, 0.3801, 0.4731, 0.2950, 0.2950, 0.2879, 0.2686,\n",
       "         0.2922, 0.2675, 0.2950, 0.2749, 0.2950, 0.2827, 0.2675, 0.2659, 0.2781,\n",
       "         0.2950, 0.2811, 0.2675, 0.2753, 0.2768, 0.2864, 0.2832, 0.2675, 0.2768,\n",
       "         0.2675, 0.2718, 0.2800, 0.2751, 0.2675, 0.2768, 0.2768, 0.2950, 0.2675,\n",
       "         0.2703, 0.2664, 0.2675, 0.2950, 0.2938, 0.2938, 0.2668, 0.2675, 0.2675,\n",
       "         0.2675, 0.2675, 0.2775, 0.2929, 0.2671, 0.2646, 0.2672, 0.2675, 0.2950,\n",
       "         0.2675, 0.2768, 0.2768, 0.2768, 0.2636, 0.2903, 0.2675, 0.2788, 0.2781,\n",
       "         0.2671, 0.2675, 0.2680, 0.2664, 0.2693, 0.2675, 0.2708, 0.2675, 0.2950,\n",
       "         0.2804, 0.2651, 0.2708, 0.2950, 0.2660, 0.2781, 0.2938, 0.2950, 0.2768,\n",
       "         0.2675, 0.2740, 0.2700, 0.2938, 0.2938, 0.3270, 0.3017, 0.5174, 0.3578,\n",
       "         0.4041, 0.5174, 0.3017, 0.3151, 0.3017, 0.3016, 0.3784, 0.5174, 0.5174,\n",
       "         0.4041, 0.3578, 0.3048, 0.3207, 0.3578, 0.4041, 0.4041, 0.3578, 0.3578,\n",
       "         0.4041, 0.3578, 0.3270, 0.3017, 0.4041, 0.3315, 0.3619, 0.3784, 0.5174,\n",
       "         0.3048, 0.3578, 0.3678, 0.3284, 0.3048, 0.4041, 0.3784, 0.3016, 0.3784,\n",
       "         0.3151, 0.3378, 0.3270, 0.3017, 0.3151, 0.3689, 0.4041, 0.3048, 0.3784,\n",
       "         0.3784, 0.5174, 0.3017, 0.3151, 0.3784, 0.3048, 0.3701, 0.3016, 0.2963,\n",
       "         0.3784, 0.3284, 0.3701, 0.5174, 0.3048, 0.4041, 0.3048, 0.3784, 0.5174,\n",
       "         0.3048, 0.3016, 0.3017, 0.3016, 0.3270, 0.3016, 0.3151, 0.5174, 0.3284,\n",
       "         0.3578, 0.3016, 0.3151, 0.3016, 0.4041, 0.3151, 0.3270, 0.3701, 0.3701,\n",
       "         0.3411, 0.5174, 0.3784, 0.3678, 0.3284, 0.3701, 0.3434, 0.3277, 0.3578,\n",
       "         0.3270, 0.3017, 0.3284, 0.3662, 0.3016, 0.3578, 0.3017, 0.3784, 0.3181,\n",
       "         0.5174, 0.5174, 0.3336, 0.3270, 0.3703, 0.4041, 0.3784, 0.3048, 0.3270,\n",
       "         0.4101, 0.3151, 0.3048, 0.4041, 0.3016, 0.3017, 0.3048, 0.3578, 0.3016,\n",
       "         0.3048, 0.3543, 0.3016, 0.3093, 0.3270, 0.5174, 0.3163, 0.3151, 0.3701,\n",
       "         0.3669, 0.3701, 0.3784, 0.3151, 0.3701, 0.3270, 0.3578, 0.3151, 0.3284,\n",
       "         0.3151, 0.3413, 0.5174, 0.3205, 0.3270, 0.4041, 0.3270, 0.3048, 0.3048,\n",
       "         0.3270, 0.3017, 0.3016, 0.3016, 0.3016, 0.3270, 0.3578, 0.3784, 0.3701,\n",
       "         0.3511, 0.4041, 0.3151, 0.3046, 0.2916, 0.3240, 0.3065, 0.3002, 0.3105,\n",
       "         0.3118, 0.3075, 0.3084, 0.2999, 0.2997, 0.2924, 0.3028, 0.2899, 0.3026,\n",
       "         0.3173, 0.3020, 0.3065, 0.3036, 0.3047, 0.2881, 0.2925, 0.3054, 0.3008,\n",
       "         0.3165, 0.2992, 0.3081, 0.2915, 0.2908, 0.3932, 0.3090, 0.2916, 0.2999,\n",
       "         0.2935, 0.7247, 0.3792, 0.4230, 0.3071, 0.3801, 0.3962, 0.3439, 0.3909,\n",
       "         0.3370, 0.3464, 0.2896, 0.3962, 0.3792, 0.2993, 0.3374, 0.4610, 0.3030,\n",
       "         0.2798, 0.3346, 0.4089, 0.3296, 0.2859, 0.4352, 0.3372, 0.2892, 0.2870,\n",
       "         0.2988, 0.3484, 0.3353, 0.3237, 0.3962, 0.3481, 0.3264, 0.2927, 0.3484,\n",
       "         0.4797, 0.3532, 0.3400, 0.3274, 0.3358, 0.3484, 0.3030, 0.3370, 0.3801,\n",
       "         0.3088, 0.3068, 0.2844, 0.3413, 0.3221, 0.3370, 0.3792, 0.2834, 0.2795,\n",
       "         0.3632, 0.3194, 0.2912, 0.3690, 0.4610, 0.3792, 0.3962, 0.2711, 0.3372,\n",
       "         0.3226, 0.3030, 0.3372, 0.3372, 0.3161, 0.3260, 0.3081, 0.3005, 0.2949,\n",
       "         0.7247, 0.4089, 0.3063, 0.2854, 0.2914, 0.3025, 0.2935, 0.4214, 0.2888,\n",
       "         0.7247, 0.3232, 0.3071, 0.3962, 0.2934, 0.3024, 0.3044, 0.3534, 0.3227,\n",
       "         0.2948, 0.2795, 0.3962, 0.3481, 0.7247, 0.3801, 0.2745, 0.2870, 0.3355,\n",
       "         0.3632, 0.3162, 0.3632, 0.3043]),\n",
       " tensor([0.2497, 0.2415, 0.3682, 0.2836, 0.2791, 0.2361, 0.3716, 0.3175, 0.3010,\n",
       "         0.2096, 0.2970, 0.2365, 0.2639, 0.2309, 0.2737, 0.2746, 0.2380, 0.2812,\n",
       "         0.2721, 0.2829, 0.3682, 0.2055, 0.3347, 0.2970, 0.2167, 0.2839, 0.2673,\n",
       "         0.2757, 0.2891, 0.2076, 0.3682, 0.2042, 0.2970, 0.2364, 0.3224, 0.3352,\n",
       "         0.3506, 0.2034, 0.3682, 0.2625, 0.2055, 0.2970, 0.2046, 0.2519, 0.2141,\n",
       "         0.3347, 0.2343, 0.3246, 0.2970, 0.3190, 0.3246, 0.2519, 0.3347, 0.2921,\n",
       "         0.2207, 0.3347, 0.2055, 0.2545, 0.2164, 0.2970, 0.2335, 0.2929, 0.3170,\n",
       "         0.2582, 0.3175, 0.2611, 0.3352, 0.2519, 0.3190, 0.2891, 0.3682, 0.2569,\n",
       "         0.3506, 0.2359, 0.2098, 0.2248, 0.2507, 0.3306, 0.2564, 0.1938, 0.2974,\n",
       "         0.3506, 0.2708, 0.3347, 0.2268, 0.2726, 0.4354, 0.2359, 0.2970, 0.2191,\n",
       "         0.2314, 0.2767, 0.2455, 0.2285, 0.3506, 0.2363, 0.2837, 0.3175, 0.2169,\n",
       "         0.2369, 0.2640, 0.3347, 0.2370, 0.2151, 0.2457, 0.2417, 0.2887, 0.2191,\n",
       "         0.2970, 0.2055, 0.2012, 0.2506, 0.3246, 0.2977, 0.3347, 0.2469, 0.3682,\n",
       "         0.2896, 0.2631, 0.3682, 0.2055, 0.2970, 0.2507, 0.2769, 0.3066, 0.2538,\n",
       "         0.2526, 0.3090, 0.3506, 0.2965, 0.2030, 0.3352, 0.3682, 0.2166, 0.3347,\n",
       "         0.2055, 0.2293, 0.2176, 0.2039, 0.2346, 0.2443, 0.2908, 0.2201, 0.3347,\n",
       "         0.2507, 0.2917, 0.2507, 0.2656, 0.2061, 0.2348, 0.2121, 0.3347, 0.3347,\n",
       "         0.2099, 0.3091, 0.2373, 0.2969, 0.2970, 0.3079, 0.1956, 0.2478, 0.2642,\n",
       "         0.3306, 0.3306, 0.2859, 0.2535, 0.2090, 0.3306, 0.3306, 0.2080, 0.3347,\n",
       "         0.2390, 0.1946, 0.3682, 0.3506, 0.3682, 0.2698, 0.3032, 0.2884, 0.2801,\n",
       "         0.3305, 0.2929, 0.2670, 0.2960, 0.2907, 0.2720, 0.2879, 0.2707, 0.2789,\n",
       "         0.2910, 0.3190, 0.2626, 0.2814, 0.2919, 0.2952, 0.2949, 0.2642, 0.3009,\n",
       "         0.2827, 0.5224, 0.4978, 0.3306, 0.2919, 0.2642, 0.2970, 0.2970, 0.3306,\n",
       "         0.5668, 0.2919, 0.3009, 0.2642, 0.2949, 0.2949, 0.2952, 0.4176, 0.2970,\n",
       "         0.3306, 0.3009, 0.2970, 0.3009, 0.2642, 0.3306, 0.2949, 0.3009, 0.2944,\n",
       "         0.2944, 0.2952, 0.2642, 0.2949, 0.2919, 0.2944, 0.2642, 0.2970, 0.2919,\n",
       "         0.2919, 0.2919, 0.2949, 0.4812, 0.2642, 0.2944, 0.2944, 0.3246, 0.2932,\n",
       "         0.3009, 0.2970, 0.2919, 0.2949, 0.2642, 0.2970, 0.2970, 0.2642, 0.2970,\n",
       "         0.2952, 0.2919, 0.2970, 0.2949, 0.5395, 0.2970, 0.2952, 0.2919, 0.2642,\n",
       "         0.3306, 0.2944, 0.4376, 0.2642, 0.3246, 0.2881, 0.3306, 0.2642, 0.2970,\n",
       "         0.3306, 0.2952, 0.2858, 0.2949, 0.2919, 0.2949, 0.2944, 0.2970, 0.2952,\n",
       "         0.3009, 0.2952, 0.3246, 0.2944, 0.2949, 0.3246, 0.3009, 0.2952, 0.2949,\n",
       "         0.2944, 0.3306, 0.2642, 0.2877, 0.2949, 0.3009, 0.2944, 0.2970, 0.2970,\n",
       "         0.2970, 0.2528, 0.2449, 0.2582, 0.2317, 0.2532, 0.2433, 0.2319, 0.2529,\n",
       "         0.2308, 0.2347, 0.2322, 0.2351, 0.2350, 0.2460, 0.2372, 0.2441, 0.2352,\n",
       "         0.2316, 0.2324, 0.2302, 0.2334, 0.2316, 0.2359, 0.2337, 0.2310, 0.2343,\n",
       "         0.2686, 0.2222, 0.2513, 0.2195, 0.2228, 0.2217, 0.2813, 0.2181, 0.2524,\n",
       "         0.2297, 0.2451, 0.2631, 0.2256, 0.2597, 0.2192, 0.2250, 0.2642, 0.2570,\n",
       "         0.2184, 0.2489, 0.2198, 0.2217, 0.2205, 0.2190, 0.2780, 0.2151, 0.2229,\n",
       "         0.2575, 0.2248, 0.2241, 0.2541, 0.2335, 0.2164, 0.2190, 0.2235, 0.2172,\n",
       "         0.2597, 0.2196, 0.2196, 0.2217, 0.2274, 0.2184, 0.2123, 0.2317, 0.2214,\n",
       "         0.2239, 0.2642, 0.2220, 0.2236, 0.2596, 0.2203, 0.2631, 0.2607, 0.2294,\n",
       "         0.2143, 0.2268, 0.2214, 0.2199, 0.2206, 0.2646, 0.2306, 0.2228, 0.2171,\n",
       "         0.2215, 0.2299, 0.2441, 0.2352, 0.2217, 0.2553, 0.2212, 0.2363, 0.2626,\n",
       "         0.2191, 0.2252, 0.2156, 0.2262, 0.2146, 0.2230, 0.2186, 0.2220, 0.2426,\n",
       "         0.2137, 0.2213, 0.2346, 0.2137, 0.2405, 0.2731, 0.2231, 0.2238, 0.2262,\n",
       "         0.2139, 0.2221]),\n",
       " tensor([0.3007, 0.2970, 0.2970, 0.5094, 0.5094, 0.3007, 0.3291, 0.3249, 0.3291,\n",
       "         0.3091, 0.3091, 0.3091, 0.2970, 0.3641, 0.3321, 0.3007, 0.3007, 0.2970,\n",
       "         0.3249, 0.3291, 0.5094, 0.2969, 0.3091, 0.3007, 0.3321, 0.3007, 0.3776,\n",
       "         0.3091, 0.3091, 0.3291, 0.2970, 0.3822, 0.2970, 0.3321, 0.2970, 0.3776,\n",
       "         0.5094, 0.3007, 0.2970, 0.3007, 0.2970, 0.3822, 0.3091, 0.3091, 0.3291,\n",
       "         0.3249, 0.2954, 0.5094, 0.3091, 0.3249, 0.3361, 0.2970, 0.2980, 0.2970,\n",
       "         0.5094, 0.2970, 0.2899, 0.3249, 0.3091, 0.3051, 0.3091, 0.3822, 0.3321,\n",
       "         0.3321, 0.3249, 0.5094, 0.3822, 0.3249, 0.2980, 0.2938, 0.3321, 0.3291,\n",
       "         0.3822, 0.2848, 0.3007, 0.5094, 0.2980, 0.3822, 0.3776, 0.3776, 0.2879,\n",
       "         0.3641, 0.5094, 0.2970, 0.3321, 0.3007, 0.3321, 0.3776, 0.3361, 0.3091,\n",
       "         0.3249, 0.3291, 0.2980, 0.3007, 0.3291, 0.3776, 0.2901, 0.3091, 0.3822,\n",
       "         0.3007, 0.3249, 0.3291, 0.2980, 0.3007, 0.2970, 0.3291, 0.3007, 0.3776,\n",
       "         0.3091, 0.2871, 0.2970, 0.2962, 0.2849, 0.3776, 0.3291, 0.2970, 0.2970,\n",
       "         0.2940, 0.3249, 0.3776, 0.3291, 0.2970, 0.3776, 0.3007, 0.3091, 0.3012,\n",
       "         0.3007, 0.3822, 0.2980, 0.3291, 0.3007, 0.3042, 0.2889, 0.3776, 0.3011,\n",
       "         0.2980, 0.3291, 0.3321, 0.2970, 0.3641, 0.2970, 0.3291, 0.2870, 0.3291,\n",
       "         0.2980, 0.2970, 0.2949, 0.2970, 0.3091, 0.3249, 0.3321, 0.2970, 0.3249,\n",
       "         0.3007, 0.3091, 0.3051, 0.3822, 0.2854, 0.2985, 0.3007, 0.3822, 0.2864,\n",
       "         0.3321, 0.3091, 0.3291, 0.3258, 0.3641, 0.3361, 0.5094, 0.3091, 0.3091,\n",
       "         0.3321, 0.3249, 0.2942, 0.3026, 0.3249, 0.2851, 0.3321, 0.3091, 0.2963,\n",
       "         0.2980, 0.2970, 0.3091, 0.3249, 0.5094, 0.2970, 0.3091, 0.3091, 0.3822,\n",
       "         0.4317, 0.3341, 0.4317, 0.3747, 0.3575, 0.3747, 0.4519, 0.3286, 0.3037,\n",
       "         0.5115, 0.4002, 0.3170, 0.4225, 0.3138, 0.5548, 0.3747, 0.5134, 0.4266,\n",
       "         0.4317, 0.4168, 0.4010, 0.3711, 0.4914, 0.4115, 0.3747, 0.3446, 0.5253,\n",
       "         0.3249, 0.3701, 0.3473, 0.3158, 0.3105, 0.4225, 0.2945, 0.4317, 0.3849,\n",
       "         0.4932, 0.3867, 0.3215, 0.3468, 0.6381, 0.4035, 0.3741, 0.4087, 0.4786,\n",
       "         0.2890, 0.4229, 0.3252, 0.3003, 0.4786, 0.2891, 0.5854, 0.3420, 0.3071,\n",
       "         0.4184, 0.4321, 0.3560, 0.3983, 0.2990, 0.3384, 0.3648, 0.3555, 0.3694,\n",
       "         0.3456, 0.4482, 0.3242, 0.3059, 0.3747, 0.4029, 0.4087, 0.3045, 0.3997,\n",
       "         0.5652, 0.2979, 0.3568, 0.3278, 0.3138, 0.4317, 0.3918, 0.4087, 0.4406,\n",
       "         0.3181, 0.3741, 0.4317, 0.4006, 0.4198, 0.2909, 0.3017, 0.4091, 0.3118,\n",
       "         0.4029, 0.3204, 0.6018, 0.4010, 0.4078, 0.4786, 0.3160, 0.4087, 0.4029,\n",
       "         0.3461, 0.3284, 0.3821, 0.5198, 0.3822, 0.3111, 0.9991, 0.3436, 0.3146,\n",
       "         0.3140, 0.4226, 0.3822, 0.2875, 0.3249, 0.7914, 0.3119, 0.3075, 0.3236,\n",
       "         0.3088, 0.5199, 0.3118, 0.3023, 0.2957, 0.3361, 0.5044, 0.2980, 0.3279,\n",
       "         0.3131, 0.3240, 0.3031, 0.3776, 0.3436, 0.3186, 0.5094, 0.2957, 0.3119,\n",
       "         0.3822, 0.3091, 0.5094, 0.3076, 0.3399, 0.3377, 0.4481, 0.3282, 0.2866,\n",
       "         0.3553, 0.3269, 0.3496, 0.3116, 0.3030, 0.2881, 0.3140, 0.3502, 0.3291,\n",
       "         0.3269, 0.3313, 0.5094, 0.3390, 0.3262, 0.3822, 0.9991, 0.3822, 0.5094,\n",
       "         0.3666, 0.2979, 0.3428, 0.3401, 0.3015, 0.3091, 0.9991, 0.2964, 0.3517,\n",
       "         0.3552, 0.3003, 0.5094, 0.3748, 0.3331, 0.3776, 0.3822, 0.3822, 0.4396,\n",
       "         0.4931, 0.9991, 0.9991, 0.3822, 0.3157, 0.3091, 0.3002, 0.3147, 0.9991,\n",
       "         0.3158, 0.4034, 0.2995, 0.3338, 0.3259, 0.9991, 0.3139, 0.3436, 0.3336,\n",
       "         0.3061, 0.5199, 0.2968, 0.3822, 0.3080, 0.3278, 0.3178, 0.5199, 0.3259,\n",
       "         0.3036, 0.3049, 0.3522, 0.3053, 0.3149, 0.4395, 0.3361, 0.3194, 0.3436,\n",
       "         0.3747, 0.3075, 0.3045, 0.5638, 0.3499, 0.3075, 0.3070, 0.3091, 0.3123,\n",
       "         0.3091, 0.3013, 0.3730, 0.4619, 0.3095, 0.3091, 0.3256, 0.3054, 0.3291,\n",
       "         0.4169, 0.3348, 0.3224, 0.2935, 0.3348, 0.2944, 0.3156, 0.3091, 0.3255,\n",
       "         0.2950, 0.2945, 0.3178, 0.2934, 0.3291, 0.2983, 0.9991, 0.9991, 0.3199,\n",
       "         0.3331, 1.1425, 0.3119, 0.3085, 0.3245, 0.3309, 0.3361, 0.3152, 0.3222,\n",
       "         0.3091, 0.3211, 0.2955, 0.3004, 0.3361, 0.3395, 0.3641, 0.4050, 0.3023,\n",
       "         0.3555, 0.3147, 0.3147, 0.3147, 0.2980, 0.3005, 0.3147, 0.2958, 0.2999,\n",
       "         0.2983, 0.3002, 0.3133, 0.2958, 0.3134, 0.3147, 0.3005, 0.3128, 0.2993,\n",
       "         0.3147, 0.3008, 0.3147, 0.3147, 0.2965, 0.3124, 0.3192, 0.3180, 0.3212,\n",
       "         0.3404, 0.3148, 0.3150, 0.3170, 0.3277, 0.3301, 0.3177, 0.3253, 0.3564])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['Query Points']['Implied Volatility']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
