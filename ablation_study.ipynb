{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ivyspt.input_processing import split_surfaces, IVSurfaceDataset\n",
    "from ivyspt.trainer import Trainer\n",
    "from ivyspt.ivyspt import IvySPT\n",
    "import warnings\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "RANDOM_STATE = 0\n",
    "N_JOBS = 8\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_data = pd.read_csv('data/pre_train_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "fine_tune_data = pd.read_csv('data/fine_tune_data.csv', parse_dates=True, index_col=[0, 1], date_format=\"ISO8601\")\n",
    "pre_train_surfaces_train, pre_train_surfaces_validation, pre_train_surfaces_test = split_surfaces(\n",
    "    pre_train_data,\n",
    "    toy_sample=True,\n",
    "    max_points=20,\n",
    "    max_surfaces=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "fine_tune_surfaces_train, fine_tune_surfaces_validation, fine_tune_surfaces_test = split_surfaces(\n",
    "    fine_tune_data,\n",
    "    toy_sample=True,\n",
    "    max_points=20,\n",
    "    max_surfaces=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Input Preprocessing' : {\n",
    "        'Mask Proportions' : [0.1, 0.3, 0.5, 0.7],\n",
    "        'Number of Query Points' : 1,\n",
    "        'Batch Size' : 5\n",
    "    },\n",
    "    'Surface Embedding' : {\n",
    "        'Embedding Dimension' : 32,\n",
    "    },\n",
    "    'Surface Encoding' : {\n",
    "        'Number of Heads' : 4,\n",
    "        'FFN Hidden Dimension' : 128,\n",
    "        'Attention Dropout' : 0.,\n",
    "        'Gate Dropout' : 0.,\n",
    "        'FFN Dropout' : 0.,\n",
    "        'Number of Blocks' : 4,\n",
    "        'External Feature Dimension' : 5,\n",
    "        'Weight Initializer Std.' : 0.02,\n",
    "        'Linear Bias Initializer' : 0.0,\n",
    "        'Gate Bias Inititalizer' : 10.0\n",
    "    },\n",
    "    'Adaptive Loss Weights' : {\n",
    "        'Asymmetry' : 1.5,\n",
    "    },\n",
    "    'Trainer' : {\n",
    "        'Pre-Train' : {\n",
    "            'Number of Epochs' : 20,\n",
    "            'Warmup Ratio' : 0.15,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-5,\n",
    "            'Gradient Clipping' : 1,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : None,\n",
    "        },\n",
    "        'Fine-Tune' : {\n",
    "            'Number of Epochs' : 10,\n",
    "            'Warmup Ratio' : 0.1,\n",
    "            'Peak Learning Rate' : 1e-3,\n",
    "            'Minimal Learning Rate' : 1e-6,\n",
    "            'Gradient Clipping' : 0,\n",
    "            'Adam Betas' : (0.9, 0.999),\n",
    "            'Adam Epsilon' : 1e-8,\n",
    "            'Adam Weight Decay' : 0.01,\n",
    "            'Layer-Wise Decay' : 0.9,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_dataset_train = IVSurfaceDataset(\n",
    "    pre_train_surfaces_train, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_train = DataLoader(\n",
    "    pre_train_dataset_train, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "pre_train_dataset_validation = IVSurfaceDataset(\n",
    "    pre_train_surfaces_validation, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_validation = DataLoader(\n",
    "    pre_train_dataset_validation, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "pre_train_dataset_test = IVSurfaceDataset(\n",
    "    pre_train_surfaces_test, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "pre_train_data_loader_test = DataLoader(\n",
    "    pre_train_dataset_test, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_dataset_train = IVSurfaceDataset(\n",
    "    fine_tune_surfaces_train, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "fine_tune_data_loader_train = DataLoader(\n",
    "    fine_tune_dataset_train, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "fine_tune_dataset_validation = IVSurfaceDataset(\n",
    "    fine_tune_surfaces_validation, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "fine_tune_data_loader_validation = DataLoader(\n",
    "    fine_tune_dataset_validation, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")\n",
    "fine_tune_dataset_test = IVSurfaceDataset(\n",
    "    fine_tune_surfaces_test, \n",
    "    hyperparameters['Input Preprocessing']['Mask Proportions'], \n",
    "    RANDOM_STATE, \n",
    "    hyperparameters['Input Preprocessing']['Number of Query Points'] \n",
    ")\n",
    "fine_tune_data_loader_test = DataLoader(\n",
    "    fine_tune_dataset_test, \n",
    "    batch_size=hyperparameters['Input Preprocessing']['Batch Size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=IVSurfaceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import tracemalloc\n",
    "\n",
    "def measure_time_memory(func, device, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # Measure the peak memory usage\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    result = func(*args, **kwargs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if device == \"cuda\":\n",
    "        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert to MB\n",
    "    else:\n",
    "        _, peak = tracemalloc.get_traced_memory()\n",
    "        max_memory = peak / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    print(f\"Time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Max Memory: {max_memory:.2f} MB\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Information:\n",
      "Model Name: Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\n",
      "\n",
      "\n",
      "GPU Information:\n",
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "\n",
    "def get_cpu_info():\n",
    "    # Run the lscpu command\n",
    "    result = subprocess.run(['lscpu'], stdout=subprocess.PIPE)\n",
    "    # Decode the output from bytes to string\n",
    "    lscpu_output = result.stdout.decode('utf-8')\n",
    "    \n",
    "    # Parse the lscpu output\n",
    "    cpu_info = {}\n",
    "    for line in lscpu_output.split('\\n'):\n",
    "        if line.strip():\n",
    "            parts = line.split(':', 1)\n",
    "            if len(parts) == 2:\n",
    "                key, value = parts\n",
    "                cpu_info[key.strip()] = value.strip()\n",
    "\n",
    "    # Extract useful information\n",
    "    useful_info = {\n",
    "        \"Model name\": cpu_info.get(\"Model name\"),\n",
    "    }\n",
    "\n",
    "    return useful_info\n",
    "\n",
    "def format_cpu_info(cpu_info):\n",
    "    report = (\n",
    "        f\"Model Name: {cpu_info['Model name']}\\n\"\n",
    "    )\n",
    "    return report\n",
    "\n",
    "def print_device_info():\n",
    "    # Print CPU info\n",
    "    cpu_info = get_cpu_info()\n",
    "    print(\"CPU Information:\")\n",
    "    print(format_cpu_info(cpu_info))\n",
    "\n",
    "    # Check if CUDA (GPU) is available\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nGPU Information:\")\n",
    "        print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        print(\"\\nNo GPU available. Running on CPU.\")\n",
    "\n",
    "# Call the function\n",
    "print_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_results(\n",
    "    name, \n",
    "    loss_coefficients_history, \n",
    "    train_loss_components_history, \n",
    "    validate_loss_components_history, \n",
    "    test_loss_records\n",
    "):\n",
    "    # Convert numpy arrays to DataFrames\n",
    "    loss_coefficients_df = pd.DataFrame(loss_coefficients_history, columns=['MSE Coeff', 'Calendar Coeff', 'Butterfly Coeff'])\n",
    "    train_loss_components_df = pd.DataFrame(train_loss_components_history, columns=['Train MSE Loss', 'Train Calendar Loss', 'Train Butterfly Loss'])\n",
    "    validate_loss_components_df = pd.DataFrame(validate_loss_components_history, columns=['Validate MSE Loss', 'Validate Calendar Loss', 'Validate Butterfly Loss'])\n",
    "\n",
    "    # Combine into one DataFrame\n",
    "    combined_history_df = pd.concat([loss_coefficients_df, train_loss_components_df, validate_loss_components_df], axis=1)\n",
    "\n",
    "    # Save the training history DataFrame\n",
    "    combined_history_df.to_csv(f'tests/{name}_train_history.csv', index=False)\n",
    "\n",
    "    # Save the test loss records\n",
    "    test_loss_records.to_csv(f'tests/{name}_test_loss_records.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Pre-Training:\n",
      "10000000000\n",
      "Time: 82.99 seconds\n",
      "Max Memory: 0.76 MB\n",
      "\n",
      "Testing Pre-Trained Model on Pre-Train Dataset:\n",
      "Time: 0.47 seconds\n",
      "Max Memory: 0.29 MB\n",
      "Pre-Train Test Loss Components: tensor([0.0364, 0.0051, 0.0000])\n",
      "\n",
      "Testing Pre-Trained Model on Fine-Tune Dataset:\n",
      "Time: 0.46 seconds\n",
      "Max Memory: 0.26 MB\n",
      "Pre-Train Fine-Tune Dataset Test Loss Components: tensor([2.5841e-02, 5.9880e-03, 1.3396e+01])\n",
      "\n",
      "Fine-Tuning:\n",
      "10000000000\n",
      "Time: 39.77 seconds\n",
      "Max Memory: 0.41 MB\n",
      "\n",
      "Testing Fine-Tuned Model:\n",
      "Time: 0.48 seconds\n",
      "Max Memory: 0.27 MB\n",
      "Fine-Tune Test Loss Components: tensor([0.0304, 0.0144, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "model_pre_train = IvySPT(\n",
    "    hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "    hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "    hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "    hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "    hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "    hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "    hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "    hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "    hyperparameters['Surface Encoding']['Gate Bias Inititalizer']\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "pre_trainer = Trainer(\n",
    "    model_pre_train,\n",
    "    pre_train_data_loader_train,\n",
    "    pre_train_data_loader_validation,\n",
    "    pre_train_data_loader_test,\n",
    "    hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "# Pre-Training\n",
    "print(\"\\nPre-Training:\")\n",
    "pre_train_loss_coefficients_history, pre_train_train_loss_components_history, pre_train_validate_loss_components_history = \\\n",
    "    measure_time_memory(\n",
    "        pre_trainer.train, \n",
    "        device, \n",
    "        # experiment_name=\"pre_train_experiment\"\n",
    "    )\n",
    "\n",
    "# Test Pre-Trained Model on Pre-Train Dataset\n",
    "print(\"\\nTesting Pre-Trained Model on Pre-Train Dataset:\")\n",
    "pre_train_test_loss_components, pre_train_test_loss_records = measure_time_memory(pre_trainer.test, device)\n",
    "print(f\"Pre-Train Test Loss Components: {pre_train_test_loss_components}\")\n",
    "\n",
    "save_training_results(\n",
    "    'pre_train',\n",
    "    pre_train_loss_coefficients_history, \n",
    "    pre_train_train_loss_components_history, \n",
    "    pre_train_validate_loss_components_history, \n",
    "    pre_train_test_loss_records\n",
    ")\n",
    "\n",
    "# Test Pre-Trained Model on Fine-Tune Dataset\n",
    "print(\"\\nTesting Pre-Trained Model on Fine-Tune Dataset:\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pre_train_trainer_fine_test = Trainer(\n",
    "    model_pre_train,  # Use the same pre-trained model\n",
    "    fine_tune_data_loader_train,\n",
    "    fine_tune_data_loader_validation,\n",
    "    fine_tune_data_loader_test,\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "\n",
    "pre_train_fine_test_loss_components, pre_train_fine_test_loss_records = measure_time_memory(pre_train_trainer_fine_test.test, device)\n",
    "print(f\"Pre-Train Fine-Tune Dataset Test Loss Components: {pre_train_fine_test_loss_components}\")\n",
    "\n",
    "# Fine-Tuning\n",
    "print(\"\\nFine-Tuning:\")\n",
    "\n",
    "# Create a copy of the pre-trained model\n",
    "model_fine_tune = copy.deepcopy(model_pre_train)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "fine_tuner = Trainer(\n",
    "    model_fine_tune,\n",
    "    fine_tune_data_loader_train,\n",
    "    fine_tune_data_loader_validation,\n",
    "    fine_tune_data_loader_test,\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "\n",
    "fine_tune_loss_coefficients_history, fine_tune_train_loss_components_history, fine_tune_validate_loss_components_history = measure_time_memory(\n",
    "    fine_tuner.train, \n",
    "    device, \n",
    "    # experiment_name=\"fine_tune_experiment\"\n",
    ")\n",
    "\n",
    "# Test Fine-Tuned Model\n",
    "print(\"\\nTesting Fine-Tuned Model:\")\n",
    "fine_tune_test_loss_components, fine_tune_test_loss_records = measure_time_memory(fine_tuner.test, device)\n",
    "print(f\"Fine-Tune Test Loss Components: {fine_tune_test_loss_components}\")\n",
    "save_training_results(\n",
    "    'pre_train',\n",
    "    fine_tune_loss_coefficients_history, \n",
    "    fine_tune_train_loss_components_history, \n",
    "    fine_tune_validate_loss_components_history, \n",
    "    fine_tune_test_loss_records\n",
    ")\n",
    "\n",
    "# Save the pre-trained model\n",
    "torch.save(model_pre_train.state_dict(), 'models/model_pre_train.pth')\n",
    "# Save the fine-tuned model\n",
    "torch.save(model_fine_tune.state_dict(), 'models/model_fine_tune.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the Continuous Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Pre-Training:\n",
      "Time: 81.71 seconds\n",
      "Max Memory: 0.44 MB\n",
      "\n",
      "Testing Pre-Trained Model:\n",
      "Time: 0.49 seconds\n",
      "Max Memory: 0.27 MB\n",
      "Pre-Train Test Loss Components: tensor([0.0066, 0.0012, 0.0000])\n",
      "\n",
      "Fine-Tuning:\n",
      "Time: 40.12 seconds\n",
      "Max Memory: 0.40 MB\n",
      "\n",
      "Testing Fine-Tuned Model:\n",
      "Time: 0.47 seconds\n",
      "Max Memory: 0.27 MB\n",
      "Fine-Tune Test Loss Components: tensor([0.0057, 0.0023, 0.1761])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "remove_kernel_model_pre_train = IvySPT(\n",
    "    hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "    hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "    hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "    hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "    hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "    hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "    hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "    hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "    hyperparameters['Surface Encoding']['Gate Bias Inititalizer'],\n",
    "    remove_kernel=True,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_kernel_pre_trainer = Trainer(\n",
    "    remove_kernel_model_pre_train,\n",
    "    pre_train_data_loader_train,\n",
    "    pre_train_data_loader_validation,\n",
    "    pre_train_data_loader_test,\n",
    "    hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "# Pre-Training\n",
    "print(\"\\nPre-Training:\")\n",
    "remove_kernel_pre_train_loss_coefficients_history, remove_kernel_pre_train_train_loss_components_history, remove_kernel_pre_train_validate_loss_components_history = \\\n",
    "    measure_time_memory(\n",
    "        remove_kernel_pre_trainer.train, \n",
    "        device, \n",
    "        # experiment_name=\"pre_train_experiment\"\n",
    "    )\n",
    "\n",
    "# Test Pre-Trained Model\n",
    "print(\"\\nTesting Pre-Trained Model:\")\n",
    "remove_kernel_pre_train_test_loss_components, remove_kernel_pre_train_test_loss_records = measure_time_memory(remove_kernel_pre_trainer.test, device)\n",
    "print(f\"Pre-Train Test Loss Components: {remove_kernel_pre_train_test_loss_components}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_kernel_fine_tuner = Trainer(\n",
    "    remove_kernel_model_pre_train,\n",
    "    fine_tune_data_loader_train,\n",
    "    fine_tune_data_loader_validation,\n",
    "    fine_tune_data_loader_test,\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "\n",
    "# Fine-Tuning\n",
    "print(\"\\nFine-Tuning:\")\n",
    "remove_kernel_fine_tune_loss_coefficients_history, remove_kernel_fine_tune_train_loss_components_history, remove_kernel_fine_tune_validate_loss_components_history = measure_time_memory(\n",
    "    remove_kernel_fine_tuner.train, \n",
    "    device, \n",
    "    # experiment_name=\"fine_tune_experiment\"\n",
    ")\n",
    "\n",
    "# Test Fine-Tuned Model\n",
    "print(\"\\nTesting Fine-Tuned Model:\")\n",
    "remove_kernel_fine_tune_test_loss_components, remove_kernel_fine_tune_test_loss_records = measure_time_memory(remove_kernel_fine_tuner.test, device)\n",
    "print(f\"Fine-Tune Test Loss Components: {remove_kernel_fine_tune_test_loss_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Pre-Training:\n",
      "Time: 82.45 seconds\n",
      "Max Memory: 0.40 MB\n",
      "\n",
      "Testing Pre-Trained Model:\n",
      "Time: 0.48 seconds\n",
      "Max Memory: 0.26 MB\n",
      "Pre-Train Test Loss Components: tensor([nan, nan, nan])\n",
      "\n",
      "Fine-Tuning:\n",
      "Time: 39.96 seconds\n",
      "Max Memory: 0.39 MB\n",
      "\n",
      "Testing Fine-Tuned Model:\n",
      "Time: 0.47 seconds\n",
      "Max Memory: 0.26 MB\n",
      "Fine-Tune Test Loss Components: tensor([nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "remove_pe_model_pre_train = IvySPT(\n",
    "    hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "    hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "    hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "    hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "    hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "    hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "    hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "    hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "    hyperparameters['Surface Encoding']['Gate Bias Inititalizer'],\n",
    "    remove_positional_embedding=True,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_pe_pre_trainer = Trainer(\n",
    "    remove_pe_model_pre_train,\n",
    "    pre_train_data_loader_train,\n",
    "    pre_train_data_loader_validation,\n",
    "    pre_train_data_loader_test,\n",
    "    hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "# Pre-Training\n",
    "print(\"\\nPre-Training:\")\n",
    "remove_pe_pre_train_loss_coefficients_history, remove_pe_pre_train_train_loss_components_history, remove_pe_pre_train_validate_loss_components_history = \\\n",
    "    measure_time_memory(\n",
    "        remove_pe_pre_trainer.train, \n",
    "        device, \n",
    "        # experiment_name=\"pre_train_experiment\"\n",
    "    )\n",
    "\n",
    "# Test Pre-Trained Model\n",
    "print(\"\\nTesting Pre-Trained Model:\")\n",
    "remove_pe_pre_train_test_loss_components, remove_pe_pre_train_test_loss_records = measure_time_memory(remove_pe_pre_trainer.test, device)\n",
    "print(f\"Pre-Train Test Loss Components: {remove_pe_pre_train_test_loss_components}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_pe_fine_tuner = Trainer(\n",
    "    remove_pe_model_pre_train,\n",
    "    fine_tune_data_loader_train,\n",
    "    fine_tune_data_loader_validation,\n",
    "    fine_tune_data_loader_test,\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "\n",
    "# Fine-Tuning\n",
    "print(\"\\nFine-Tuning:\")\n",
    "remove_pe_fine_tune_loss_coefficients_history, remove_pe_fine_tune_train_loss_components_history, remove_pe_fine_tune_validate_loss_components_history = measure_time_memory(\n",
    "    remove_pe_fine_tuner.train, \n",
    "    device, \n",
    "    # experiment_name=\"fine_tune_experiment\"\n",
    ")\n",
    "\n",
    "# Test Fine-Tuned Model\n",
    "print(\"\\nTesting Fine-Tuned Model:\")\n",
    "remove_pe_fine_tune_test_loss_components, remove_pe_fine_tune_test_loss_records = measure_time_memory(remove_pe_fine_tuner.test, device)\n",
    "print(f\"Fine-Tune Test Loss Components: {remove_pe_fine_tune_test_loss_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the External Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Pre-Training:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Pre-Training\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPre-Training:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m remove_market_pre_train_loss_coefficients_history, remove_market_pre_train_train_loss_components_history, remove_market_pre_train_validate_loss_components_history \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mmeasure_time_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_market_pre_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# experiment_name=\"pre_train_experiment\"\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Test Pre-Trained Model\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting Pre-Trained Model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mmeasure_time_memory\u001b[0;34m(func, device, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     11\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mreset_peak_memory_stats()\n\u001b[0;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/scikit_learn_data/implied-volatility-surface-with-transformers/ivyspt/trainer.py:168\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m train_loss_components_sums \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss_components\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    166\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 168\u001b[0m \u001b[43madaptive_loss_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip)\n\u001b[1;32m    171\u001b[0m total_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scikit_learn_data/implied-volatility-surface-with-transformers/ivyspt/adaptive_loss_coefficients.py:63\u001b[0m, in \u001b[0;36mAdaptiveLossCoefficients.forward\u001b[0;34m(self, current_losses, final_layer)\u001b[0m\n\u001b[1;32m     60\u001b[0m relative_inverse_rates \u001b[38;5;241m=\u001b[39m loss_ratios \u001b[38;5;241m/\u001b[39m loss_ratios\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Compute gradient norms for each weighted loss\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m gradient_norms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m     64\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnorm(torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i] \u001b[38;5;241m*\u001b[39m loss, final_layer\u001b[38;5;241m.\u001b[39mparameters(), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, loss \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(current_losses)\n\u001b[1;32m     66\u001b[0m ])\n\u001b[1;32m     68\u001b[0m target_gradient_norms \u001b[38;5;241m=\u001b[39m (gradient_norms\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m*\u001b[39m (relative_inverse_rates \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha))\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     69\u001b[0m gradnorm_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mabs(gradient_norms \u001b[38;5;241m-\u001b[39m target_gradient_norms))\n",
      "File \u001b[0;32m~/scikit_learn_data/implied-volatility-surface-with-transformers/ivyspt/adaptive_loss_coefficients.py:64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m relative_inverse_rates \u001b[38;5;241m=\u001b[39m loss_ratios \u001b[38;5;241m/\u001b[39m loss_ratios\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Compute gradient norms for each weighted loss\u001b[39;00m\n\u001b[1;32m     63\u001b[0m gradient_norms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[0;32m---> 64\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, loss \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(current_losses)\n\u001b[1;32m     66\u001b[0m ])\n\u001b[1;32m     68\u001b[0m target_gradient_norms \u001b[38;5;241m=\u001b[39m (gradient_norms\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m*\u001b[39m (relative_inverse_rates \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha))\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     69\u001b[0m gradnorm_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mabs(gradient_norms \u001b[38;5;241m-\u001b[39m target_gradient_norms))\n",
      "File \u001b[0;32m~/anaconda3/envs/Apache/lib/python3.10/site-packages/torch/autograd/__init__.py:394\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    390\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    391\u001b[0m         grad_outputs_\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    405\u001b[0m         output\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28minput\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (output, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, t_inputs)\n\u001b[1;32m    409\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "remove_market_model_pre_train = IvySPT(\n",
    "    hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "    hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "    hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "    hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "    hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "    hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "    hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "    hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "    hyperparameters['Surface Encoding']['Gate Bias Inititalizer'],\n",
    "    remove_external_attention=True,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_market_pre_trainer = Trainer(\n",
    "    remove_market_model_pre_train,\n",
    "    pre_train_data_loader_train,\n",
    "    pre_train_data_loader_validation,\n",
    "    pre_train_data_loader_test,\n",
    "    hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "# Pre-Training\n",
    "print(\"\\nPre-Training:\")\n",
    "remove_market_pre_train_loss_coefficients_history, remove_market_pre_train_train_loss_components_history, remove_market_pre_train_validate_loss_components_history = \\\n",
    "    measure_time_memory(\n",
    "        remove_market_pre_trainer.train, \n",
    "        device, \n",
    "        # experiment_name=\"pre_train_experiment\"\n",
    "    )\n",
    "\n",
    "# Test Pre-Trained Model\n",
    "print(\"\\nTesting Pre-Trained Model:\")\n",
    "remove_market_pre_train_test_loss_components, remove_market_pre_train_test_loss_records = measure_time_memory(remove_market_pre_trainer.test, device)\n",
    "print(f\"Pre-Train Test Loss Components: {remove_market_pre_train_test_loss_components}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_market_fine_tuner = Trainer(\n",
    "    remove_market_model_pre_train,\n",
    "    fine_tune_data_loader_train,\n",
    "    fine_tune_data_loader_validation,\n",
    "    fine_tune_data_loader_test,\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "\n",
    "# Fine-Tuning\n",
    "print(\"\\nFine-Tuning:\")\n",
    "remove_market_fine_tune_loss_coefficients_history, remove_market_fine_tune_train_loss_components_history, remove_market_fine_tune_validate_loss_components_history = measure_time_memory(\n",
    "    remove_market_fine_tuner.train, \n",
    "    device, \n",
    "    # experiment_name=\"fine_tune_experiment\"\n",
    ")\n",
    "\n",
    "# Test Fine-Tuned Model\n",
    "print(\"\\nTesting Fine-Tuned Model:\")\n",
    "remove_market_fine_tune_test_loss_components, remove_market_fine_tune_test_loss_records = measure_time_memory(remove_market_fine_tuner.test, device)\n",
    "print(f\"Fine-Tune Test Loss Components: {remove_market_fine_tune_test_loss_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the Gated Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Pre-Training:\n",
      "Time: 79.52 seconds\n",
      "Max Memory: 0.43 MB\n",
      "\n",
      "Testing Pre-Trained Model:\n",
      "Time: 0.48 seconds\n",
      "Max Memory: 0.26 MB\n",
      "Pre-Train Test Loss Components: tensor([0.0470, 0.0005, 0.0000])\n",
      "\n",
      "Fine-Tuning:\n",
      "Time: 39.64 seconds\n",
      "Max Memory: 0.40 MB\n",
      "\n",
      "Testing Fine-Tuned Model:\n",
      "Time: 0.49 seconds\n",
      "Max Memory: 0.26 MB\n",
      "Fine-Tune Test Loss Components: tensor([0.0903, 0.0003, 0.0432])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "remove_gate_model_pre_train = IvySPT(\n",
    "    hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "    hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "    hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "    hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "    hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "    hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "    hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "    hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "    hyperparameters['Surface Encoding']['Gate Bias Inititalizer'],\n",
    "    remove_gate=True\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_gate_pre_trainer = Trainer(\n",
    "    remove_gate_model_pre_train,\n",
    "    pre_train_data_loader_train,\n",
    "    pre_train_data_loader_validation,\n",
    "    pre_train_data_loader_test,\n",
    "    hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "# Pre-Training\n",
    "print(\"\\nPre-Training:\")\n",
    "remove_gate_pre_train_loss_coefficients_history, remove_gate_pre_train_train_loss_components_history, remove_gate_pre_train_validate_loss_components_history = \\\n",
    "    measure_time_memory(\n",
    "        remove_gate_pre_trainer.train, \n",
    "        device, \n",
    "        # experiment_name=\"pre_train_experiment\"\n",
    "    )\n",
    "\n",
    "# Test Pre-Trained Model\n",
    "print(\"\\nTesting Pre-Trained Model:\")\n",
    "remove_gate_pre_train_test_loss_components, remove_gate_pre_train_test_loss_records = measure_time_memory(remove_gate_pre_trainer.test, device)\n",
    "print(f\"Pre-Train Test Loss Components: {remove_gate_pre_train_test_loss_components}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_gate_fine_tuner = Trainer(\n",
    "    remove_gate_model_pre_train,\n",
    "    fine_tune_data_loader_train,\n",
    "    fine_tune_data_loader_validation,\n",
    "    fine_tune_data_loader_test,\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device\n",
    ")\n",
    "\n",
    "# Fine-Tuning\n",
    "print(\"\\nFine-Tuning:\")\n",
    "remove_gate_fine_tune_loss_coefficients_history, remove_gate_fine_tune_train_loss_components_history, remove_gate_fine_tune_validate_loss_components_history = measure_time_memory(\n",
    "    remove_gate_fine_tuner.train, \n",
    "    device, \n",
    "    # experiment_name=\"fine_tune_experiment\"\n",
    ")\n",
    "\n",
    "# Test Fine-Tuned Model\n",
    "print(\"\\nTesting Fine-Tuned Model:\")\n",
    "remove_gate_fine_tune_test_loss_components, remove_gate_fine_tune_test_loss_records = measure_time_memory(remove_gate_fine_tuner.test, device)\n",
    "print(f\"Fine-Tune Test Loss Components: {remove_gate_fine_tune_test_loss_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the No Arbitrage Conditions Soft Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Pre-Training:\n",
      "Time: 15.39 seconds\n",
      "Max Memory: 0.46 MB\n",
      "\n",
      "Testing Pre-Trained Model:\n",
      "Time: 0.49 seconds\n",
      "Max Memory: 0.27 MB\n",
      "Pre-Train Test Loss Components: tensor([0.0033, 0.0045, 0.0000])\n",
      "\n",
      "Fine-Tuning:\n",
      "Time: 7.60 seconds\n",
      "Max Memory: 0.43 MB\n",
      "\n",
      "Testing Fine-Tuned Model:\n",
      "Time: 0.49 seconds\n",
      "Max Memory: 0.27 MB\n",
      "Fine-Tune Test Loss Components: tensor([0.0066, 0.0002, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "remove_arbitrage_model_pre_train = IvySPT(\n",
    "    hyperparameters['Surface Embedding']['Embedding Dimension'], \n",
    "    hyperparameters['Surface Encoding']['Number of Blocks'],\n",
    "    hyperparameters['Surface Encoding']['Number of Heads'], \n",
    "    hyperparameters['Surface Encoding']['FFN Hidden Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Attention Dropout'], \n",
    "    hyperparameters['Surface Encoding']['Gate Dropout'],\n",
    "    hyperparameters['Surface Encoding']['FFN Dropout'],\n",
    "    hyperparameters['Surface Encoding']['External Feature Dimension'],\n",
    "    hyperparameters['Surface Encoding']['Weight Initializer Std.'],\n",
    "    hyperparameters['Surface Encoding']['Linear Bias Initializer'],\n",
    "    hyperparameters['Surface Encoding']['Gate Bias Inititalizer']\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_arbitrage_pre_trainer = Trainer(\n",
    "    remove_arbitrage_model_pre_train,\n",
    "    pre_train_data_loader_train,\n",
    "    pre_train_data_loader_validation,\n",
    "    pre_train_data_loader_test,\n",
    "    hyperparameters['Trainer']['Pre-Train']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Pre-Train']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device,\n",
    "    remove_multi_loss=True\n",
    ")\n",
    "# Pre-Training\n",
    "print(\"\\nPre-Training:\")\n",
    "remove_arbitrage_pre_train_loss_coefficients_history, remove_arbitrage_pre_train_train_loss_components_history, remove_arbitrage_pre_train_validate_loss_components_history = \\\n",
    "    measure_time_memory(\n",
    "        remove_arbitrage_pre_trainer.train, \n",
    "        device, \n",
    "        # experiment_name=\"pre_train_experiment\"\n",
    "    )\n",
    "\n",
    "# Test Pre-Trained Model\n",
    "print(\"\\nTesting Pre-Trained Model:\")\n",
    "remove_arbitrage_pre_train_test_loss_components, remove_arbitrage_pre_train_test_loss_records = measure_time_memory(remove_arbitrage_pre_trainer.test, device)\n",
    "print(f\"Pre-Train Test Loss Components: {remove_arbitrage_pre_train_test_loss_components}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "remove_arbitrage_fine_tuner = Trainer(\n",
    "    remove_arbitrage_model_pre_train,\n",
    "    fine_tune_data_loader_train,\n",
    "    fine_tune_data_loader_validation,\n",
    "    fine_tune_data_loader_test,\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Number of Epochs'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Warmup Ratio'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Peak Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Minimal Learning Rate'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Gradient Clipping'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Betas'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Epsilon'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Adam Weight Decay'],\n",
    "    hyperparameters['Trainer']['Fine-Tune']['Layer-Wise Decay'],\n",
    "    hyperparameters['Adaptive Loss Weights']['Asymmetry'],\n",
    "    device,\n",
    "    remove_multi_loss=True\n",
    ")\n",
    "\n",
    "# Fine-Tuning\n",
    "print(\"\\nFine-Tuning:\")\n",
    "remove_arbitrage_fine_tune_loss_coefficients_history, remove_arbitrage_fine_tune_train_loss_components_history, remove_arbitrage_fine_tune_validate_loss_components_history = measure_time_memory(\n",
    "    remove_arbitrage_fine_tuner.train, \n",
    "    device, \n",
    "    # experiment_name=\"fine_tune_experiment\"\n",
    ")\n",
    "\n",
    "# Test Fine-Tuned Model\n",
    "print(\"\\nTesting Fine-Tuned Model:\")\n",
    "remove_arbitrage_fine_tune_test_loss_components, remove_arbitrage_fine_tune_test_loss_records = measure_time_memory(remove_arbitrage_fine_tuner.test, device)\n",
    "print(f\"Fine-Tune Test Loss Components: {remove_arbitrage_fine_tune_test_loss_components}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
